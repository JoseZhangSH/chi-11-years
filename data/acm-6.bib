
@inproceedings{gurari_crowdverge_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{CrowdVerge}: {Predicting} {If} {People} {Will} {Agree} on the {Answer} to a {Visual} {Question}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025781},
	doi = {10.1145/3025453.3025781},
	abstract = {Visual question answering systems empower users to ask any question about any image and receive a valid answer. However, existing systems do not yet account for the fact that a visual question can lead to a single answer or multiple different answers. While a crowd often agrees, disagreements do arise for many reasons including that visual questions are ambiguous, subjective, or difficult. We propose a model, CrowdVerge, for automatically predicting from a visual question whether a crowd would agree on one answer. We then propose how to exploit these predictions in a novel application to efficiently collect all valid answers to visual questions. Specifically, we solicit fewer human responses when answer agreement is expected and more human responses otherwise. Experiments on 121,811 visual questions asked by sighted and blind people show that, compared to existing crowdsourcing systems, our system captures the same answer diversity with typically 14-23\% less crowd involvement.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gurari, Danna and Grauman, Kristen},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, machine learning, visual question answering},
	pages = {3511--3522},
}

@inproceedings{valentine_flash_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Flash {Organizations}: {Crowdsourcing} {Complex} {Work} by {Structuring} {Crowds} {As} {Organizations}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025811},
	doi = {10.1145/3025453.3025811},
	abstract = {This paper introduces flash organizations: crowds structured like organizations to achieve complex and open-ended goals. Microtask workflows, the dominant crowdsourcing structures today, only enable goals that are so simple and modular that their path can be entirely pre-defined. We present a system that organizes crowd workers into computationally-represented structures inspired by those used in organizations - roles, teams, and hierarchies - which support emergent and adaptive coordination toward open-ended goals. Our system introduces two technical contributions: 1) encoding the crowd's division of labor into de-individualized roles, much as movie crews or disaster response teams use roles to support coordination between on-demand workers who have not worked together before; and 2) reconfiguring these structures through a model inspired by version control, enabling continuous adaptation of the work and the division of labor. We report a deployment in which flash organizations successfully carried out open-ended and complex goals previously out of reach for crowdsourcing, including product design, software development, and game production. This research demonstrates digitally networked organizations that flexibly assemble and reassemble themselves from a globally distributed online workforce to accomplish complex work.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Valentine, Melissa A. and Retelny, Daniela and To, Alexandra and Rahmati, Negar and Doshi, Tulsee and Bernstein, Michael S.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, expert crowd work, flash organizations},
	pages = {3523--3537},
}

@inproceedings{park_facilitating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Facilitating {Pervasive} {Community} {Policing} on the {Road} with {Mobile} {Roadwatch}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025867},
	doi = {10.1145/3025453.3025867},
	abstract = {We consider community policing on the road with pervasive recording technologies such as dashcams and smartphones where citizens are actively volunteering to capture and report various threats to traffic safety to the police via mobile apps. This kind of novel community policing has recently gained significant popularity in Korea and India. In this work, we identify people's general attitude and concerns toward community policing on the road through an online survey. We then address the major concerns by building a mobile app that supports easy event capture/access, context tagging, and privacy preservation. Our two-week user study (n = 23) showed Roadwatch effectively supported community policing activities on the road. Further, we found that the critical factors for reporting are personal involvement and seriousness of risks, and participants were mainly motivated by their contribution to traffic safety. Finally, we discuss several practical design implications to facilitate community policing on the road.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Park, Sangkeun and Ilincai, Emilia-Stefania and Oh, Jeungmin and Kwon, Sujin and Mizouni, Rabeb and Lee, Uichin},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, mobile, community policing, neighborhood watch, traffic},
	pages = {3538--3550},
}

@inproceedings{chen_retool_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ReTool}: {Interactive} {Microtask} and {Workflow} {Design} through {Demonstration}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025969},
	doi = {10.1145/3025453.3025969},
	abstract = {In addition to simple form filling, there is an increasing need for crowdsourcing workers to perform freeform interactions directly on content in microtask crowdsourcing (e.g. proofreading articles or specifying object boundary in an image). Such microtasks are often organized within well-designed workflows to optimize task quality and workload distribution. However, designing and implementing the interface and workflow for such microtasks is challenging because it typically requires programming knowledge and tedious manual effort. We present ReTool, a web-based tool for requesters to design and publish interactive microtasks and workflows by demonstrating the microtasks for text and image content. We evaluated ReTool against a task-design tool from a popular crowdsourcing platform and showed the advantages of ReTool over the existing approach.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Chen and Meng, Xiaojun and Zhao, Shengdong and Fjeld, Morten},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, workflow, freeform interactive microtasks, programming by demonstration, retool},
	pages = {3551--3556},
}

@inproceedings{cecchinato_always_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Always {On}({Line})? {User} {Experience} of {Smartwatches} and {Their} {Role} within {Multi}-{Device} {Ecologies}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025538},
	doi = {10.1145/3025453.3025538},
	abstract = {Users have access to a growing ecosystem of devices (desktop, mobile and wearable) that can deliver notifications and help people to stay in contact. Smartwatches are gaining popularity, yet little is known about the user experience and their impact on our increasingly always online culture. We report on a qualitative study with existing users on their everyday use of smartwatches to understand both the added value and the challenges of being constantly connected at the wrist. Our findings show that users see a large benefit in receiving notifications on their wrist, especially in terms of helping manage expectations of availability. Moreover, we find that response rates after viewing a notification on a smartwatch change based on the other devices available: laptops prompt quicker replies than smartphones. Finally, there are still many costs associated with using smartwatches, thus we make a series of design recommendations to improve the user experience of smartwatches.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cecchinato, Marta E. and Cox, Anna L. and Bird, Jon},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {notifications, user experience, wearable, autoethnography, context-aware, cross-device interaction, device ecologies, multi-device experience, smartwatches},
	pages = {3557--3568},
}

@inproceedings{visuri_quantifying_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Quantifying {Sources} and {Types} of {Smartwatch} {Usage} {Sessions}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025817},
	doi = {10.1145/3025453.3025817},
	abstract = {We seek to quantify smartwatch use, and establish differences and similarities to smartphone use. Our analysis considers use traces from 307 users that include over 2.8 million notifications and 800,000 screen usage events, and we compare our findings to previous work that quantifies smartphone use. The results show that smartwatches are used more briefly and more frequently throughout the day, with half the sessions lasting less than 5 seconds. Interaction with notifications is similar across both types of devices, both in terms of response times and preferred application types. We also analyse the differences between our smartwatch dataset and a dataset aggregated from four previously conducted smartphone studies. The similarities and differences between smartwatch and smartphone use suggest effect on usage that go beyond differences in form factor.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Visuri, Aku and Sarsenbayeva, Zhanna and van Berkel, Niels and Goncalves, Jorge and Rawassizadeh, Reza and Kostakos, Vassilis and Ferreira, Denzil},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {smartphones, smartwatches, applications, interactions, session, usage},
	pages = {3569--3581},
}

@inproceedings{mcmillan_situating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Situating {Wearables}: {Smartwatch} {Use} in {Context}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025993},
	doi = {10.1145/3025453.3025993},
	abstract = {Drawing on 168 hours of video recordings of smartwatch use, this paper studies how context influences smartwatch use. We explore the effects of the presence of others, activity, location and time of day on 1,009 instances of use. Watch interaction is significantly shorter when in conversation than when alone. Activity also influences watch use with significantly longer use while eating than when socialising or performing domestic tasks. One surprising finding is that length of use is similar at home and work. We note that usage peaks around lunchtime, with an average of 5.3 watch uses per hour throughout a day. We supplement these findings with qualitative analysis of the videos, focusing on how use is modified by the presence of others, and the lack of impact of watch glances on conversation. Watch use is clearly a context-sensitive activity and in discussion we explore how smartwatches could be designed taking this into consideration.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McMillan, Donald and Brown, Barry and Lampinen, Airi and McGregor, Moira and Hoggan, Eve and Pizza, Stefania},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {wearable, smartwatch, video analysis},
	pages = {3582--3594},
}

@inproceedings{cambo_breaksense_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{BreakSense}: {Combining} {Physiological} and {Location} {Sensing} to {Promote} {Mobility} during {Work}-{Breaks}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026021},
	doi = {10.1145/3025453.3026021},
	abstract = {Work breaks can play an important role in the mental and physical well-being of workers and contribute positively to productivity. In this paper we explore the use of activity-, physiological-, and indoor-location sensing to promote mobility during work-breaks. While the popularity of devices and applications to promote physical activity is growing, prior research highlights important constraints when designing for the workplace. With these constraints in mind, we developed BreakSense, a mobile application that uses a Bluetooth beacon infrastructure, a smartphone and a smartwatch to encourage mobility during breaks with a game-like design. We discuss constraints imposed by design for work and the workplace, and highlight challenges associated with the use of noisy sensors and methods to overcome them. We then describe a short deployment of BreakSense within our lab that examined bound vs. unbound augmented breaks and how they affect users' sense of completion and readiness to work.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cambo, Scott A. and Avrahami, Daniel and Lee, Matthew L.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {workplace, activity recognition, wellbeing, context aware, indoor location, work breaks},
	pages = {3595--3607},
}

@inproceedings{bopp_disempowered_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Disempowered by {Data}: {Nonprofits}, {Social} {Enterprises}, and the {Consequences} of {Data}-{Driven} {Work}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025694},
	doi = {10.1145/3025453.3025694},
	abstract = {Organizations across many sectors are under intense pressure to become data-driven. Yet, for mission-driven organizations, the path to becoming and value of being data-driven is not always clear. We present results from an interview-based study of the role of data in the monitoring and evaluation practices of mission-driven organizations. Instead of leading to productive and empowering data-driven decision making, monitoring and evaluation work is characterized by the erosion of autonomy, data drift, and data fragmentation. Together, these consequences of monitoring and evaluation practices play into a cycle of increasing disempowerment for the mission-driven organization. These findings suggest that the design of information systems should work towards empowering organizations in ways that make sense for their unique data needs and those of their constituents.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bopp, Chris and Harmon, Ellie and Voida, Amy},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {data, accountability, metrics, mission-driven, monitoring and evaluation, nonprofit organization, social enterprise},
	pages = {3608--3619},
}

@inproceedings{dasgupta_scratch_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Scratch {Community} {Blocks}: {Supporting} {Children} as {Data} {Scientists}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025847},
	doi = {10.1145/3025453.3025847},
	abstract = {In this paper, we present Scratch Community Blocks, a new system that enables children to programmatically access, analyze, and visualize data about their participation in Scratch, an online community for learning computer programming. At its core, our approach involves a shift in who analyzes data: from adult data scientists to young learners themselves. We first introduce the goals and design of the system and then demonstrate it by describing example projects that illustrate its functionality. Next, we show through a series of case studies how the system engages children in not only representing data and answering questions with data but also in self-reflection about their own learning and participation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dasgupta, Sayamindu and Hill, Benjamin Mako},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {learning, creativity support tools, data science, block-based programming, computers and children, social computing and social navigation},
	pages = {3620--3631},
}

@inproceedings{tolmie_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting the {Use} of {User} {Generated} {Content} in {Journalistic} {Practice}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025892},
	doi = {10.1145/3025453.3025892},
	abstract = {Social media and user-generated content (UGC) are increasingly important features of journalistic work in a number of different ways. However, their use presents major challenges, not least because information posted on social media is not always reliable and therefore its veracity needs to be checked before it can be considered as fit for use in the reporting of news. We report on the results of a series of in-depth ethnographic studies of journalist work practices undertaken as part of the requirements gathering for a prototype of a social media verification 'dashboard' and its subsequent evaluation. We conclude with some reflections upon the broader implications of our findings for the design of tools to support journalistic work.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tolmie, Peter and Procter, Rob and Randall, David William and Rouncefield, Mark and Burger, Christian and Wong Sak Hoi, Geraldine and Zubiaga, Arkaitz and Liakata, Maria},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ethnography, journalism, collaborative work practices, dashboard design, social media verification},
	pages = {3632--3644},
}

@inproceedings{boukhelifa_how_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {How {Data} {Workers} {Cope} with {Uncertainty}: {A} {Task} {Characterisation} {Study}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025738},
	doi = {10.1145/3025453.3025738},
	abstract = {Uncertainty plays an important and complex role in data analysis, where the goal is to find pertinent patterns, build robust models, and support decision making. While these endeavours are often associated with professional data scientists, many domain experts engage in such activities with varying skill levels. To understand how these domain experts (or "data workers") analyse uncertain data we conducted a qualitative user study with 12 participants from a variety of domains. In this paper, we describe their various coping strategies to understand, minmise, exploit or even ignore this uncertainty. The choice of the coping strategy is influenced by accepted domain practices, but appears to depend on the types and sources of uncertainty and whether participants have access to support tools. Based on these findings, we propose a new process model of how data workers analyse various types of uncertain data and conclude with design considerations for uncertainty-aware data analytics.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Boukhelifa, Nadia and Perrin, Marc-Emmanuel and Huron, Samuel and Eagan, James},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {data science, uncertainty, qualitative study, data analysis},
	pages = {3645--3656},
}

@inproceedings{peek_cardboard_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Cardboard {Machine} {Kit}: {Modules} for the {Rapid} {Prototyping} of {Rapid} {Prototyping} {Machines}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025491},
	doi = {10.1145/3025453.3025491},
	abstract = {Digital fabrication machines (such as laser cutters or 3D printers) can be instructed to produce any part geometry within their application space. However, machines' application spaces are not easily modified or extended. How can we enable the production of application-specific computer-controlled machines by machine building novices? How can we facilitate rapid prototyping of rapid prototyping tools? We propose a novel set of modules, the Cardboard Machine Kit, for the construction of digital fabrication machines. These open-source modules are implemented using cardboard frames, stepper motors, and networked electronics controlled through a Python library. We evaluated the kit both through machine building workshops and by studying the usage of the kit in the wild. In the wild we observed more than 500 novice machine builders who built 125 different machines for 15 different application types. We argue that this breadth demonstrates the efficacy of this modular approach. Finally we discuss the limitations of the Cardboard Machine Kit and discuss how it could inform future machine building infrastructure.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Peek, Nadya and Coleman, James and Moyer, Ilan and Gershenfeld, Neil},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {prototyping, digital fabrication, CAD/CAM, cardboard, CNC, machine building},
	pages = {3657--3668},
}

@inproceedings{sareen_printflatables_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Printflatables: {Printing} {Human}-{Scale}, {Functional} and {Dynamic} {Inflatable} {Objects}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025898},
	doi = {10.1145/3025453.3025898},
	abstract = {Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. We use inextensible thermoplastic fabric as the raw material with the key principle of introducing folds and thermal sealing. Upon inflation, the sealed object takes the expected three dimensional shape. The workflow begins with the user specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled thermal contact iron that seals layers of thermoplastic fabric. In this paper, we discuss the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sareen, Harpreet and Umapathi, Udayan and Shin, Patrick and Kakehi, Yasuaki and Ou, Jifei and Ishii, Hiroshi and Maes, Pattie},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {3D printing, digital fabrication, human-material interaction, inflatables, radical atoms, shape changing},
	pages = {3669--3680},
}

@inproceedings{bucci_sketching_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Sketching {CuddleBits}: {Coupled} {Prototyping} of {Body} and {Behaviour} for an {Affective} {Robot} {Pet}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025774},
	doi = {10.1145/3025453.3025774},
	abstract = {Social robots that physically display emotion invite natural communication with their human interlocutors, enabling applications like robot-assisted therapy where a complex robot's breathing influences human emotional and physiological state. Using DIY fabrication and assembly, we explore how simple 1-DOF robots can express affect with economy and user customizability, leveraging open-source designs.We developed low-cost techniques for coupled iteration of a simple robot's body and behaviour, and evaluated its potential to display emotion. Through two user studies, we (1) validated these CuddleBits' ability to express emotions (N=20); (2) sourced a corpus of 72 robot emotion behaviours from participants (N=10); and (3) analyzed it to link underlying parameters to emotional perception (N=14).We found that CuddleBits can express arousal (activation), and to a lesser degree valence (pleasantness). We also show how a sketch-refine paradigm combined with DIY fabrication and novel input methods enable parametric design of physical emotion display, and discuss how mastering this parsimonious case can give insight into layering simple behaviours in more complex robots.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bucci, Paul and Cang, Xi Laura and Valair, Anasazi and Marino, David and Tseng, Lucia and Jung, Merel and Rantala, Jussi and Schneider, Oliver S. and MacLean, Karon E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {affective computing, haptics, do-it-yourself (DIY), human-robot interaction (HRI), physical prototyping},
	pages = {3681--3692},
}

@inproceedings{yue_wiredraw_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{WireDraw}: {3D} {Wire} {Sculpturing} {Guided} with {Mixed} {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025792},
	doi = {10.1145/3025453.3025792},
	abstract = {The availability of commodity 3D extruder pen allows direct drawing of 3D wire sculptures for novice users, enabling many novel applications such as intuitive spatial intelligence development for school students. However, the lack of spatial and structural cues among individual pen strokes makes the 3D drawing process challenging, which often leads to highly distorted and even incomplete wire sculptures. We present a mixed reality system, called `WireDraw', to immersively guide the 3D drawing for easy wire sculpturing. The system design is based on novel 3D drawing principles and the subsequent optimization, making the stroke sequence of the wire model drawable and easy to draw. On-the-fly edits on unsatisfactory strokes are also allowed for creative design. We demonstrate the effectiveness of our system by testing on a variety of wire models and a user study. The results show that the visual guidance provided by our system is extremely helpful for drawing high-quality wire sculptures.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yue, Ya-Ting and Zhang, Xiaolong and Yang, Yongliang and Ren, Gang and Choi, Yi-King and Wang, Wenping},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {mixed reality, 3D extruder pen, drawing optimization, stroke generation, wire sculpture},
	pages = {3693--3704},
}

@inproceedings{spelmezan_sparkle_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Sparkle: {Hover} {Feedback} with {Touchable} {Electric} {Arcs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025782},
	doi = {10.1145/3025453.3025782},
	abstract = {Many finger sensing input devices now support proximity input, enabling users to perform in-air gestures. While near-surface interactions increase the input vocabulary, they lack tactile feedback, making it hard for users to perform gestures or to know when the interaction takes place. Sparkle stimulates the fingertip with touchable electric arcs above a hover sensing device to give users in-air tactile or thermal feedback, sharper and more feelable than acoustic mid-air haptic devices. We present the design of a high voltage resonant transformer with a low-loss soft ferrite core and self-tuning driver circuit, with which we create electric arcs 6 mm in length, and combine this technology with infrared proximity sensing in two proof-of-concept devices with form factor and functionality similar to a button and a touchpad. We provide design guidelines for Sparkle devices and examples of stimuli in application scenarios, and report the results of a user study on the perceived sensations. Sparkle is the first step towards providing a new type of hover feedback, and it does not require users to wear tactile stimulators.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Spelmezan, Daniel and Sahoo, Deepak Ranjan and Subramanian, Sriram},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {electric discharge, high voltage resonant transformer, hover input, in-air feedback, infrared proximity sensor.},
	pages = {3705--3717},
}

@inproceedings{cheng_sparse_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Sparse {Haptic} {Proxy}: {Touch} {Feedback} in {Virtual} {Environments} {Using} a {General} {Passive} {Prop}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025753},
	doi = {10.1145/3025453.3025753},
	abstract = {We propose a class of passive haptics that we call Sparse Haptic Proxy: a set of geometric primitives that simulate touch feedback in elaborate virtual reality scenes. Unlike previous passive haptics that replicate the virtual environment in physical space, a Sparse Haptic Proxy simulates a scene's detailed geometry by redirecting the user's hand to a matching primitive of the proxy. To bridge the divergence of the scene from the proxy, we augment an existing Haptic Retargeting technique with an on-the-fly target remapping: We predict users' intentions during interaction in the virtual space by analyzing their gaze and hand motions, and consequently redirect their hand to a matching part of the proxy. We conducted three user studies on haptic retargeting technique and implemented a system from three main results: 1) The maximum angle participants found acceptable for retargeting their hand is 40°, with an average rating of 4.6 out of 5. 2) Tracking participants' eye gaze reliably predicts their touch intentions (97.5\%), even while simultaneously manipulating the user's hand-eye coordination for retargeting. 3) Participants preferred minimized retargeting distances over better-matching surfaces of our Sparse Haptic Proxy when receiving haptic feedback for single-finger touch input. We demonstrate our system with two virtual scenes: a flight cockpit and a room quest game. While their scene geometries differ substantially, both use the same sparse haptic proxy to provide haptic feedback to the user during task completion.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cheng, Lung-Pan and Ofek, Eyal and Holz, Christian and Benko, Hrvoje and Wilson, Andrew D.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, perception, passive haptics, retargeting},
	pages = {3718--3728},
}

@inproceedings{kaul_haptichead_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{HapticHead}: {A} {Spherical} {Vibrotactile} {Grid} around the {Head} for {3D} {Guidance} in {Virtual} and {Augmented} {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025684},
	doi = {10.1145/3025453.3025684},
	abstract = {Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4\% vs. 54.2\% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7\% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kaul, Oliver Beren and Rohs, Michael},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, augmented reality, vibrotactile, haptic feedback, 3d output, guidance, navigation, spatial interaction},
	pages = {3729--3740},
}

@inproceedings{morales_gonzalez_passive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Passive yet {Expressive} {TouchTokens}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025894},
	doi = {10.1145/3025453.3025894},
	abstract = {TouchTokens are passive tokens that can be recognized on any capacitive surface based on the spatial configuration of the fingers that hold them. However, interaction with these tokens is confined to the basic two-state model of touch interaction as the system only knows the tokens' position and cannot detect tokens that are not touched. We increase the expressive power of TouchTokens by introducing laser-cut lattice hinges in their design, so as to make them flexible. A new recognizer, that analyzes the micro-movements of the fingers that hold the tokens, enables the system to detect when a token is left on the surface rather than taken off it. It can also detect bend events that can be mapped to command triggers, and a squeezed state that can be used for quasi-modal interaction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Morales González, Rafael and Appert, Caroline and Bailly, Gilles and Pietriga, Emmanuel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {tangible interaction, micro-movements, multi-touch input},
	pages = {3741--3745},
}

@inproceedings{wang_enhancing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Enhancing {Pen}-{Based} {Interaction} {Using} {Electrovibration} and {Vibration} {Haptic} {Feedback}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025555},
	doi = {10.1145/3025453.3025555},
	abstract = {This paper presents the EV2-Pen which leverages electrovibration technology and vibration technology in pen interaction. Electrovibration technology can produce multisensory feedback when the pen is in motion (sliding/moving on the screen), and vibration technology can provide vibrative feedback when the pen is stationary (pointing/resting on the screen). We conducted an experiment to investigate user performance with the EV2-Pen. The results indicated that the EV2-Pen outperformed the EV-Pen [18, 19] in pointing-steering tasks. Finally, we discuss the characteristics of the EV2-Pen, and explore some possible applications and scenarios.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qinglong and Ren, Xiangshi and Sun, Xiaoying},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {haptic feedback, vibration, electrovibration, pen-based interaction},
	pages = {3746--3750},
}

@inproceedings{abdelrahman_stay_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Stay {Cool}! {Understanding} {Thermal} {Attacks} on {Mobile}-{Based} {User} {Authentication}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025461},
	doi = {10.1145/3025453.3025461},
	abstract = {PINs and patterns remain among the most widely used knowledge-based authentication schemes. As thermal cameras become ubiquitous and affordable, we foresee a new form of threat to user privacy on mobile devices. Thermal cameras allow performing thermal attacks, where heat traces, resulting from authentication, can be used to reconstruct passwords. In this work we investigate in details the viability of exploiting thermal imaging to infer PINs and patterns on mobile devices. We present a study (N=18) where we evaluated how properties of PINs and patterns influence their thermal attacks resistance. We found that thermal attacks are indeed viable on mobile devices; overlapping patterns significantly decrease successful thermal attack rate from 100\% to 16.67\%, while PINs remain vulnerable (\&gt;72\% success rate) even with duplicate digits. We conclude by recommendations for users and designers of authentication schemes on how to resist thermal attacks.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Abdelrahman, Yomna and Khamis, Mohamed and Schneegass, Stefan and Alt, Florian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {thermal imaging, mobile authentication, touchscreens},
	pages = {3751--3763},
}

@inproceedings{das_thumprint_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Thumprint: {Socially}-{Inclusive} {Local} {Group} {Authentication} {Through} {Shared} {Secret} {Knocks}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025991},
	doi = {10.1145/3025453.3025991},
	abstract = {Small, local groups who share protected resources (e.g., families, work teams, student organizations) have unmet authentication needs. For these groups, existing authentication strategies either create unnecessary social divisions (e.g., biometrics), do not identify individuals (e.g., shared passwords), do not equitably distribute security responsibility (e.g., individual passwords), or make it difficult to share or revoke access (e.g., physical keys). To explore an alternative, we designed Thumprint: inclusive group authentication with a shared secret knock. All group members share one secret knock, but individual expressions of the secret are discernible. We evaluated the usability and security of our concept through two user studies with 30 participants. Our results suggest that (1) individuals who enter the same shared thumprint are distinguishable from one another, (2) that people can enter thumprints consistently over time, and (3) that thumprints are resilient to casual adversaries.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Das, Sauvik and Laput, Gierad and Harrison, Chris and Hong, Jason I.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {hci, usable security, authentication, sensors, social cybersecurity, socially-inclusive authentication},
	pages = {3764--3774},
}

@inproceedings{ur_design_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Design and {Evaluation} of a {Data}-{Driven} {Password} {Meter}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026050},
	doi = {10.1145/3025453.3026050},
	abstract = {Despite their ubiquity, many password meters provide inaccurate strength estimates. Furthermore, they do not explain to users what is wrong with their password or how to improve it. We describe the development and evaluation of a data-driven password meter that provides accurate strength measurement and actionable, detailed feedback to users. This meter combines neural networks and numerous carefully combined heuristics to score passwords and generate data-driven text feedback about the user's password. We describe the meter's iterative development and final design. We detail the security and usability impact of the meter's design dimensions, examined through a 4,509-participant online study. Under the more common password-composition policy we tested, we found that the data-driven meter with detailed feedback led users to create more secure, and no less memorable, passwords than a meter with only a bar as a strength indicator.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ur, Blase and Alfieri, Felicia and Aung, Maung and Bauer, Lujo and Christin, Nicolas and Colnago, Jessica and Cranor, Lorrie Faith and Dixon, Henry and Emami Naeini, Pardis and Habib, Hana and Johnson, Noah and Melicher, William},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {usable security, data-driven, feedback, meter, passwords},
	pages = {3775--3786},
}

@inproceedings{tan_can_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Can {Unicorns} {Help} {Users} {Compare} {Crypto} {Key} {Fingerprints}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025733},
	doi = {10.1145/3025453.3025733},
	abstract = {Many authentication schemes ask users to manually compare compact representations of cryptographic keys, known as fingerprints. If the fingerprints do not match, that may signal a man-in-the-middle attack. An adversary performing an attack may use a fingerprint that is similar to the target fingerprint, but not an exact match, to try to fool inattentive users. Fingerprint representations should thus be both usable and secure. We tested the usability and security of eight fingerprint representations under different configurations. In a 661-participant between-subjects experiment, participants compared fingerprints under realistic conditions and were subjected to a simulated attack. The best configuration allowed attacks to succeed 6\% of the time; the worst 72\%. We find the seemingly effective compare-and-select approach performs poorly for key fingerprints and that graphical fingerprint representations, while intuitive and fast, vary in performance. We identify some fingerprint representations as particularly promising.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Joshua and Bauer, Lujo and Bonneau, Joseph and Cranor, Lorrie Faith and Thomas, Jeremy and Ur, Blase},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {usability, authentication, key fingerprints, secure messaging},
	pages = {3787--3798},
}

@inproceedings{gonzalez_cabanas_fdvt_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{FDVT}: {Data} {Valuation} {Tool} for {Facebook} {Users}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025903},
	doi = {10.1145/3025453.3025903},
	abstract = {The OECD, the European Union and other public and private initiatives are claiming for the necessity of tools that create awareness among Internet users about the monetary value associated to the commercial exploitation of their online personal information. This paper presents the first tool addressing this challenge, the Facebook Data Valuation Tool (FDVT). The FDVT provides Facebook users with a personalized and real-time estimation of the revenue they generate for Facebook. Relying on the FDVT, we are able to shed light into several relevant HCI research questions that require a data valuation tool in place. The obtained results reveal that (i) there exists a deep lack of awareness among Internet users regarding the monetary value of personal information, (ii) data valuation tools such as the FDVT are useful means to reduce such knowledge gap, (iii) 1/3 of the users testing the FDVT show a substantial engagement with the tool.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {González Cabañas, José and Cuevas, Ángel and Cuevas, Rubén},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, facebook, personal data, data valuation, FDVT},
	pages = {3799--3809},
}

@inproceedings{usmani_characterizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Characterizing {Social} {Insider} {Attacks} on {Facebook}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025901},
	doi = {10.1145/3025453.3025901},
	abstract = {Facebook accounts are secured against unauthorized access through passwords and device-level security. Those defenses, however, may not be sufficient to prevent social insider attacks, where attackers know their victims, and gain access to a victim's account by interacting directly with their device. To characterize these attacks, we ran two MTurk studies. In the first (n = 1,308), using the list experiment method, we estimated that 24\% of participants had perpetrated social insider attacks and that 21\% had been victims (and knew about it). In the second study (n = 45), participants wrote stories detailing personal experiences with such attacks. Using thematic analysis, we typified attacks around five motivations (fun, curiosity, jealousy, animosity, and utility), and explored dimensions associated with each type. Our combined findings indicate that social insider attacks are common, often have serious emotional consequences, and have no simple mitigation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Usmani, Wali Ahmed and Marques, Diogo and Beschastnikh, Ivan and Beznosov, Konstantin and Guerreiro, Tiago and Carriço, Luís},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, facebook, usable security, insider attack},
	pages = {3810--3820},
}

@inproceedings{such_photo_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Photo {Privacy} {Conflicts} in {Social} {Media}: {A} {Large}-{Scale} {Empirical} {Study}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025668},
	doi = {10.1145/3025453.3025668},
	abstract = {Items in social media such as photos may be co-owned by multiple users, i.e., the sharing decisions of the ones who upload them have the potential to harm the privacy of the others. Previous works uncovered coping strategies by co-owners to manage their privacy, but mainly focused on general practices and experiences. We establish an empirical base for the prevalence, context and severity of privacy conflicts over co-owned photos. To this aim, a parallel survey of pre-screened 496 uploaders and 537 co-owners collected occurrences and type of conflicts over co-owned photos, and any actions taken towards resolving them. We uncover nuances and complexities not known before, including co-ownership types, and divergences in the assessment of photo audiences. We also find that an all-or-nothing approach seems to dominate conflict resolution, even when parties actually interact and talk about the conflict. Finally, we derive key insights for designing systems to mitigate these divergences and facilitate consensus.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Such, Jose M. and Porter, Joel and Preibusch, Sören and Joinson, Adam},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {social media, privacy, online social networks, co-ownership, conflicts, photo sharing},
	pages = {3821--3832},
}

@inproceedings{bullek_towards_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Towards {Understanding} {Differential} {Privacy}: {When} {Do} {People} {Trust} {Randomized} {Response} {Technique}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025698},
	doi = {10.1145/3025453.3025698},
	abstract = {As a consequence of living in a data ecosystem, we often relinquish personal information to be used in contexts in which we have no control. In this paper, we begin to examine the usability of differential privacy, a mechanism that proposes to promise privacy with a mathematical "proof" to the data donor. Do people trust this promise and adjust their privacy decisions if the interfaces through which they interact make differential privacy less opaque? In a study with 228 participants, we measured comfort, understanding, and trust using a variant of differential privacy known as Randomized Response Technique (RRT). We found that allowing individuals to see the amount of obfuscation applied to their responses increased their trust in the privacy-protecting mechanism. However, participants who associated obfuscating privacy mechanisms with deception did not make the "safest" privacy decisions, even as they demonstrated an understanding of RRT. We demonstrate that prudent privacy-related decisions can be cultivated with simple explanations of usable privacy.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bullek, Brooke and Garboski, Stephanie and Mir, Darakhshan J. and Peck, Evan M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {randomized response, user-centered differential privacy},
	pages = {3833--3837},
}

@inproceedings{hoyle_was_2017,
	address = {New York, NY, USA},
	title = {Was {My} {Message} {Read}? {Privacy} and {Signaling} on {Facebook} {Messenger}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025925},
	doi = {10.1145/3025453.3025925},
	abstract = {Major online messaging services such as Facebook Messenger and WhatsApp are starting to provide users with real-time information about when people read their messages, while useful, the feature has the potential to negatively impact privacy as well as cause concern over access to self. We report on two surveys using Mechanical Turk which looked at senders' (N=402},
	publisher = {Association for Computing Machinery},
	author = {Hoyle, Roberto and Das, Srijita and Kapadia, Apu and Lee, Adam J. and Vaniea, Kami},
	year = {2017},
}

@inproceedings{uddin_effects_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Effects} of {Artificial} {Landmarks} on {Learning} and {Performance} in {Spatial}-{Memory} {Interfaces}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025497},
	doi = {10.1145/3025453.3025497},
	abstract = {Spatial memory is a powerful way for users to become expert with an interface, because remembering item locations means that users do not have to carry out slow visual search. Spatial learning in the real world benefits greatly from landmarks in the environment, but user interfaces often provide very few visual landmarks. In this paper we explore the use of artificial landmarks as a way to improve people's spatial memory in spatially-stable grid menus called CommandMaps. We carried out three studies to test the effects of three types of artificial landmarks (standard grid, simple anchor marks, and a transparent image) on spatial learning. We found that for small grid menus, the artificial landmarks had little impact on performance, whereas for medium and large grids, the simple anchor marks significantly improved performance. The simple visual anchors were faster and less error-prone than the visually richer transparent image. Our studies show that artificial landmarks can be a valuable addition to spatial interfaces.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Uddin, Md. Sami and Gutwin, Carl and Cockburn, Andy},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {command selection, expertise, landmarks, spatial memory},
	pages = {3843--3855},
}

@inproceedings{verma_studying_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Studying {Space} {Use}: {Bringing} {HCI} {Tools} to {Architectural} {Projects}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026055},
	doi = {10.1145/3025453.3026055},
	abstract = {Understanding how people use different spaces in a building can inform design interventions aimed at improving the utility of that building, but can also inform the design of future buildings. We studied space use in an office building following a method we have designed to reveal the occupancy rate and navigational patterns. Our method involves two key components: 1) a pervasive sensing system that is scalable for large buildings, and high number of occupants, and 2) participatory data analysis engaging stakeholders including interior architects and building performance engineers, to refine the questions and define the needs for further analyses through multiple iterations.In this paper, we describe our method in detail, and exemplify how HCI methods and approaches can contribute to professional building design projects.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Verma, Himanshu and Alavi, Hamed S. and Lalanne, Denis},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {human-building interaction, hci in architecture, participatory data analysis, post-occupancy evaluation},
	pages = {3856--3866},
}

@inproceedings{dye_locating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Locating the {Internet} in the {Parks} of {Havana}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025728},
	doi = {10.1145/3025453.3025728},
	abstract = {Since March 2015, the public squares of Havana have been transformed from places where people stroll and children play to places where crowds gather to try to connect to the internet at all hours of the day and night. We present a field investigation of public WiFi hotspots in Havana, Cuba, and examine the possibilities of internet access these limited and expensive hotspots present to individuals, many of who are experiencing the internet for the first time. Drawing on fieldwork conducted in 2015-2016, we underscore the reconfigurations that have resulted from this access, as evolving internet users reconfigure their interactions with place, time, and individuals in their efforts to locate the internet. We also discuss the implications our findings have for the design of internet access interventions in Cuba and in other low-resource environments across the world, as well as the broader implications for social computing across diverse geographies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dye, Michaelanne and Nemer, David and Pina, Laura R. and Sambasivan, Nithya and Bruckman, Amy S. and Kumar, Neha},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {hci4d, social computing, cuba, internet, place, wifi},
	pages = {3867--3878},
}

@inproceedings{gil_tritap_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{TriTap}: {Identifying} {Finger} {Touches} on {Smartwatches}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025561},
	doi = {10.1145/3025453.3025561},
	abstract = {The small screens of smartwatches provide limited space for input tasks. Finger identification is a promising technique to address this problem by associating different functions with different fingers. However, current technologies for finger identification are unavailable or unsuitable for smartwatches. To address this problem, this paper observes that normal smartwatch use takes places with a relatively static pose between the two hands. In this situation, we argue that the touch and angle profiles generated by different fingers on a standard smartwatch touch screen will differ sufficiently to support reliable identification. The viability of this idea is explored in two studies that capture touches in natural and exaggerated poses during tapping and swiping tasks. Machine learning models report accuracies of up to 93\% and 98\% respectively, figures that are sufficient for many common interaction tasks. Furthermore, the exaggerated poses show modest costs (in terms of time/errors) compared to the natural touches. We conclude by presenting examples and discussing how interaction designs using finger identification can be adapted to the smartwatch form factor.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gil, Hyunjae and Lee, DoYoung and Im, Seunggyu and Oakley, Ian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {smartwatch, finger identification, touch contact profile},
	pages = {3879--3890},
}

@inproceedings{sridhar_watchsense_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{WatchSense}: {On}- and {Above}-{Skin} {Input} {Sensing} through a {Wearable} {Depth} {Sensor}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026005},
	doi = {10.1145/3025453.3026005},
	abstract = {This paper contributes a novel sensing approach to support on- and above-skin finger input for interaction on the move. WatchSense uses a depth sensor embedded in a wearable device to expand the input space to neighboring areas of skin and the space above it. Our approach addresses challenging camera-based tracking conditions, such as oblique viewing angles and occlusions. It can accurately detect fingertips, their locations, and whether they are touching the skin or hovering above it. It extends previous work that supported either mid-air or multitouch input by simultaneously supporting both. We demonstrate feasibility with a compact, wearable prototype attached to a user's forearm (simulating an integrated depth sensor). Our prototype—which runs in real-time on consumer mobile devices—enables a 3D input space on the back of the hand. We evaluated the accuracy and robustness of the approach in a user study. We also show how WatchSense increases the expressiveness of input by interweaving mid-air and multitouch for several interactive applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sridhar, Srinath and Markussen, Anders and Oulasvirta, Antti and Theobalt, Christian and Boring, Sebastian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {smartwatch, depth sensor, finger tracking, skin interaction},
	pages = {3891--3902},
}

@inproceedings{singh_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting {Everyday} {Function} in {Chronic} {Pain} {Using} {Wearable} {Technology}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025947},
	doi = {10.1145/3025453.3025947},
	abstract = {While most rehabilitation technologies target situated exercise sessions and associated performance metrics, physiotherapists recommend physical activities that are integrated with everyday functioning. We conducted a 1-2 week home study to explore how people with chronic pain use wearable technology that senses and sonifies movement (i.e., movement mapped to sound in real-time) to do functional activity (e.g., loading the dishwasher). Our results show that real-time movement sonification led to an increased sense of control during challenging everyday tasks. Sonification calibrated to functional activity facilitated application of pain management techniques such as pacing. When calibrated to individual needs, sonification enabled serendipitous discovery of physical capabilities otherwise obscured by a focus on pain or a dysfunctional proprioceptive system. A physiotherapist was invited to comment on the implications of our findings. We conclude by discussing opportunities provided by wearable sensing technology to enable better functioning, the ultimate goal of physical rehabilitation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Aneesha and Bianchi-Berthouze, Nadia and Williams, Amanda CdeC},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {wearables, feedback, chronic pain, everyday function, home rehabilitation, sonification, ubiquitous technology.},
	pages = {3903--3915},
}

@inproceedings{aggarwal_sophy_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{SoPhy}: {A} {Wearable} {Technology} for {Lower} {Limb} {Assessment} in {Video} {Consultations} of {Physiotherapy}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025489},
	doi = {10.1145/3025453.3025489},
	abstract = {Physiotherapists are increasingly using video conferencing tools for their teleconsultations. Yet, the assessment of subtle differences in body movements remains a challenge. To support lower limb assessment in video consultations, we present SoPhy, a wearable technology consisting of a pair of socks with embedded sensors for patients to wear; and a web interface that displays information about range of weight distribution, foot movement, and foot orientation for physiotherapists in real-time. We conducted a laboratory study of 40 video consultations, in which postgraduate physiotherapy students assessed lower limb function. We compare assessment with and without SoPhy. Findings show that SoPhy increased the confidence in assessing squats exercise and fewer repetitions were required to assess patients when using SoPhy. We discuss the significance of SoPhy to address the challenges of assessing bodily information over video, and present considerations for its integration with clinical practices and tools.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Aggarwal, Deepti and Zhang, Weiyi and Hoang, Thuong and Ploderer, Bernd and Vetere, Frank and Bradford, Mark},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {wearable technology, bodily communication, clinical consultation, physiotherapy, video communication},
	pages = {3916--3928},
}

@inproceedings{wu_eaglesense_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{EagleSense}: {Tracking} {People} and {Devices} in {Interactive} {Spaces} {Using} {Real}-{Time} {Top}-{View} {Depth}-{Sensing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025562},
	doi = {10.1145/3025453.3025562},
	abstract = {Real-time tracking of people's location, orientation and activities is increasingly important for designing novel ubiquitous computing applications. Top-view camera-based tracking avoids occlusion when tracking people while collaborating, but often requires complex tracking systems and advanced computer vision algorithms. To facilitate the prototyping of ubiquitous computing applications for interactive spaces, we developed EagleSense, a real-time human posture and activity recognition system with a single top-view depth-sensing camera. We contribute our novel algorithm and processing pipeline, including details for calculating silhouette-extremities features and applying gradient tree boosting classifiers for activity recognition optimized for top-view depth sensing. EagleSense provides easy access to the real-time tracking data and includes tools for facilitating the integration into custom applications. We report the results of a technical evaluation with 12 participants and demonstrate the capabilities of EagleSense with application case studies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Chi-Jui and Houben, Steven and Marquardt, Nicolai},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {depth-infrared sensing, phone and tablet recognition, posture and activity recognition, real-time top-view tracking},
	pages = {3929--3942},
}

@inproceedings{wagemakers_interactive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Interactive {Visual} {Calibration} of {Volumetric} {Head}-{Tracked} {3D} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025685},
	doi = {10.1145/3025453.3025685},
	abstract = {Head-tracked 3D displays can provide a compelling 3D effect, but even small inaccuracies in the calibration of the participant's viewpoint to the display can disrupt the 3D illusion. We propose a novel interactive procedure for a participant to easily and accurately calibrate a head-tracked display by visually aligning patterns across a multi-screen display. Head-tracker measurements are then calibrated to these known viewpoints. We conducted a user study to evaluate the effectiveness of different visual patterns and different display shapes. We found that the easiest to align shape was the spherical display and the best calibration pattern was the combination of circles and lines. We performed a quantitative camera-based calibration of a cubic display and found visual calibration outperformed manual tuning and generated viewpoint calibrations accurate to within a degree. Our work removes the usual, burdensome step of manual calibration when using head-tracked displays and paves the way for wider adoption of this inexpensive and effective 3D display technology.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wagemakers, Andrew John and Fafard, Dylan Brodie and Stavness, Ian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {head tracking, calibration, fish tank virtual reality, visual perception},
	pages = {3943--3953},
}

@inproceedings{lindlbauer_changing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Changing the {Appearance} of {Real}-{World} {Objects} {By} {Modifying} {Their} {Surroundings}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025795},
	doi = {10.1145/3025453.3025795},
	abstract = {We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lindlbauer, David and Mueller, Jörg and Alexa, Marc},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {augmented reality, dynamic appearance},
	pages = {3954--3965},
}

@inproceedings{grubert_headphones_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{HeadPhones}: {Ad} {Hoc} {Mobile} {Multi}-{Display} {Environments} through {Head} {Tracking}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025533},
	doi = {10.1145/3025453.3025533},
	abstract = {We present HeadPhones (Headtracking + smartPhones), a novel approach for the spatial registration of multiple mobile devices into an ad hoc multi-display environment. We propose to employ the user's head as external reference frame for the registration of multiple mobile devices into a common coordinate system. Our approach allows for dynamic repositioning of devices during runtime without the need for external infrastructure such as separate cameras or fiducials. Specifically, our only requirements are local network connections and mobile devices with built-in front facing cameras. This way, HeadPhones enables spatially-aware multi-display applications in mobile contexts. A user study and accuracy evaluation indicate the feasibility of our approach.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Grubert, Jens and Kranz, Matthias},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	pages = {3966--3971},
}

@inproceedings{sato_zensei_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Zensei: {Embedded}, {Multi}-{Electrode} {Bioimpedance} {Sensing} for {Implicit}, {Ubiquitous} {User} {Recognition}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025536},
	doi = {10.1145/3025453.3025536},
	abstract = {Interactions and connectivity is increasingly expanding to shared objects and environments, such as furniture, vehicles, lighting, and entertainment systems. For transparent personalization in such contexts, we see an opportunity for embedded recognition, to complement traditional, explicit authentication. We introduce Zensei, an implicit sensing system that leverages bio-sensing, signal processing and machine learning to classify uninstrumented users by their body's electrical properties. Zensei could allow many objects to recognize users. E.g., phones that unlock when held, cars that automatically adjust mirrors and seats, or power tools that restore user settings. We introduce wide-spectrum bioimpedance hardware that measures both amplitude and phase. It extends previous approaches through multi-electrode sensing and high-speed wireless data collection for embedded devices. We implement the sensing in devices and furniture, where unique electrode configurations generate characteristic profiles based on user's unique electrical properties. Finally, we discuss results from a comprehensive longitudinal 22-day data collection experiment with 46 subjects. Our analysis shows promising classification accuracy and low false acceptance rate.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sato, Munehiko and Puri, Rohan S. and Olwal, Alex and Ushigome, Yosuke and Franciszkiewicz, Lukas and Chandra, Deepak and Poupyrev, Ivan and Raskar, Ramesh},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ubiquitous computing, bio-sensing, electrical sensing, embedded devices, implicit sensing, user recognition},
	pages = {3972--3985},
}

@inproceedings{laput_synthetic_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Synthetic {Sensors}: {Towards} {General}-{Purpose} {Sensing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025773},
	doi = {10.1145/3025453.3025773},
	abstract = {The promise of smart environments and the Internet of Things (IoT) relies on robust sensing of diverse environmental facets. Traditional approaches rely on direct and distributed sensing, most often by measuring one particular aspect of an environment with a special purpose sensor. This approach can be costly to deploy, hard to maintain, and aesthetically and socially obtrusive. In this work, we explore the notion of general purpose sensing, wherein a single enhanced sensor can indirectly monitor a large context, without direct instrumentation of objects. Further, through what we call Synthetic Sensors, we can virtualize raw sensor data into actionable feeds, whilst simultaneously mitigating immediate privacy issues. A series of structured, formative studies informed the development of our new sensor hardware and accompanying information architecture. We deployed our system across many months and environments, the results of which show the versatility, accuracy and potential utility of our approach.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Laput, Gierad and Zhang, Yang and Harrison, Chris},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {smart home, internet-of-things, IoT, universal sensor},
	pages = {3986--3999},
}

@inproceedings{xiao_deus_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Deus {EM} {Machina}: {On}-{Touch} {Contextual} {Functionality} for {Smart} {IoT} {Appliances}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025828},
	doi = {10.1145/3025453.3025828},
	abstract = {Homes, offices and many other environments will be increasingly saturated with connected, computational appliances, forming the "Internet of Things" (IoT). At present, most of these devices rely on mechanical inputs, webpages, or smartphone apps for control. However, as IoT devices proliferate, these existing interaction methods will become increasingly cumbersome. Will future smart-home owners have to scroll though pages of apps to select and dim their lights? We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality. To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces. Our user study suggests high accuracy 98.8\% recognition accuracy among 17 appliances. Finally, to underscore the immediate feasibility and utility of our system, we built twelve example applications, including six fully functional end-to-end demonstrations.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Robert and Laput, Gierad and Zhang, Yang and Harrison, Chris},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {internet of things, smart home, IoT, context sensing, EMI, object sensing, recognition, smart appliances},
	pages = {4000--4008},
}

@inproceedings{fdili_alaoui_seeing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Seeing, {Sensing} and {Recognizing} {Laban} {Movement} {Qualities}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025530},
	doi = {10.1145/3025453.3025530},
	abstract = {Human movement has historically been approached as a functional component of interaction within human computer interaction. Yet movement is not only functional, it is also highly expressive. In our research, we explore how movement expertise as articulated in Laban Movement Analysis (LMA) can contribute to the design of computational models of movement's expressive qualities as defined in the framework of Laban Efforts. We include experts in LMA in our design process, in order to select a set of suitable multimodal sensors as well as to compute features that closely correlate to the definitions of Efforts in LMA. Evaluation of our model shows that multimodal data combining positional, dynamic and physiological information allows for a better characterization of Laban Efforts. We conclude with implications for design that illustrate how our methodology and our approach to multimodal capture and recognition of Effort qualities can be integrated to design interactive applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fdili Alaoui, Sarah and Françoise, Jules and Schiphorst, Thecla and Studd, Karen and Bevilacqua, Frederic},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {laban movement analysis, movement observation, movement qualities, movement recognition, movement-based interaction, multimodal measures},
	pages = {4009--4020},
}

@inproceedings{gugenheimer_sharevr_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ShareVR}: {Enabling} {Co}-{Located} {Experiences} for {Virtual} {Reality} between {HMD} and {Non}-{HMD} {Users}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025683},
	doi = {10.1145/3025453.3025683},
	abstract = {Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {asymmetric virtual reality, co-located virtual reality, consumer virtual reality, multi-user virtual reality, sharevr},
	pages = {4021--4033},
}

@inproceedings{hock_carvr_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{CarVR}: {Enabling} {In}-{Car} {Virtual} {Reality} {Entertainment}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025665},
	doi = {10.1145/3025453.3025665},
	abstract = {Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hock, Philipp and Benedikter, Sebastian and Gugenheimer, Jan and Rukzio, Enrico},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, immersion, force-feedback, automotive, entertainment, gaming, motion platform},
	pages = {4034--4044},
}

@inproceedings{dey_effects_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Effects of {Sharing} {Physiological} {States} of {Players} in a {Collaborative} {Virtual} {Reality} {Gameplay}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026028},
	doi = {10.1145/3025453.3026028},
	abstract = {Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dey, Arindam and Piumsomboon, Thammathip and Lee, Youngho and Billinghurst, Mark},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, emotions, collaborative gameplay, empathic computing, physiological sensors, user study.},
	pages = {4045--4056},
}

@inproceedings{sousa_vrrrroom_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{VRRRRoom}: {Virtual} {Reality} for {Radiologists} in the {Reading} {Room}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025566},
	doi = {10.1145/3025453.3025566},
	abstract = {Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sousa, Maurício and Mendes, Daniel and Paulo, Soraia and Matela, Nuno and Jorge, Joaquim and Lopes, Daniel Simões},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, interaction design, medical visualization, multitouch surfaces},
	pages = {4057--4062},
}

@inproceedings{tregillus_handsfree_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Handsfree {Omnidirectional} {VR} {Navigation} {Using} {Head} {Tilt}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025521},
	doi = {10.1145/3025453.3025521},
	abstract = {Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze–which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tregillus, Sam and Al Zayer, Majed and Folmer, Eelke},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {locomotion, virtual reality, games, head-tilt, inertial sensing, mobile vr, simulator-sickness, walking-in-place},
	pages = {4063--4068},
}

@inproceedings{hentschel_rice_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Rice {Today}, {Roti} {Tomorrow}: {Diets} and {Diabetes} in {Urban} {Indian} {Households}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025656},
	doi = {10.1145/3025453.3025656},
	abstract = {In India, where diabetes is a growing concern and approximately 69 million are affected, we investigate the factors that influence diet management, a critical component of living with the disease. Taking the middle-income diabetes-affected household as our unit of analysis, we use a combination of semi-structured interviews and a design probe to understand if and how diets are monitored, tailored, and balanced. We research the various information-seeking behaviors of our participants and their culturally situated approaches to food and eating. Our findings illuminate how contextual nuances shape individuals' beliefs around dealing with diabetes and the ways in which family, friends, and broader social networks influence dietary decisions. We conclude by offering a framework of Learning-Being-Doing to inform the holistic design of technologies for managing diets and diabetes.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hentschel, Jasmine and Sherugar, Samyukta Manjayya and Zhou, Rui and Kameswaran, Vaishnav and Chandwani, Rajesh and Kumar, Neha},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {india, diabetes, diets, qualitative methods},
	pages = {4069--4081},
}

@inproceedings{hwang_monster_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Monster {Appetite}: {Effects} of {Subversive} {Framing} on {Nutritional} {Choices} in a {Digital} {Game} {Environment}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026052},
	doi = {10.1145/3025453.3026052},
	abstract = {Americans' health has reached a dangerous obesity epidemic from overconsumption and unhealthy food choices. In response, persuasive games for health encourage healthier lifestyles typically by providing positive reinforcement for the desired behaviors. However, positive reinforcement is only one of the many possibly effective approaches. We explore two types of message framing in a nutrition game, Monster Appetite (MA). In MA, players' choices of high or low calorie snacks impact visual appearance of their monster avatar. MA utilizes two types of health messages: subversive, which encourages players to make unhealthy choices and focuses on costs, and inoculation, which encourages players to eventually defend healthy choices and focuses on benefits. We test message framing's effect by tracking users' purchasing behavior in our online snack shop, Snackazon. The study showed that when positive messages were embedded in MA mixed with negative visuals through the monster avatars, participants exhibited better snack choices post-gameplay.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hwang, Maria L. and Mamykina, Lena},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {behavior modification, framing, nutritional choices., persuasive games, subversive approach, two-sided inoculation},
	pages = {4082--4096},
}

@inproceedings{burgermaster_role_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Role} of {Explanations} in {Casual} {Observational} {Learning} about {Nutrition}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025874},
	doi = {10.1145/3025453.3025874},
	abstract = {The ubiquity of internet-based nutrition information sharing indicates an opportunity to use social computing platforms to promote nutrition literacy and healthy nutritional choices. We conducted a series of experiments with unpaid volunteers using an online Nutrition Knowledge Test. The test asked participants to examine pairs of photographed meals and identify meals higher in a specific macronutrient (e.g., carbohydrate). After each answer, participants received no feedback on the accuracy of their answers, viewed proportions of peers choosing each response, received correctness feedback from an expert dietitian with or without expert-generated explanations, or received correctness feedback with crowd-generated explanations. The results showed that neither viewing peer responses nor correctness feedback alone improved learning. However, correctness feedback with explanations (i.e., modeling) led to significant learning gains, with no significant difference between explanations generated by experts or peers. This suggests the importance of explanations in social computing-based casual learning about nutrition and the potential for scaling this approach via crowdsourcing.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Burgermaster, Marissa and Gajos, Krzysztof Z. and Davidson, Patricia and Mamykina, Lena},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, casual learning, nutrition literacy, observational learning},
	pages = {4097--4145},
}

@inproceedings{freeman_audible_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Audible {Beacons} and {Wearables} in {Schools}: {Helping} {Young} {Visually} {Impaired} {Children} {Play} and {Move} {Independently}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025518},
	doi = {10.1145/3025453.3025518},
	abstract = {Young children with visual impairments tend to engage less with their surroundings, limiting the benefits from activities at school. We investigated novel ways of using sound from a bracelet, such as speech or familiar noises, to tell children about nearby people, places and activities, to encourage them to engage more during play and help them move independently. We present a series of studies, the first two involving visual impairment educators, that give insight into challenges faced by visually impaired children at school and how sound might help them. We then present a focus group with visually impaired children that gives further insight into the effective use of sound. Our findings reveal novel ways of combining sounds from wearables with sounds from the environment, motivating audible beacons, devices for audio output and proximity estimation. We present scenarios, findings and a design space that show the novel ways such devices could be used alongside wearables to help visually impaired children at school.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Freeman, Euan and Wilson, Graham and Brewster, Stephen and Baud-Bovy, Gabriel and Magnusson, Charlotte and Caltenco, Hector},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, play, visual impairment, wearables, beacons},
	pages = {4146--4157},
}

@inproceedings{abdolrahmani_embracing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Embracing {Errors}: {Examining} {How} {Context} of {Use} {Impacts} {Blind} {Individuals}' {Acceptance} of {Navigation} {Aid} {Errors}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025528},
	doi = {10.1145/3025453.3025528},
	abstract = {Prevention of errors has been an orienting goal within the field of Human-Computer Interaction since its inception, with particular focus on minimizing human errors through appropriate technology design. However, there has been relatively little exploration into how designers can best support users of technologies that will inevitably make errors. We present a mixed-methods study in the domain of navigation technology for visually impaired individuals. We examined how users respond to device errors made in realistic scenarios of use. Contrary to conventional wisdom that usable systems must be error-free, we found that 42\% of errors were acceptable to users. Acceptance of errors depends on error type, building feature, and environmental context. Further, even when a technical error is acceptable to the user, the misguided social responses of others nearby can negatively impact user experience. We conclude with design recommendations that embrace errors while also supporting user management of errors in technical systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Abdolrahmani, Ali and Easley, William and Williams, Michele and Branham, Stacy and Hurst, Amy},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {assistive technology, visual impairments, disability, blindness, navigation, device errors, stigmatization},
	pages = {4158--4169},
}

@inproceedings{zhao_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding {Low} {Vision} {People}'s {Visual} {Perception} on {Commercial} {Augmented} {Reality} {Glasses}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025949},
	doi = {10.1145/3025453.3025949},
	abstract = {People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device's constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people's walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Yuhang and Hu, Michele and Hashash, Shafeka and Azenkot, Shiri},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user study, accessibility, augmented reality, low vision},
	pages = {4170--4181},
}

@inproceedings{leiva_synthesizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Synthesizing {Stroke} {Gestures} {Across} {User} {Populations}: {A} {Case} for {Users} with {Visual} {Impairments}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025906},
	doi = {10.1145/3025453.3025906},
	abstract = {We introduce a new principled method grounded in the Kinematic Theory of Rapid Human Movements to automatically generate synthetic stroke gestures across user populations in order to support ability-based design of gesture user interfaces. Our method is especially useful when the target user population is difficult to sample adequately and, consequently, when there is not enough data to train gesture recognizers to deliver high levels of accuracy. To showcase the relevance and usefulness of our method, we collected gestures from people without visual impairments and successfully synthesized gestures with the articulation characteristics of people with visual impairments. We also show that gesture recognition accuracy improves significantly when using our synthetic gesture samples for training. Our contributions will benefit researchers and practitioners that wish to design gesture user interfaces for people with various abilities by helping them prototype, evaluate, and predict gesture recognition performance without having to expressly recruit and involve people with disabilities in long, time-consuming gesture collection experiments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Leiva, Luis A. and Martín-Albo, Daniel and Vatavu, Radu-Daniel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {bootstrapping, gesture synthesis, kinematic theory, rapid prototyping, sigma-lognormal model, touch gestures},
	pages = {4182--4193},
}

@inproceedings{yeo_investigating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Investigating {Tilt}-{Based} {Gesture} {Keyboard} {Entry} for {Single}-{Handed} {Text} {Entry} on {Large} {Devices}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025520},
	doi = {10.1145/3025453.3025520},
	abstract = {The popularity of mobile devices with large screens is making single-handed interaction difficult. We propose and evaluate a novel design point around a tilt-based text entry technique which supports single handed usage. Our technique is based on the gesture keyboard (shape writing). However, instead of drawing gestures with a finger or stylus, users articulate a gesture by tilting the device. This can be especially useful when the user's other hand is otherwise encumbered or unavailable. We show that novice users achieve an entry rate of 15 words-per-minute (wpm) after minimal practice. A pilot longitudinal study reveals that a single participant achieved an entry rate of 32 wpm after approximate 90 minutes of practice. Our data indicate that tilt-based gesture keyboard entry enables walk-up use and provides a suitable text entry rate for occasional use and can act as a promising alternative to single-handed typing in certain situations.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yeo, Hui-Shyong and Phang, Xiao-Shen and Castellucci, Steven J. and Kristensson, Per Ola and Quigley, Aaron},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {text entry, gesture keyboard, phablets, shape writing, single-handed, tilt},
	pages = {4194--4202},
}

@inproceedings{jokinen_modelling_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Modelling {Learning} of {New} {Keyboard} {Layouts}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025580},
	doi = {10.1145/3025453.3025580},
	abstract = {Predicting how users learn new or changed interfaces is a long-standing objective in HCI research. This paper contributes to understanding of visual search and learning in text entry. With a goal of explaining variance in novices' typing performance that is attributable to visual search, a model was designed to predict how users learn to locate keys on a keyboard: initially relying on visual short-term memory but then transitioning to recall-based search. This allows predicting search times and visual search patterns for completely and partially new layouts. The model complements models of motor performance and learning in text entry by predicting change in visual search patterns over time. Practitioners can use it for estimating how long it takes to reach the desired level of performance with a given layout.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jokinen, Jussi P. P. and Sarcar, Sayan and Oulasvirta, Antti and Silpasuwanchai, Chaklam and Wang, Zhenxin and Ren, Xiangshi},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {keyboard layouts, models of learning, visual search},
	pages = {4203--4215},
}

@inproceedings{yi_word_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Word {Clarity} as a {Metric} in {Sampling} {Keyboard} {Test} {Sets}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025701},
	doi = {10.1145/3025453.3025701},
	abstract = {Test sets play an essential role in evaluating text entry techniques. In this paper, we argue that in addition to the widely adopted metric of bigram representativeness and memorability, word clarity should also be considered as a metric when creating test sets from the target dataset. Word clarity quantifies the extent to which a word is likely to confuse with other words on a keyboard. We formally define word clarity, derive equations calculating it, and both theoretically and empirically show that word clarity has a significant effect on text entry performance: it can yield up to 26.4\% difference in error rate, and 25\% difference in input speed. We later propose a Pareto optimization method for sampling test sets with different sizes, which optimizes the word clarity and bigram representativeness, and memorability of the test set. The obtained test sets are published on the Internet.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yi, Xin and Yu, Chun and Shi, Weinan and Bi, Xiaojun and Shi, Yuanchun},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {phrase set, sampling, text entry evaluation, word clarity},
	pages = {4216--4228},
}

@inproceedings{banovic_quantifying_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Quantifying {Aversion} to {Costly} {Typing} {Errors} in {Expert} {Mobile} {Text} {Entry}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025695},
	doi = {10.1145/3025453.3025695},
	abstract = {Text entry is an increasingly important activity for mobile device users. As a result, increasing text entry speed of expert typists is an important design goal for physical and soft keyboards. Mathematical models that predict text entry speed can help with keyboard design and optimization. Making typing errors when entering text is inevitable. However, current models do not consider how typists themselves reduce the risk of making typing errors (and lower error frequency) by typing more slowly. We demonstrate that users respond to costly typing errors by reducing their typing speed to minimize typing errors. We present a model that estimates the effects of risk aversion to errors on typing speed. We estimate the magnitude of this speed change, and show that disregarding the adjustments to typing speed that expert typists use to reduce typing errors leads to overly optimistic estimates of maximum errorless expert typing speeds.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Banovic, Nikola and Rao, Varun and Saravanan, Abinaya and Dey, Anind K. and Mankoff, Jennifer},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {error cost, speed-accuracy tradeoff, typing speed},
	pages = {4229--4241},
}

@inproceedings{vitale_high_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {High {Costs} and {Small} {Benefits}: {A} {Field} {Study} of {How} {Users} {Experience} {Operating} {System} {Upgrades}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025509},
	doi = {10.1145/3025453.3025509},
	abstract = {Users must manage frequent software and operating system upgrades across multiple computing devices. While current research focuses primarily on the security aspect, we investigate the user's perspective of upgrading software. Our first study (n=65) found that users delay major upgrades by an average of 80 days. We then ran a field study (n=14), beginning with in-depth observations during an operating system upgrade, followed by a four-week diary study. Very few participants prepared for upgrades (e.g., backing up files), and over half had negative reactions to the upgrade process and other changes (e.g., bugs, lost settings, unwanted features). During the upgrade process, waiting times were too long, feedback was confusing or misleading, and few had clear mental models of what was happening. Users almost never mentioned security as a concern or reason for upgrading. By contrast, interviews (n=3) with technical staff responsible for one organization's upgrades focused only on security and licensing, not user interface changes. We conclude with recommendations to improve the user's upgrade experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Vitale, Francesco and McGrenere, Joanna and Tabard, Aurélien and Beaudouin-Lafon, Michel and Mackay, Wendy E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {observational study, qualitative analysis, software upgrades},
	pages = {4242--4253},
}

@inproceedings{eiband_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding {Shoulder} {Surfing} in the {Wild}: {Stories} from {Users} and {Observers}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025636},
	doi = {10.1145/3025453.3025636},
	abstract = {Research has brought forth a variety of authentication systems to mitigate observation attacks. However, there is little work about shoulder surfing situations in the real world. We present the results of a user survey (N=174) in which we investigate actual stories about shoulder surfing on mobile devices from both users and observers. Our analysis indicates that shoulder surfing mainly occurs in an opportunistic, non-malicious way. It usually does not have serious consequences, but evokes negative feelings for both parties, resulting in a variety of coping strategies. Observed data was personal in most cases and ranged from information about interests and hobbies to login data and intimate details about third persons and relationships. Thus, our work contributes evidence for shoulder surfing in the real world and informs implications for the design of privacy protection mechanisms.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Eiband, Malin and Khamis, Mohamed and von Zezschwitz, Emanuel and Hussmann, Heinrich and Alt, Florian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, mobile devices, shoulder surfing},
	pages = {4254--4265},
}

@inproceedings{malloch_fieldward_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Fieldward and {Pathward}: {Dynamic} {Guides} for {Defining} {Your} {Own} {Gestures}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025764},
	doi = {10.1145/3025453.3025764},
	abstract = {Although users accomplish ever more tasks on touch-enabled mobile devices, gesture-based interaction remains limited and almost never customizable by users. Our goal is to help users create gestures that are both personally memorable and reliably recognized by a touch-enabled mobile device. We address these competing requirements with two dynamic guides that use progressive feedforward to interactively visualize the "negative space" of unused gestures: the Pathward technique suggests four possible completions to the current gesture, and the Fieldward technique uses color gradients to reveal optimal directions for creating recognizable gestures. We ran a two-part experiment in which 27 participants each created 42 personal gesture shortcuts on a smartphone, using Pathward, Fieldward or No Feedforward. The Fieldward technique best supported the most common user strategy, i.e. to create a memorable gesture first and then adapt it to be recognized by the system. Users preferred the Fieldward technique to Pathward or No Feedforward, and remembered gestures more easily when using the technique. Dynamic guides can help developers design novel gesture vocabularies and support users as they design custom gestures for mobile applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Malloch, Joseph and Griggio, Carla F. and McGrenere, Joanna and Mackay, Wendy E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gesture recognition, controlled experiment., dynamic guides, personalized gestures, progressive feedforward},
	pages = {4266--4277},
}

@inproceedings{liu_gesture_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gesture {Interfaces}: {Minor} {Change} in {Effort}, {Major} {Impact} on {Appeal}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025513},
	doi = {10.1145/3025453.3025513},
	abstract = {Making gestures easy for imaging systems to reliably recognize often comes at the expense of user effort. But what is the impact of increasing a gesture's effort, even slightly, on user preference? We investigate physical effort, system reliability, and user satisfaction in two experiments. The first explores eight basic command gestures. Participants preferred the less effortful gestures in two of the three easy-difficult gesture pairs when they perceived the difference in effort to be significantly different. The second experiment explores two separate three-dimensional pointing and selection conditions that differ only in the movement distance required to finish the task. In both experiments, there is a significant negative correlation between a gesture's effort and its appeal. The results show the great impact that effort has on a user's willingness to utilize the system. The findings provide evidence that the trade-off between user effort and system reliability must be carefully considered to build an effective gesture interface.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Xiaoxing and Thomas, Geb W.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user experience, usability evaluation, effort-based measurement, gesture interfaces},
	pages = {4278--4283},
}

@inproceedings{sun_movemeant_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{MoveMeant}: {Anonymously} {Building} {Community} {Through} {Shared} {Location} {Histories}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025653},
	doi = {10.1145/3025453.3025653},
	abstract = {Awareness of and connections to a local community are important for building social capital, sharing resources, and providing physical support, but have been elusive to create in dense urban environments. We describe the design and implementation of MoveMeant, a system aimed to increase local community awareness through shared location traces. MoveMeant securely uses anonymized location data generated automatically by mobile devices to display aggregate, community-level location data. We report findings from interviews with residents in the Bronx, New York City who participated in a deployment of MoveMeant over a 6-week period. Our findings show that people use the anonymous information to make judgments about the people and places in their community, while opting to reveal their identity for third places where there is an opportunity to connect socially.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Emily and McLachlan, Ross and Naaman, Mor},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {local community, location traces, movemeant, third places},
	pages = {4284--4289},
}

@inproceedings{maezawa_muens_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{MuEns}: {A} {Multimodal} {Human}-{Machine} {Music} {Ensemble} for {Live} {Concert} {Performance}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025505},
	doi = {10.1145/3025453.3025505},
	abstract = {Musical ensemble between human musicians and computers is a challenging task. We achieve this with a concert-quality synchronization using machine learning. Our system recognizes the position in a given song from the human performance using the microphone and camera inputs, and responds in real-time with audio and visual feedback as a music ensemble. We address three crucial requirements in a musical ensemble system. First, our system interacts with human players through both audio and visual cues, the conventional modes of coordination for musicians. Second, our system synchronizes with human performances while retaining its intended musical expression. Third, our system prevents failures during a concert due to bad tracking, by displaying an internal confidence measure and allowing a backstage human operator to "intervene" if the system is unconfident. We show the feasibility of the system with several experiments, including a professional concert.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Maezawa, Akira and Yamamoto, Kazuhiko},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {machine learning, multimodal interaction, human-machine music ensemble, live concert system, score following},
	pages = {4290--4301},
}

@inproceedings{greenhalgh_playing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Playing {Fast} and {Loose} with {Music} {Recognition}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025900},
	doi = {10.1145/3025453.3025900},
	abstract = {We report lessons from iteratively developing a music recognition system to enable a wide range of musicians to embed musical codes into their typical performance practice. The musician composes fragments of music that can be played back with varying levels of embellishment, disguise and looseness to trigger digital interactions. We collaborated with twenty-three musicians, spanning professionals to amateurs and working with a variety of instruments. We chart the rapid evolution of the system to meet their needs as they strove to integrate music recognition technology into their performance practice, introducing multiple features to enable them to trade-off reliability with musical expression. Collectively, these support the idea of deliberately introducing "looseness" into interactive systems by addressing the three key challenges of control, feedback and attunement, and highlight the potential role for written notations in other recognition-based systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Greenhalgh, Chris and Benford, Steve and Hazzard, Adrian and Chamberlain, Alan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {notation, performance, attunement, casual interactions, H-metaphor, looseness, music recognition, sensing systems},
	pages = {4302--4313},
}

@inproceedings{ochiai_holographic_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Holographic {Whisper}: {Rendering} {Audible} {Sound} {Spots} in {Three}-{Dimensional} {Space} by {Focusing} {Ultrasonic} {Waves}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025989},
	doi = {10.1145/3025453.3025989},
	abstract = {We propose a novel method of spatial audio rendering using ultrasound. An ultrasonic phased array generates one or more focal points in air, and they act as point sources of audible sound when the ultrasound waves are modulated. Our sound-point loudspeaker has two major advantages over conventional ultrasound-based sound-beam (superdirectional) loudspeakers. The higher audience selectivity means that our sound-point loudspeaker can deliver sound to the ears of the target person, whereas a sound-beam loudspeaker delivers sound to not only the target person but also other persons standing in the same direction. The other advantage is lower exposure to ultrasound; while an audible sound beam travels along an ultrasonic beam in a soundbeam loudspeaker, audible sound can be heard along the direction perpendicular to the ultrasonic beam in our soundpoint loudspeaker. This paper reports the principles of our sound-point loudspeaker, prototype construction, evaluation, and applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ochiai, Yoichi and Hoshi, Takayuki and Suzuki, Ippei},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ultrasound, aerial interaction, phased-array focusing, point source, self-demodulation effect, spatial sound control},
	pages = {4314--4325},
}

@inproceedings{lyu_ensewing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{EnseWing}: {Creating} an {Instrumental} {Ensemble} {Playing} {Experience} for {Children} with {Limited} {Music} {Training}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025583},
	doi = {10.1145/3025453.3025583},
	abstract = {While instrumental ensemble playing can benefit children's music education and collaboration skill development, it requires extensive training on music and instruments, which many school children lack. To help children with limited music training experience instrumental ensemble playing, we created EnseWing, an interactive system that offers such an experience. In this paper, we report the design of the EnseWing experience and a two-month field study. Our results show that EnseWing preserves the music and ensemble skills from traditional instrumental ensemble and provides more collaboration opportunities for children.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lyu, Fei and Tian, Feng and Feng, Wenxin and Cao, Xiang and Zhang, Xiaolong (Luke) and Dai, Guozhong and Wang, Hongan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, collaboration, instrumental ensemble, music},
	pages = {4326--4330},
}

@inproceedings{lampinen_market_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Market {Design} for {HCI}: {Successes} and {Failures} of {Peer}-to-{Peer} {Exchange} {Platforms}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025515},
	doi = {10.1145/3025453.3025515},
	abstract = {This paper explores an HCI approach to designing markets, with a primary focus on peer-to peer exchange platforms. We draw on\&nbsp;recent work in economics that has documented how markets function, how they can be evaluated, and what can be done to fix them when they fail. We introduce five key concepts from market design: thickness, congestion, stability, safety, and repugnance. These lend HCI an analytic vocabulary for understanding why markets may succeed or struggle. Building on prior empirical work, we apply these concepts to compare two well-known network hospitality platforms, Couchsurfing and Airbnb. As a second illustrative case, we use market design to shed light on the challenges experienced by smaller-scale peer-to-peer marketplaces for lending, renting, and selling physical goods. To conclude, we discuss how this kind of analysis can make conceptual, evaluative, and generative contributions to the study and design of exchange platforms and other socio-technical systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lampinen, Airi and Brown, Barry},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {airbnb, couchsurfing, market design, matching market, platform economy, sharetribe, sharing economy},
	pages = {4331--4343},
}

@inproceedings{moser_community_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Community {Commerce}: {Facilitating} {Trust} in {Mom}-to-{Mom} {Sale} {Groups} on {Facebook}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025550},
	doi = {10.1145/3025453.3025550},
	abstract = {Consumers are turning to Facebook Groups to buy and sell with strangers in their local communities. This trend is counter-intuitive given Facebook's lack of conventional e-commerce features, such as sophisticated search engines and reputation systems. We interviewed 18 members of two Mom-to-Mom Facebook sale groups. Despite a lack of commerce tools, members perceived sale groups as an easy-to-use way to quickly and conveniently buy and sell. Most important to members was that the groups felt safe and trustworthy. Drawing on these insights, we contribute a novel framing, community commerce, which explains the trust mechanisms that enable transactions between strangers in some groups. Community commerce fosters trust through (a) exclusive membership to a closed group, (b) regulation and sanctioning of behavior at the admin, member, and group level, and (c) a shared group identity or perceived similarity (though, surprisingly, not through social bonding). We discuss how community commerce affords unique and sometimes superior trust assurances and propose design implications for platforms hoping to foster trust between members who buy, sell, or share amongst themselves.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Moser, Carol and Resnick, Paul and Schoenebeck, Sarita},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {facebook, trust, online communities, community commerce, consumer-to-consumer, e-commerce},
	pages = {4344--4357},
}

@inproceedings{moser_no_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {No {Such} {Thing} as {Too} {Much} {Chocolate}: {Evidence} {Against} {Choice} {Overload} in {E}-{Commerce}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025778},
	doi = {10.1145/3025453.3025778},
	abstract = {E-commerce designers must decide how many products to display at one time. Choice overload research has demonstrated the surprising finding that more choice is not necessarily better?selecting from larger choice sets can be more cognitively demanding and can result in lower levels of choice satisfaction. This research tests the choice overload effect in an e-commerce context and explores how the choice overload effect is influenced by an individual's tendency to maximize or satisfice decisions. We conducted an online experiment with 611 participants randomly assigned to select a gourmet chocolate bar from either 12, 24, 40, 50, 60, or 72 different options. Consistent with prior work, we find that maximizers are less satisfied with their product choice than satisficers. However, using Bayesian analysis, we find that it's unlikely that choice set size affects choice satisfaction by much, if at all. We discuss why the decision-making process may be different in e-commerce contexts than the physical settings used in previous choice overload experiments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Moser, Carol and Phelan, Chanda and Resnick, Paul and Schoenebeck, Sarita Y. and Reinecke, Katharina},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {experiment, e-commerce, choice overload, maximizing},
	pages = {4358--4369},
}

@inproceedings{bellotti_why_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Why {Users} {Disintermediate} {Peer}-to-{Peer} {Marketplaces}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025815},
	doi = {10.1145/3025453.3025815},
	abstract = {This paper reports on a study of the prevalence of and possible reasons for peer-to-peer transaction marketplace (P2PM) users turning to out-of-market (OOM) transactions after finding transaction partners within a P2P system. We surveyed 97 P2PM users and interviewed 22 of 58 who reported going OOM. We did not find any evidence of predisposing personality factors for OOM activity; instead, it seems to be a rational response to circumstances, with a variety of situationally rational motivations at play, such as liking the transaction partner and trusting that good quality repeat transactions will occur in the future.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bellotti, Victoria and Turner, Dan and Demkova, Kamila and Ambard, Alexander and Waterman, Amanda},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {disincentives, motivations, peer-to-peer marketplaces},
	pages = {4370--4382},
}

@inproceedings{tokuda_mistform_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{MistForm}: {Adaptive} {Shape} {Changing} {Fog} {Screens}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025608},
	doi = {10.1145/3025453.3025608},
	abstract = {We present MistForm, a shape changing fog display that can support one or two users interacting with either 2D or 3D content. Mistform combines affordances from both shape changing interfaces and mid-air displays. For example, a concave display can maintain content in comfortable reach for a single user, while a convex shape can support several users engaged on individual tasks. MistForm also enables unique interaction possibilities by exploiting the synergies between shape changing interfaces and mid-air fog displays. For instance, moving the screen will affect the brightness and blurriness of the screen at specific locations around the display, creating spaces with similar (collaboration) or different visibility (personalized content). We describe the design of MistForm and analyse its inherent challenges, such as image distortion and uneven brightness on dynamic curved surfaces. We provide a machine learning approach to characterize the shape of the screen and a rendering algorithm to remove aberrations. We finally explore novel interactive possibilities and reflect on their potential and limitations.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tokuda, Yutaka and Norasikin, Mohd Adili and Subramanian, Sriram and Martinez Plasencia, Diego},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {3d displays, non-solid diffusers., shape changing displays},
	pages = {4383--4395},
}

@inproceedings{berard_object_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Object} {Inside}: {Assessing} {3D} {Examination} with a {Spherical} {Handheld} {Perspective}-{Corrected} {Display}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025806},
	doi = {10.1145/3025453.3025806},
	abstract = {Handheld Perspective Corrected Displays (HPCDs) can create the feeling of holding a virtual 3D object. They offer a direct interaction that is isomorphic to the manipulation of physical objects. This illusion depends on the ability to provide a natural visuomotor coupling. High performances systems are thus required to evaluate the fundamental merits of HPCDs. We built a spherical HPCD using external projection. The system offers a lightweight wireless seamless display with head-coupled stereo, robust tracking, and low latency. We compared users' performances with this HPCD and two other interactions that used a fixed planar display and either a touchpad or the spherical display as an indirect input. The task involved the inspection of complex virtual 3D puzzles. Physical puzzles were also tested as references. Contrary to expectations, all virtual interactions were found to be more efficient than a more "natural" physical puzzle. The HPCD yielded lower performances than the touchpad. This study indicates that the object examination task did not benefit from the accurate and precise rotations offered by the HPCD, but benefited from the high C/D gain of the touchpad.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Berard, Francois and Louis, Thibault},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {evaluation, depth perception, 3d display, handheld perspective corrected display (hpcd), isomorphic rotation, object examination},
	pages = {4396--4404},
}

@inproceedings{serrano_visual_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Visual {Composition} of {Graphical} {Elements} on {Non}-{Rectangular} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025677},
	doi = {10.1145/3025453.3025677},
	abstract = {Graphical user interfaces are composed of varying elements (text, images, etc.) whose visual arrangement has been relatively well established in the context of rectangular interfaces. The advent of non-rectangular displays questions this knowledge. In this paper we study how traditional content layouts can be adapted to fit different non-rectangular displays. We performed a first qualitative study where graphic designers fitted text and images into different non-rectangular displays. From the analysis of their output we generalize and adapt ten composition principles that have been proposed in the literature for rectangular displays. We evaluate the revised principles through a paired comparison questionnaire where 57 participants compared pairs of layouts. Using the Bradley-Terry-Luce model to analyze our data we show that some results contradict current conventions on visual design for rectangular displays. We then extracted the most interesting cases and conducted a follow up study with additional shapes to investigate how the principles generalize. From these results we propose a set of guidelines for designing visual content for non-rectangular displays.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Serrano, Marcos and Roudaut, Anne and Irani, Pourang},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {freeform display, non-rectangular display, visual design guidelines},
	pages = {4405--4416},
}

@inproceedings{carrascal_effects_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Effects of {Tactile} {Feedback} on the {Perception} of {Virtual} {Shapes} on {Non}-{Planar} {DisplayObjects}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025488},
	doi = {10.1145/3025453.3025488},
	abstract = {In this paper, we report on a study investigating a novel haptic illusion for altering the perception of 3D shapes using a non-planar screen and vibrotactile friction. In our study, we presented an image of a rectangular prism on a cylindrical and a flat display. Participants were asked to move their index finger horizontally along the surface of the displays towards the edge of the rectangular prism. Participants were asked whether they were experiencing a flat, cylindrical or rectangular shape. In one condition, a vibrotactile stimulus simulated increasing friction towards the visible edge of the rectangular prism, with a sudden drop-off when this edge was crossed by the finger. Results suggest that presenting an image of a rectangular prism, and applying vibrotactile friction, particularly on a cylindrical display, significantly increased participant ratings stating that they were experiencing a physical rectangular shape.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Carrascal, Juan Pablo and Vertegaal, Roel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {vibrotactile feedback, displayobjects, organic user interfaces, shaped displays},
	pages = {4417--4423},
}

@inproceedings{alakarppa_breathscreen_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{BreathScreen}: {Design} and {Evaluation} of an {Ephemeral} {UI}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025973},
	doi = {10.1145/3025453.3025973},
	abstract = {We present BreathScreen, a concept where clouds created by breathing are used as a projection surface for a picoprojector, creating an ephemeral user interface. In cold weather conditions the clouds are created naturally by warm breath condensing, but in other conditions an electric vaporizer may be used. We present an initial evaluation of the concept in a user study (n = 8), utilising a vaporizer-based BreathScreen prototype. The concept was positively received by study participants as a natural, hands-free interface and considered magical and aesthetically beautiful. Additionally, we provide guidance on the quantity of content that may be displayed on a BreathScreen, which is limited both by the length of a human breath and the contrast of the system.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alakärppä, Ismo and Jaakkola, Elisa and Colley, Ashley and Häkkilä, Jonna},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ephemeral user interfaces, evanescent screen, fog screen, picoprojector},
	pages = {4424--4429},
}

@inproceedings{hale_foreign-language_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Foreign-{Language} {Reviews}: {Help} or {Hindrance}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025575},
	doi = {10.1145/3025453.3025575},
	abstract = {The number and quality of user reviews greatly affects consumer purchasing decisions. While reviews in all languages are increasing, it is still often the case (especially for non-English speakers) that there are only a few reviews in a person's first language. Using an online experiment, we examine the value that potential purchasers receive from interfaces showing additional reviews in a second language. The results paint a complicated picture with both positive and negative reactions to the inclusion of foreign-language reviews. Roughly 26-28\% of subjects clicked to see translations of the foreign-language content when given the opportunity, and those who did so were more likely to select the product with foreign-language reviews than those who did not.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hale, Scott A. and Eleta, Irene},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {experiment, e-commerce, bilingualism, internationalization and localization, multilingualism, product reviews, user-generated content},
	pages = {4430--4442},
}

@inproceedings{hupfeld_getting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Getting {Something} for {Nothing}? {A} {User}-{Centric} {Perspective} on {Loyalty} {Card} {Schemes}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026017},
	doi = {10.1145/3025453.3026017},
	abstract = {Loyalty cards are a form of tracking and recording technology (TRT) that enables retailers to collect data about their customers' demographic and purchase behaviours. As recompense for sharing their data consumers receive 'loyalty points' which they can redeem for exclusive discounts and rewards. The design of loyalty schemes, and TRTs more generally, plays a key role in defining the economic terms of that exchange, and ultimately the economic value of personal data. In this paper we present findings from an interview study with 12 loyalty cardholders in the UK explicating the ways in which they create (and lose) value through the everyday practice of shopping with loyalty cards and the orientations associated with them. Based on our findings we suggest cardholders are less concerned with the protection of their privacy than with leveraging its value, only some of which was economic. We provide design guidelines for TRTs that may enable consumers to derive greater value from the data they produce and share.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hupfeld, Annika and Speed, Chris},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {interview study, personal data, digital economy, loyalty cards, retail shopping, tracking and recording technologies},
	pages = {4443--4453},
}

@inproceedings{foong_online_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Online {Feedback} {Exchange}: {A} {Framework} for {Understanding} the {Socio}-{Psychological} {Factors}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025791},
	doi = {10.1145/3025453.3025791},
	abstract = {To meet the demand for authentic, timely, and affordable feedback, researchers have explored technologies to connect designers with feedback providers online. While researchers have implemented mechanisms to improve the content of feedback, most systems for online feedback exchange do not support an end-to-end cycle, from help-seeking to sense-making to action. Building on extant literature in learning sciences, design, organizational behavior, and online communities, we propose a conceptual framework to highlight critical processes that affect online feedback exchange. We contribute research questions for future feedback systems and argue that online feedback systems must be able to support designers through five activities that happen before, during, and after the feedback exchange. Our framework suggests that systems should address broader socio-psychological factors, such as how intent should be communicated online, how dialogue can support the interpretation of feedback, and how to balance the tradeoffs of anonymizing feedback providers.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Foong, Eureka and Dow, Steven P. and Bailey, Brian P. and Gerber, Elizabeth M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design methods, crowdsourcing, online communities, social networks, feedback, online feedback exchange},
	pages = {4454--4467},
}

@inproceedings{pope_geometry_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Geometry} of {Storytelling}: {Theatrical} {Use} of {Space} for 360-{Degree} {Videos} and {Virtual} {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025581},
	doi = {10.1145/3025453.3025581},
	abstract = {360-degree filming and head-mounted displays (HMDs) give recorded media a new sense of space. Theatre practitioners' expertise in manipulating spatial interactions has much to contribute to immersive recorded content. Four theatre directors led teams of three actors to stage the same scene for both immersive theatre and for 360-degree filming. Each team was recorded performing the scene at least six times, three in each condition, to extract actors' coordinates. This study establishes how to quantify theatre practitioners' use of spatial interactions and examines the spatial adaptations made when transferring these relationships to 360-degree filming.Staging for a 360-degree camera compared to staging for an audience member had shorter distances from the camera and between performers, along with fewer instances of the camera being in the middle of the action. Across all groups, interpersonal distance between characters and between the audience/camera dropped at the end of the scene when the characters come together as a team, suggesting that elements of Proxemics may be applicable to narrative performance.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pope, Vanessa C. and Dawes, Robert and Schweiger, Florian and Sheikh, Alia},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, head-mounted display, narrative, workflow, performance, 360-degree video, cinematic vr, interpersonal space, theatre},
	pages = {4468--4478},
}

@inproceedings{yu_tap_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Tap, {Dwell} or {Gesture}? {Exploring} {Head}-{Based} {Text} {Entry} {Techniques} for {HMDs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025964},
	doi = {10.1145/3025453.3025964},
	abstract = {Despite the increasing popularity of head mounted displays (HMDs), development of efficient text entry methods on these devices has remained under explored. In this paper, we investigate the feasibility of head-based text entry for HMDs, by which, the user controls a pointer on a virtual keyboard using head rotation. Specifically, we investigate three techniques: TapType, DwellType, and GestureType. Users of TapType select a letter by pointing to it and tapping a button. Users of DwellType select a letter by pointing to it and dwelling over it for a period of time. Users of GestureType perform word-level input using a gesture typing style. Two lab studies were conducted. In the first study, users typed 10.59 WPM, 15.58 WPM, and 19.04 WPM with DwellType, TapType, and GestureType, respectively. Users subjectively felt that all three of the techniques were easy to learn and considered the induced fatigue to be acceptable. In the second study, we further investigated GestureType. We improved its gesture-word recognition algorithm by incorporating the head movement pattern obtained from the first study. This resulted in users reaching 24.73 WPM after 60 minutes of training. Based on these results, we argue that head-based text entry is feasible and practical on HMDs, and deserves more attention.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yu, Chun and Gu, Yizheng and Yang, Zhican and Yi, Xin and Luo, Hengliang and Shi, Yuanchun},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gesture keyboard, dwelling, head-based text entry, hmd},
	pages = {4479--4488},
}

@inproceedings{oogjes_videos_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Videos of {Things}: {Speculating} on, {Anticipating} and {Synthesizing} {Technological} {Mediations}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025748},
	doi = {10.1145/3025453.3025748},
	abstract = {In this paper we present Videos of Things: videos that portray the mediated, lived world of computational artifacts informed by postphenomenology. In a post-phenomenological understanding, things and us are interdependent in that they mutually shape each other. And as a whole, technology or designed things mediate the relations between our world and us. This can be a challenge for designers. Through the making of design videos, we explored narrative strategies for creating stories featuring technological mediation. These include humanness, patterns in time, and non-human ensembles. We reflect on how the videos at different stages of the design process have helped to a) speculate on technological mediated relationships, b) synthesize and reflect on qualitative data on technological mediation and c) anticipate technological mediation. The paper contributes different narrative strategies for design videos and the role these videos can play within a design process aimed at elaborating the mediated qualities of technologies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oogjes, Doenja and Wakkary, Ron},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design videos, material speculation, mediation theory, post-phenomenology, speculative design.},
	pages = {4489--4500},
}

@inproceedings{tang_watching_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Watching 360° {Videos} {Together}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025519},
	doi = {10.1145/3025453.3025519},
	abstract = {360° videos are made using omnidirectional cameras that capture a sphere around the camera. Viewers get an immersive experience by freely changing their field of view around the sphere. The problem is that current interfaces are designed for a single user, and we do not know what challenges groups of people will have when viewing these videos together. We report on the findings of a study where 16 pairs of participants watched 360° videos together in a "guided tour" scenario. Our findings indicate that while participants enjoyed the ability to view the scene independently, this caused challenges establishing joint references, leading to breakdowns in conversation. We conclude by discussing how gaze awareness widgets and gesturing mechanisms may support smoother collaborative interaction around collaborative viewing of 360° videos.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Anthony and Fakourfar, Omid},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {shared experience, 360° videos, omnidirectional videos},
	pages = {4501--4506},
}

@inproceedings{le_revisiting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Revisiting {The} {American} {Voter} on {Twitter}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025543},
	doi = {10.1145/3025453.3025543},
	abstract = {The American Voter - a seminal work in political science - uncovered the multifaceted nature of voting behavior which has been corroborated in electoral research for decades since. In this paper, we leverage The American Voter as an analysis framework in the realm of computational political science, employing the factors of party, personality, and policy to structure the analysis of public discourse on online social media during the 2016 U.S. presidential primaries. Our analysis of 50 million tweets reveals the continuing importance of these three factors; our understanding is also enriched by the application of sentiment analysis techniques. The overwhelmingly negative sentiment of conversations surrounding 10 major presidential candidates reveals more "crosstalk" from Democratic leaning users towards Republican candidates, and less vice-versa. We uncover the lack of moderation as the most discussed personality dimension during this campaign season, as the political field becomes more extreme - Clinton and Rubio are perceived as moderate, while Trump, Sanders, and Cruz are not. While the most discussed issues are foreign policy and immigration, Republicans tweet more about abortion than Democrats who tweet more about gay rights than Republicans. Finally, we illustrate the importance of multifaceted political discourse analysis by applying regression to quantify the impact of party, personality, and policy on national polls.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Le, Huyen T. and Boynton, G. R. and Mejova, Yelena and Shafiq, Zubair and Srinivasan, Padmini},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {twitter, sentiment analysis, election, political affiliation},
	pages = {4507--4519},
}

@inproceedings{gui_managing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Managing {Uncertainty}: {Using} {Social} {Media} for {Risk} {Assessment} during a {Public} {Health} {Crisis}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025891},
	doi = {10.1145/3025453.3025891},
	abstract = {Recently, diseases like H1N1 influenza, Ebola, and Zika virus have created severe crises, requiring public resources and personal behavior adaptation. Crisis Informatics literature examines interconnections of people, organizations, and IT during crisis events. However, how people use technology to cope with disease crises (outbreaks, epidemics, and pandemics) remains understudied. We investigate how individuals used social media in response to the outbreak of Zika, focusing on travel-related decisions. We found that extreme uncertainty and ambiguity characterized the Zika virus crisis. To cope, people turned to social media for information gathering and social learning geared towards personal risk assessment and modifying decisions when dealing with partial and conflicting information about Zika. In particular, individuals sought local information and used socially informed logical reasoning to deduce the risk at a specific locale. We conclude with implications for designing information systems to support individual risk assessment and decision-making when faced with uncertainty and ambiguity during public health crises.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gui, Xinning and Kou, Yubo and Pine, Kathleen H. and Chen, Yunan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {decision-making, social media, crisis informatics, online forums, public health, risk assessment, uncertainty reduction, Zika virus},
	pages = {4520--4533},
}

@inproceedings{nelimarkka_theory-driven_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Theory-{Driven} {Collocated} {CMC}: {A} {Study} of {Collocated} {Mediated} {Interaction} as a {Public} {Sphere}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025885},
	doi = {10.1145/3025453.3025885},
	abstract = {Computer-mediated communication (CMC) tools are used to increase social interaction in collocated settings. Recent research has been primarily constructive (oriented to building of systems) or phenomenon-driven (serving attempts to understand interactions in collocated CMC). The paper contributes a theory-driven approach and examines collocated CMC as a Habermasean "public sphere": a space that supports inclusive, civil, and rational discussion. An in-the-wild experimental study comparing CMC with face-to-face (F2F) communication enabled ascertaining that CMC is more inclusive than F2F communication. Respectfulness levels did not differ but were established differently: via collective construction of a common narrative in F2F and through quick reactions in CMC. Similarly, while rationality figures were on a par, F2F communication allowed participants to justify their claims better. The article discusses how a theory-based approach can strengthen phenomenon-driven research with new conceptual frames and measurement tools, and steer constructive research with a normative framework.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nelimarkka, Matti and Salovaara, Antti and Semaan, Bryan and Jacucci, Giulio},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {collocated computer mediated communication, deliberative democracy, public sphere},
	pages = {4534--4547},
}

@inproceedings{plank_is_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Is {Two} {Enough}? ! {Studying} {Benefits}, {Barriers}, and {Biases} of {Multi}-{Tablet} {Use} for {Collaborative} {Visualization}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025537},
	doi = {10.1145/3025453.3025537},
	abstract = {A sizable part of HCI research on cross-device interaction is driven by the vision of users conducting complex knowledge work seamlessly across multiple mobile devices. This is based on the Weiserian assumption that people will be inclined to distribute their work across multiple “pads' if such are available. We observed that this is not the reality today, even when devices were in abundance. We present a study with 24 participants in 12 dyads completing a collaborative visualization task with up to six tablets. They could choose between three different visualization types to answer questions about economic data. Tasks were designed to afford simultaneous use of tablets, either with linked or independent views. We found that users typically utilized only one tablet per user. A quantitative and qualitative analysis revealed a “legacy bias' that introduced barriers for using more tablets and reduced the overall benefit of multi-device visualization.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Plank, Thomas and Jetter, Hans-Christian and Rädle, Roman and Klokmose, Clemens N. and Luger, Thomas and Reiterer, Harald},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {information visualization, cross-device interaction, group work, multiple coordinated views, tablets},
	pages = {4548--4560},
}

@inproceedings{kim_letspic_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{LetsPic}: {Supporting} {In}-{Situ} {Collaborative} {Photography} over a {Large} {Physical} {Space}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025693},
	doi = {10.1145/3025453.3025693},
	abstract = {Recent advances in mobile computing technology have made it increasingly common for collocated users to perform collaborative photography over a large physical space in various group activity scenarios such as field trips, site surveys, and group tours. Unlike traditional collocated interactions in a shared physical space, we find that mobility and group dynamics make awareness of group activities over a large physical space very challenging. In this work, we design LetsPic, a group photoware that supports group awareness for in-situ collaborative photography over the large physical space. We have iteratively built the app and performed user studies in site survey and group tour scenarios (n = 31, n = 24). Our results confirmed that LetsPic effectively promotes group awareness, facilitates group coordination, and encourages collaboration in both scenarios. We discuss practical design implications based on our findings.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Auk and Kang, Sungjoon and Lee, Uichin},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {photography, awareness, collaborative photowork, collocated interaction, photoware},
	pages = {4561--4573},
}

@inproceedings{nebeling_xdbrowser_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{XDBrowser} 2.0: {Semi}-{Automatic} {Generation} of {Cross}-{Device} {Interfaces}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025547},
	doi = {10.1145/3025453.3025547},
	abstract = {Several recent studies have highlighted the need to support parallel usage of multiple devices for cross-device use. Yet, most interfaces today are still designed for single-device use and require re-authoring to enable cross-device interaction. This paper presents two studies to inform the design of a new web browser with support for semi-automatic generation of cross-device interfaces. Based on the results of a recent study in which users manually customized web pages for cross-device use, our first study elicits from users how they might want to trigger popular cross-device patterns to transform single-device designs with relatively little effort. Our second study then examines how the emerging design patterns could be applied to the Alexa top 50 sites from 10 different genres. Based on these studies, we design semi-automatic techniques for page segmentation and distribution between multiple devices that can work on many existing web sites and require only minimal user input to switch between different cross-device designs. Finally, we discuss possible extensions to the Chrome web browser to make the techniques available for a wide range of desktop, mobile, and wearable devices, and successfully test them on popular web sites.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nebeling, Michael},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {cross-device interaction, distributed user interfaces, semi-automatic page segmentation},
	pages = {4574--4584},
}

@inproceedings{dombrowski_low-wage_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Low-{Wage} {Precarious} {Workers}' {Sociotechnical} {Practices} {Working} {Towards} {Addressing} {Wage} {Theft}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025633},
	doi = {10.1145/3025453.3025633},
	abstract = {Nearly 40 million workers in the USA, a third of the working population, are low-wage, meaning they make less than \$11.65 per hour. These workers face the pervasive and detrimental challenge of wage violations, also known as wage theft, which is any illegal activity by an employer that denies benefits or wages to employees. We interviewed 24 low-wage workers who experienced wage theft and sought justice about their work practices, challenges, and information technology usage. Based on these interviews, we identify three key sociotechnical practices these workers engaged in to address their wage theft: 1) identifying wage and payment discrepancies; 2) tracking and documenting work; and 3) pursuing wage claims. Seeking to leverage HCI research to interrupt uneven social, economic, and information relations in the low-wage workplace, we ultimately reflect on the possibility and limits of several key design recommendations.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dombrowski, Lynn and Alvarado Garcia, Adriana and Despard, Jessica},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {workplace studies, labor, low-wage workers, precarious workers, technologies in the workplace, wage and hourly violations, wage disputes, wage theft, work practice},
	pages = {4585--4598},
}

@inproceedings{alkhatib_examining_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Examining {Crowd} {Work} and {Gig} {Work} {Through} {The} {Historical} {Lens} of {Piecework}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025974},
	doi = {10.1145/3025453.3025974},
	abstract = {The internet is empowering the rise of crowd work, gig work, and other forms of on-demand labor. A large and growing body of scholarship has attempted to predict the socio-technical outcomes of this shift, especially addressing three questions: 1) What are the complexity limits of on-demand work?, 2) How far can work be decomposed into smaller microtasks?, and 3) What will work and the place of work look like for workers? In this paper, we look to the historical scholarship on piecework — a similar trend of work decomposition, distribution, and payment that was popular at the turn of the 20th century — to understand how these questions might play out with modern on-demand work. We identify the mechanisms that enabled and limited piecework historically, and identify whether on-demand work faces the same pitfalls or might differentiate itself. This approach introduces theoretical grounding that can help address some of the most persistent questions in crowd work, and suggests design interventions that learn from history rather than repeat it.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alkhatib, Ali and Bernstein, Michael S. and Levi, Margaret},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	pages = {4599--4616},
}

@inproceedings{huang_leveraging_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Leveraging {Complementary} {Contributions} of {Different} {Workers} for {Efficient} {Crowdsourcing} of {Video} {Captions}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026032},
	doi = {10.1145/3025453.3026032},
	abstract = {Hearing-impaired people and non-native speakers rely on captions for access to video content, yet most videos remain uncaptioned or have machine-generated captions with high error rates. In this paper, we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos. We consider four stakeholder groups as our source of crowd workers: (i) individuals with hearing impairments, (ii) second-language speakers with low proficiency, (iii) second-language speakers with high proficiency, and (iv) native speakers. Each group has different abilities and incentives, which our workflow leverages. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Yun and Huang, Yifeng and Xue, Na and Bigham, Jeffrey P.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, complementary contributions, video caption},
	pages = {4617--4626},
}

@inproceedings{krause_critique_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Critique {Style} {Guide}: {Improving} {Crowdsourced} {Design} {Feedback} with a {Natural} {Language} {Model}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025883},
	doi = {10.1145/3025453.3025883},
	abstract = {Designers are increasingly leveraging online crowds; yet, online contributors may lack the expertise, context, and sensitivity to provide effective critique. Rubrics help feedback providers but require domain experts to write them and may not generalize across design domains. This paper introduces and tests a novel semi-automated method to support feedback providers by analyzing feedback language. In our first study, 52 students from two design courses created design solutions and received feedback from 176 online providers. Instructors, students, and crowd contributors rated the helpfulness of each feedback response. From this data, an algorithm extracted a set of natural language features (e.g., specificity, sentiment etc.) that correlated with the ratings. The features accurately predicted the ratings and remained stable across different raters and design solutions. Based on these features, we produced a critique style guide with feedback examples - automatically selected for each feature - to help providers revise their feedback through self-assessment. In a second study, we tested the validity of the guide through a between-subjects experiment (n=50). Providers wrote feedback on design solutions with or without the guide. Providers generated feedback with higher perceived helpfulness when using our style-based guidance.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Krause, Markus and Garncarz, Tom and Song, JiaoJiao and Gerber, Elizabeth M. and Bailey, Brian P. and Dow, Steven P.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {machine learning, artificial intelligence, online education, feedback, natural language model, peer feedback, review},
	pages = {4627--4639},
}

@inproceedings{buschek_probui_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ProbUI}: {Generalising} {Touch} {Target} {Representations} to {Enable} {Declarative} {Gesture} {Definition} for {Probabilistic} {GUIs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025502},
	doi = {10.1145/3025453.3025502},
	abstract = {We present ProbUI, a mobile touch GUI framework that merges ease of use of declarative gesture definition with the benefits of probabilistic reasoning. It helps developers to handle uncertain input and implement feedback and GUI adaptations. ProbUI replaces today's static target models (bounding boxes) with probabilistic gestures ("bounding behaviours"). It is the first touch GUI framework to unite concepts from three areas of related work: 1) Developers declaratively define touch behaviours for GUI targets. As a key insight, the declarations imply simple probabilistic models (HMMs with 2D Gaussian emissions). 2) ProbUI derives these models automatically to evaluate users' touch sequences. 3) It then infers intended behaviour and target. Developers bind callbacks to gesture progress, completion, and other conditions. We show ProbUI's value by implementing existing and novel widgets, and report developer feedback from a survey and a lab study.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Buschek, Daniel and Alt, Florian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {touch gestures, gui framework, probabilistic modelling},
	pages = {4640--4653},
}

@inproceedings{corsten_backxpress_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{BackXPress}: {Using} {Back}-of-{Device} {Finger} {Pressure} to {Augment} {Touchscreen} {Input} on {Smartphones}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025565},
	doi = {10.1145/3025453.3025565},
	abstract = {When people hold their smartphone in landscape orientation, they use their thumbs for input on the frontal touchscreen, while their remaining fingers rest on the back of the device (BoD) to stabilize the grip. We present BackXPress, a new interaction technique that lets users create BoD pressure input with these remaining fingers to augment their interaction with the touchscreen on the front: Users can apply various pressure levels with each of these fingers to enter different temporary "quasi-modes" that are only active as long as that pressure is applied. Both thumbs can then interact with the frontal screen in that mode. We illustrate the practicality of BackXPress with several sample applications, and report our results from three user studies: Study 1 investigated which fingers can be used to exert BoD pressure and found index, middle, and ring finger from both hands to be practical. Study 2 revealed how pressure touches from these six fingers are distributed across the BoD. Study 3 examined user performance for applying BoD pressure (a) during single touches at the front and (b) for 20 seconds while touching multiple consecutive frontal targets. Participants achieved up to 92\% pressure accuracy for three separate pressure levels above normal resting pressure, with the middle fingers providing the highest accuracy. BoD pressure did not affect frontal touch accuracy. We conclude with design guidelines for BoD pressure input.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Corsten, Christian and Daehlmann, Bjoern and Voelker, Simon and Borchers, Jan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {smartphone, bimanual input, back-of-device, pressure},
	pages = {4654--4666},
}

@inproceedings{vatavu_improving_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Improving {Gesture} {Recognition} {Accuracy} on {Touch} {Screens} for {Users} with {Low} {Vision}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025941},
	doi = {10.1145/3025453.3025941},
	abstract = {We contribute in this work on gesture recognition to improve the accessibility of touch screens for people with low vision. We examine the accuracy of popular recognizers for gestures produced by people with and without visual impairments, and we show that the user-independent accuracy of \$P, the best recognizer among those evaluated, is small for people with low vision (83.8\%), despite \$P being very effective for gestures produced by people without visual impairments (95.9\%). By carefully analyzing the gesture articulations produced by people with low vision, we inform key algorithmic revisions for the P recognizer, which we call P+. We show significant accuracy improvements of \$P+ for gestures produced by people with low vision, from 83.8\% to 94.7\% on average and up to 98.2\%, and 3x faster execution times compared to P.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Vatavu, Radu-Daniel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gesture recognition, evaluation, visual impairments, algorithms, recognition, low vision, touch gestures, 1, P, P+, point clouds, recognition accuracy, touch screens},
	pages = {4667--4679},
}

@inproceedings{eardley_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding {Grip} {Shifts}: {How} {Form} {Factors} {Impact} {Hand} {Movements} on {Mobile} {Phones}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025835},
	doi = {10.1145/3025453.3025835},
	abstract = {In this paper we present an investigation into how hand usage is affected by different mobile phone form factors. Our initial (qualitative) study explored how users interact with various mobile phone types (touchscreen, physical keyboard and stylus). The analysis of the videos revealed that each type of mobile phone affords specific handgrips and that the user shifts these grips and consequently the tilt and rotation of the phone depending on the context of interaction. In order to further investigate the tilt and rotation effects we conducted a controlled quantitative study in which we varied the size of the phone and the type of grips (Symmetric bimanual, Asymmetric bimanual with finger, Asymmetric bimanual with thumb and Single handed) to better understand how they affect the tilt and rotation during a dual pointing task. The results showed that the size of the phone does have a consequence and that the distance needed to reach action items affects the phones' tilt and rotation. Additionally, we found that the amount of tilt, rotation and reach required corresponded with the participant's grip preference. We finish the paper by discussing the design lessons for mobile UI and proposing design guidelines and applications for these insights.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Eardley, Rachel and Roudaut, Anne and Gill, Steve and Thompson, Stephen J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {mobile device, design, interaction, grasp, handgrip},
	pages = {4680--4691},
}

@inproceedings{kang_structured_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Structured {Input} {Improves} {Usability} and {Precision} for {Solving} {Geometry}-{Based} {Algebraic} {Problems}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025468},
	doi = {10.1145/3025453.3025468},
	abstract = {Previous research has shown that sketch-based input is efficient and preferable in the context of algebraic equation solving. However, research has not been conducted to evaluate whether this holds true when involving geometry input to facilitate quantitative problem-solving. We developed a bimodal (graphing geometric shapes and writing algebraic expressions) user interface, in order to conduct a within-subject, controlled experiment with 24 college students and varied two types of geometry input: 1) sketch-based input and 2) structured input. The sketch-based input was significantly faster than the structured input, but there were no significant differences based on perception and cognition. However, after a post-hoc analysis, we found a significant interaction effect on perception between prior knowledge and geometry input. Novice students preferred the sketch-based input, but advanced students preferred the structured input. Our study implies that natural sketch-based input may be less preferable than structured input for geometry-based interfaces toward math problem-solving.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kang, Bo and LaViola Jr., Joseph J. and Wisniewski, Pamela},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {geometry editing, gesture input, math learning environment, sketch input},
	pages = {4692--4702},
}

@inproceedings{swearngin_genie_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Genie: {Input} {Retargeting} on the {Web} through {Command} {Reverse} {Engineering}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025506},
	doi = {10.1145/3025453.3025506},
	abstract = {Most web applications are designed as one-size-fits-all, despite considerable variation in people's expertise, physical abilities, and other factors that impact interaction. For example, some web applications require the use of a mouse, precluding use by many people with severe motor disabilities. Other applications require laborious manual input that a skilled developer could automate if the application were scriptable. This paper presents Genie, a system that automatically reverse engineers an abstract model of the underlying commands in a web application, then enables interaction with that functionality through alternative interfaces and other input modalities (e.g., speech, keyboard, or command line input). Genie comprises an abstract model of command properties, behaviors, and dependencies as well as algorithms that reverse engineer this model from an existing web application through static and dynamic program analysis. We evaluate Genie by developing several interfaces that automatically add support for speech, keyboard, and command line input to arbitrary web applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Swearngin, Amanda and Ko, Amy J. and Fogarty, James},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {program analysis, reverse engineering, web applications},
	pages = {4703--4714},
}

@inproceedings{giannisakis_iconhk_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{IconHK}: {Using} {Toolbar} {Button} {Icons} to {Communicate} {Keyboard} {Shortcuts}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025595},
	doi = {10.1145/3025453.3025595},
	abstract = {We propose a novel perspective on the design of toolbar buttons that aims to increase keyboard shortcut accessibility. IconHK implements this perspective by blending visual cues that convey keyboard shortcut information into toolbar buttons without denaturing the pictorial representation of their command. We introduce three design strategies to embed the hotkey, a visual encoding to convey the modifiers, and a magnification factor that determines the blending ratio between the pictogram of the button and the visual representation of the keyboard shortcut. Two studies examine the benefits of IconHK for end-users and provide insights from professional designers on the practicality of our approach for creating iconsets. Building on these insights, we develop a tool to assist designers in applying the IconHK design principle.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Giannisakis, Emmanouil and Bailly, Gilles and Malacria, Sylvain and Chevalier, Fanny},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gui design, hotkeys, icons, keyboard shortcuts},
	pages = {4715--4726},
}

@inproceedings{besancon_mouse_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Mouse, {Tactile}, and {Tangible} {Input} for {3D} {Manipulation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025863},
	doi = {10.1145/3025453.3025863},
	abstract = {We evaluate the performance and usability of mouse-based, touch-based, and tangible interaction for manipulating objects in a 3D virtual environment. This comparison is a step toward a better understanding of the limitations and benefits of these existing interaction techniques, with the ultimate goal of facilitating an easy transition between the different 3D data exploration environments. For this purpose we analyze participants' performance in 3D manipulation using a docking task. We measured completion times, docking accuracy, as well as subjective criteria such as fatigue, workload, and preference. Our results show that the three input modalities provide similar levels of precision but require different completion times. We also discuss our qualitative observations as well as people's preferences and put our findings into context of the application domain of 3D data analysis environments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Besançon, Lonni and Issartel, Paul and Ammi, Mehdi and Isenberg, Tobias},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {tangible interaction, mouse, 3D interaction, tactile interaction, TUI, usability study},
	pages = {4727--4740},
}

@inproceedings{chandra_market_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Market {Practices} and the {Bazaar}: {Technology} {Consumption} in {ICT} {Markets} in the {Global} {South}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025970},
	doi = {10.1145/3025453.3025970},
	abstract = {Local informal markets or bazaars play a central role in embedding the adoption, consumption, and reproduction of digital technologies within the economic and cultural fabric of the Global South. This paper presents ethnographic accounts of informal ICT markets in two sites, one in India and the other in Bangladesh, and assesses how technology consumption unfolds within local practices. Building on social practice theory, this paper depicts the role of materiality, relationships, and situated knowledge in the functioning of a bazaar. We discuss how this knowledge expands our understanding of the evaluation of technology and technical expertise, and the persistence of these informal spaces despite the uptake of corporatized technology marketplaces. We argue that the bazaar represents a special kind of local voice that enriches the HCI scholarship in postcolonial computing.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chandra, Priyank and Ahmed, Syed Ishtiaque and Pal, Joyojeet},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {global south, bazaars, informality, markets, postcolonial, practice theory},
	pages = {4741--4752},
}

@inproceedings{green_cinehacking_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Cinehacking {Cape} {Town} - {Embracing} {Informality} in {Pursuit} of {High} {Quality} {Media}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025481},
	doi = {10.1145/3025453.3025481},
	abstract = {Although many common tools of media making such as video cameras have become more accessible in recent years, many remain inaccessible. Cinematography, lighting and sound-recording equipment for example can be prohibitively expensive to obtain, complex to configure, and/or require specialist knowledge to operate effectively. These barriers can prevent non-professionals who want to produce high-quality media from being able to. Cinehack is an ongoing project to research ways to overcome these barriers. In this paper, we specifically report on Cinehack: Cape Town, a participatory media making project. By co-producing hip hop videos within a community for whom media making is often a "means-to-an-end", we were able gain insights into the kinds of support needed to enable high quality media making by non-professionals. Specifically, we highlight ways to meet users' needs by embracing informal codes of practice via experimental making and peer-support.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Green, David Philip and Schofield, Guy and Pritchard, Gary and Olivier, Patrick and Wright, Peter},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {diy, making, africa, hacking, hip-hop, media},
	pages = {4753--4764},
}

@inproceedings{chandra_informality_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Informality and {Invisibility}: {Traditional} {Technologies} as {Tools} for {Collaboration} in an {Informal} {Market}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025643},
	doi = {10.1145/3025453.3025643},
	abstract = {This paper explores how actors in local markets in the Global South adapt traditional communication technologies to successfully collaborate to sustain the markets and their business practices. Drawing on ethnographic observations at a local technology goods market in Bangalore, India, the study details the use of a landline telephone intercom system as the primary tool for business communication in the market. Through analyzing how the intercom system relates to informality and physical space, the paper argues that it bridges the formal with the informal, and helps facilitate informal business practices while also allowing them to remain hidden from the formal regulatory gaze of the state.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chandra, Priyank},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {HCI4D, infrastructure, development, informal markets, intercom, telephone},
	pages = {4765--4775},
}

@inproceedings{singh_margins_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {From {Margins} to {Seams}: {Imbrication}, {Inclusion}, and {Torque} in the {Aadhaar} {Identification} {Project}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025910},
	doi = {10.1145/3025453.3025910},
	abstract = {Problems of marginalization and inclusion are central to HCI scholarship and impact in the world, but are badly named in the binary models of access that currently dominate the field. Building on prior work in ICTD and infrastructure studies, this paper explores the problem of inclusion through historical and ethnographic study of Aadhaar, India's biometrics-based national identification project. We illustrate tensions between Aadhaar users' ability to register, authenticate and successfully deploy their registered identity to participate in the Public Distribution System (PDS), a government scheme that provides subsidized food grains to the Indian poor. We argue that rather than an all-or-nothing state, inclusion in ICTD infrastructures is an ongoing and fragile process, achieved (unevenly) at the seams of multiple interconnected systems. Finally, we show that questions of (effective) inclusion are determined not just at margins of a system (who is in and who is out) but also through the artful and often challenging negotiation of the seams that run through and connect complex distributed infrastructures.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Ranjit and Jackson, Steven J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ICTD, materiality, infrastructure, inclusion, bureaucracy, access, biometrics, India},
	pages = {4776--4824},
}

@inproceedings{taylor_performing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Performing {Research}: {Four} {Contributions} to {HCI}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025751},
	doi = {10.1145/3025453.3025751},
	abstract = {This paper identifies a body of HCI research wherein the researchers take part in digitally mediated creative experiences alongside participants. We present our definition and rationale for "self-situated performance research" based on theories in both the HCI and performance literatures. We then analyse four case studies of this type of work, ranging from overtly "performative" staged events to locative audio and public making.We argue that by interrogating experience from within the context of self-situated performance, the 'performer/researcher' extends traditional practices in HCI in the following four ways: developing an intimate relationship between researchers and participants, providing new means of making sense of interactions, shaping participants' relationship to the research, and enabling researchers to refine their work as it is being conducted.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Taylor, Robyn and Spence, Jocelyn and Walker, Brendan and Nissen, Bettina and Wright, Peter},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {performance, design from within, performing research, practice, public making, self-situated research, sense-making},
	pages = {4825--4837},
}

@inproceedings{javornik_magicface_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{MagicFace}: {Stepping} into {Character} through an {Augmented} {Reality} {Mirror}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025722},
	doi = {10.1145/3025453.3025722},
	abstract = {Augmented Reality (AR) is coming of age and appearing in various smartphone apps. One emerging AR type uses the front-facing camera and overlays a user's face with digital features that transform the physical appearance, making the user look like someone else, such as a popstar or a historical character. However, little is known about how people react to such stepping into character and how convincing they perceive it to be. We developed an app with two Egyptian looks, MagicFace, which was situated both in an opera house and a museum. In the first setting, people were invited to use the app, while in the second setting they came across it on their own when visiting the exhibition. Our findings show marked differences in how people approach and experience the MagicFace in these different contexts. We discuss how realistic and compelling this kind of AR technology is, as well as its implications for educational and cultural settings.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Javornik, Ana and Rogers, Yvonne and Gander, Delia and Moutinho, Ana},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {augmented reality, interface design, in-the-wild study, opera characters},
	pages = {4838--4849},
}

@inproceedings{rossitto_interactive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Interactive {Performance} as a {Means} of {Civic} {Dialogue}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025897},
	doi = {10.1145/3025453.3025897},
	abstract = {This paper presents a case study of an interactive performance that was produced and designed to encourage civic engagement and reflection in relation to the social tensions in a low-income suburb, mostly inhabited by people with immigrant backgrounds. The design of the technological setup in the performance encouraged participation by means of text entries that audience members could share with others. The analysis draws on the corpus of interview and observational data collected, as well as the related text messages that were shared during the performance. We illustrate the different levels at which citizens make sense of societal issues they are concerned about, as well as the audience-citizens' perception of participating in such an artistic experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rossitto, Chiara and Normark, Maria and Barkhuus, Louise},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {digital civics, interactive performance, mobile technology., qualitative studies, social participation},
	pages = {4850--4862},
}

@inproceedings{pellicone_game_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Game} of {Performing} {Play}: {Understanding} {Streaming} as {Cultural} {Production}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025854},
	doi = {10.1145/3025453.3025854},
	abstract = {Live streaming has become pervasive in digital game culture. Previous work has focused largely on technological considerations in streaming platforms. However, little is known about how streamers enter the practice, gain skills, and operate as content producers. We present a qualitative study of an online forum dedicated to streaming. By observing the conversations between veterans and newcomers to the practice, we develop an understanding of how streamers must tie together technological, social, and gameplay-based skills to craft an appealing performance of play. We find that a key skill in streaming is the development of a unique attitude and persona as a gamer, which permeates into every element of a streamer's performance. As individual identity becomes important in streaming practice, design considerations for platform features such as community moderation and stream metrics may help improve equitable participation in this increasingly important aspect of game culture.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pellicone, Anthony J. and Ahn, June},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {digital games, games and learning, games studies, streaming media},
	pages = {4863--4874},
}

@inproceedings{chung_finding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Finding the {Right} {Fit}: {Understanding} {Health} {Tracking} in {Workplace} {Wellness} {Programs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025510},
	doi = {10.1145/3025453.3025510},
	abstract = {Workplace health and wellness programs are increasingly integrating personal health tracking technologies, such as Fitbit and Apple Watch. Many question whether these technologies truly support employees in their pursuit of better wellness levels, raising objections about workplace surveillance and further blurring of boundaries between work and personal life. We conducted a study to understand how tracking tools are adopted in wellness programs and employees' opinions about these programs. We find that employees are generally positive about incentivized health tracking in the workplace, as it helps raise awareness of activity levels. However, there is a gap between the intentions of the programs and individual experiences and health goals. This sometimes results in confusion and creates barriers to participation. Even if this gap can be addressed, health tracking in the workplace will not be for everyone; this has implications for the design of both workplace wellness programs and tracking technologies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chung, Chia-Fang and Gorm, Nanna and Shklovski, Irina A. and Munson, Sean},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {self-tracking, workplace, health and wellness program},
	pages = {4875--4886},
}

@inproceedings{fan_mastery_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Mastery {Learning} of {Second} {Language} through {Asynchronous} {Modeling} of {Native} {Speakers} in a {Collaborative} {Mobile} {Game}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025544},
	doi = {10.1145/3025453.3025544},
	abstract = {Acquiring Chinese tones is often considered as the most difficult task in learning Chinese as a Second Language (CSL). Recently, ToneWars, a collaborative mobile learning game, demonstrated the feasibility and efficacy of connecting CSL learners with native speakers for tone learning. However, the synchronous gameplay nature in ToneWars can be hard to scale due to the time constraint and limited availability of native speakers. We present principled research to make ToneWars scalable and sustainable. First, we address the scalability issue via asynchronous modeling of native speakers. Second, we quantify whether a CSL learner achieves native level mastery for a specific phrase, and explore the use of fine-grained feedback on language mastery as a sustainable motivator for language learning. The insights in this research are generalizable to designing second language learning technologies beyond Chinese. In a longitudinal study with 18 CSL learners, we found that asynchronous gameplay significantly improved learning with an average gain of 29.7 tones and 16.4 syllables, and helped participants achieve native level mastery on 58.2 out of 69 phrases.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fan, Xiangmin and Luo, Wencan and Wang, Jingtao},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {evaluation, collaborative learning, mandarin tones, mobile learning, serious games},
	pages = {4887--4898},
}

@inproceedings{marshall_misrepresentation_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Misrepresentation of {Health} {Research} in {Exertion} {Games} {Literature}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025691},
	doi = {10.1145/3025453.3025691},
	abstract = {HCI often requires scholars to build upon research from fields outside their expertise, creating the risk that foundational work is misunderstood and misrepresented. The prevailing goal of "exergames" research towards ameliorating obesity appears to be built on just such a misunderstanding of health research. In this paper, we analyse all citations to a single influential study, which has been extensively cited to justify research on exergames. We categorise the 375 citations based on whether they represent the findings of that study accurately or inaccurately. Our findings suggest that 69\% of exergames papers citing this study misrepresent the findings, demonstrating a systematic failure of scholarship in exergames research. We argue that exergaming research should cease focusing on games as treatment for obesity, and that HCI publications should demand more critical and scholarly engagement with research from outside HCI.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Marshall, Joe and Linehan, Conor},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {health, games, exertion, exertion games, obesity},
	pages = {4899--4910},
}

@inproceedings{arawjo_teaching_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Teaching {Programming} with {Gamified} {Semantics}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025711},
	doi = {10.1145/3025453.3025711},
	abstract = {Dominant approaches to programming education emphasize program construction over language comprehension. We present Reduct, an educational game embodying a new, comprehension-first approach to teaching novices core programming concepts which include functions, Booleans, equality, conditionals, and mapping functions over sets. In this novel teaching strategy, the player executes code using reduction-based operational semantics. During gameplay, code representations fade from concrete, block-based graphics to the actual syntax of JavaScript ES2015. We describe our design rationale and report on the results of a study evaluating the efficacy of our approach on young adults (18+) without prior coding experience. In a short timeframe, novices demonstrated promising learning of core concepts expressed in actual JavaScript. We also present results from an online deployment. Finally, we discuss ramifications for the design of future computational thinking games.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Arawjo, Ian and Wang, Cheng-Yao and Myers, Andrew C. and Andersen, Erik and Guimbretière, François},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {block-based programming, concreteness fading, educational games, novice programming},
	pages = {4911--4923},
}

@inproceedings{prabhakar_investigating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Investigating the {Suitability} of the {Asynchronous}, {Remote}, {Community}-{Based} {Method} for {Pregnant} and {New} {Mothers}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025546},
	doi = {10.1145/3025453.3025546},
	abstract = {Traditional qualitative research methods, such as, interviews and focus groups, may not be feasible for certain populations- who face time, mobility, and availability constraints. We adapted the Asynchronous, Remote, Community-based (ARC) method that used closed Facebook groups to study people with rare diseases, to study a different population - pregnant and new mothers. During the course of eight weeks, we engaged 48 participants in 19 study activities using three closed Facebook groups. We added new activities to the original ARC method, informed by past HCI research, to triangulate participant input. We carefully analyzed participation patterns and activity engagement, to assess the suitability of the ARC method for engaging pregnant and new mothers in remote, group-based, qualitative research. We provide an in-depth analysis of the ARC method, noting participation characteristics, activity preferences, and the suitability of the ARC method as an online focus group.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Prabhakar, Annu Sible and Guerra-Reyes, Lucia and Kleinschmidt, Vanessa M. and Jelen, Ben and MacLeod, Haley and Connelly, Kay and Siek, Katie A.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {focus groups, maternal health, facebook groups, remote populations, socialsupport},
	pages = {4924--4934},
}

@inproceedings{le_moignan_has_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Has {Instagram} {Fundamentally} {Altered} the '{Family} {Snapshot}'?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025928},
	doi = {10.1145/3025453.3025928},
	abstract = {This paper considers how parents use the social media platform Instagram to facilitate the capture, curation and sharing of 'family snapshots'. Our work draws upon established cross-disciplinary literature relating to film photography and the composition of family albums in order to establish whether social media has changed the way parents visually present their families. We conducted a qualitative visual analysis of a sample of 4,000 photographs collected from Instagram using hashtags relating to children and parenting. We show that the style and composition of snapshots featuring children remains fundamentally unchanged and continues to be dominated by rather bland and idealised images of the happy family and the cute child. In addition, we find that the frequent taking and sharing of photographs via Instagram has inevitably resulted in a more mundane visual catalogue of daily life. We note a tension in the desire to use social media as a means to evidence good parenting, while trying to effectively manage the social identity of the child and finally, we note the reluctance of parents to use their own snapshots to portray family tension or disharmony, but their willingness to use externally generated content for this purpose.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Le Moignan, Effie and Lawson, Shaun and Rowland, Duncan A. and Mahoney, Jamie and Briggs, Pam},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {social media, families, instagram, parenting, photo-sharing},
	pages = {4935--4947},
}

@inproceedings{kim_internet_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Internet {Search} {Roles} of {Adults} in {Their} {Homes}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025572},
	doi = {10.1145/3025453.3025572},
	abstract = {Internet search is one of the major activities that American adults engage in online. Building on studies of youth Internet search roles, this paper investigates adults' online information seeking processes within the home. Through in-home interviews and observations of search task performance with 40 adult participants, we identify and describe characteristics of 9 search roles. By comparing these roles with those of youths, we explain how previously identified roles, such as Power Searcher and Social Searcher, have evolved in adult populations, and how new roles, such as Efficient Searcher and Interest-driven Searcher, have emerged. We also review the challenges and benefits associated with search roles and their potential impacts on search performance. The findings of this study provide a better understanding of how contextual factors influence search roles in relation to ELIS, what can be learned from search roles, and opportunities to support different search roles.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Jinyoung and McNally, Brenna and Norooz, Leyla and Druin, Allison},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {adult searchers, everyday life information seeking, home internet search, search roles, search strategies},
	pages = {4948--4959},
}

@inproceedings{hussien_ahmed_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding the {Role} of {Human} {Senses} in {Interactive} {Meditation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026000},
	doi = {10.1145/3025453.3026000},
	abstract = {In our fast-paced society, stress and anxiety have become increasingly common. Meditation for relaxation has received much attention. Meditation apps exploit various senses, e.g., touch, audio and vision, but the relationship between human senses and interactive meditation is not well understood. This paper empirically evaluates the effects of single and combined human senses on interactive meditation. We found that the effectiveness of human senses can be defined by their respective roles in maintaining the balance between relaxation and focus. This work is the first to attempt to understand these relationships. The findings have broad implications for the field of multi-modal interaction and interactive meditation applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hussien Ahmed, Mahmoud Mohamed and Silpasuwanchai, Chaklam and Salehzadeh Niksirat, Kavous and Ren, Xiangshi},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {mindfulness, "human senses, focus", interactive meditation, relaxation},
	pages = {4960--4965},
}

@inproceedings{lukoff_gender_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gender {Norms} and {Attitudes} about {Childcare} {Activities} {Presented} on {Father} {Blogs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025767},
	doi = {10.1145/3025453.3025767},
	abstract = {Father involvement is important for child well-being. However, fathers still do significantly less childcare than mothers, due in part to traditional gender norms. This research investigates whether incorporating do-it-yourself (DIY) language and imagery into parenting blogs is an effective mechanism for boosting fathers' willingness to perform childcare activities. We conducted a between-subjects experiment with 374 participants in the U.S. who responded to ten parenting blog posts. Subjects were randomized to view posts with either DIY or neutral language and either routine childcare activities (e.g., changing diapers) or interactive ones (e.g., finger painting). Results show that DIY language actually decreases a father's willingness to do a childcare activity. Further, fathers underestimate how socially appropriate it is for them to perform childcare activities and this misperception relates to their willingness to get involved. We draw on social norms literature to recommend next steps for designing interfaces to support father involvement in childrearing.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lukoff, Kai and Moser, Carol and Schoenebeck, Sarita},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {masculinity, diy, blogs, childcare, fathers, norms},
	pages = {4966--4971},
}

@inproceedings{culbertson_waves_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{WAVES}: {A} {Wearable} {Asymmetric} {Vibration} {Excitation} {System} for {Presenting} {Three}-{Dimensional} {Translation} and {Rotation} {Cues}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025741},
	doi = {10.1145/3025453.3025741},
	abstract = {WAVES, a Wearable Asymmetric Vibration Excitation System, is a novel wearable haptic device for presenting three dimensions of translation and rotation guidance cues. In contrast to traditional vibration feedback, which usually requires that users learn to interpret a binary cue, asymmetric vibrations have been shown to induce a pulling sensation in a desired direction. When attached to the fingers, a single voicecoil actuator presents a translation guidance cue and a pair of voicecoil actuators presents a rotation guidance cue. The directionality of mechanoreceptors in the skin led to our choice of the location and orientation of the actuators in order to elicit very strong sensations in certain directions. For example, users distinguished a "left" cue versus a "right" cue 94.5\% of the time. When presented with one of six possible direction cues, users on average correctly identified the direction of translation cues 86.1\% of the time and rotation cues 69.0\% of the time.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Culbertson, Heather and Walker, Julie M. and Raitor, Michael and Okamura, Allison M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {haptics, wearable devices, vibration, haptic guidance},
	pages = {4972--4982},
}

@inproceedings{yasu_magnetic_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Magnetic {Plotter}: {A} {Macrotexture} {Design} {Method} {Using} {Magnetic} {Rubber} {Sheets}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025702},
	doi = {10.1145/3025453.3025702},
	abstract = {This paper presents a method for designing tactile macrotextures with magnetic rubber sheets. In the method, named "Magnetic Plotter", a desktop digital plotting machine combined with a tiny neodymium magnet writes fine magnetic patterns on the surface of the magnetic rubber sheets. This method enables users to design magnetic fields freely with inexpensive commercially available materials as if they are drawing pictures. Moreover, when the magnetic sheets are rubbed together, unique haptic stimuli are displayed on the fingers. The haptic stimuli can be changed by the magnetic patterns designed on the rubber sheets. We developed a prototype of the Magnetic Plotter and investigated the range of the generated haptic stimuli and the texture design possibilities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yasu, Kentaro},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {home, diy, digital fabrication, haptic, rapid prototyping, interactive devices, magnets, tactile},
	pages = {4983--4993},
}

@inproceedings{strohmeier_generating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Generating {Haptic} {Textures} with a {Vibrotactile} {Actuator}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025812},
	doi = {10.1145/3025453.3025812},
	abstract = {Vibrotactile actuation is mainly used to deliver buzzing sensations. But if vibrotactile actuation is tightly coupled to users' actions, it can be used to create much richer haptic experiences. It is not well understood, however, how this coupling should be done or which vibrotactile parameters create which experiences. To investigate how actuation parameters relate to haptic experiences, we built a physical slider with minimal native friction, a vibrotactile actuator and an integrated position sensor. By vibrating the slider as it is moved, we create an experience of texture between the sliding element and its track. We conducted a magnitude estimation experiment to map how granularity, amplitude and timbre relate to the experiences of roughness, adhesiveness, sharpness and bumpiness. We found that amplitude influences the strength of the perceived texture, while variations in granularity and timbre create distinct experiences. Our study underlines the importance of action in haptic perception and suggests strategies for deploying such tightly coupled feedback in everyday devices.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Strohmeier, Paul and Hornbæk, Kasper},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {haptic feedback, magnitude estimation, texture perception},
	pages = {4994--5005},
}

@inproceedings{rekik_localized_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Localized {Haptic} {Texture}: {A} {Rendering} {Technique} {Based} on {Taxels} for {High} {Density} {Tactile} {Feedback}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026010},
	doi = {10.1145/3025453.3026010},
	abstract = {We investigate the relevance of surface haptic rendering techniques for tactile devices. We focus on the two major existing techniques and show that they have complementary benefits. The first one, called textscSurface textscHaptic textscObject (textscSHO), which is based on finger position, is shown to be more suitable to render sparse textures; while the second one, called textscSurface textscHaptic textscTexture (textscSHT), which is based on finger velocity, is shown to be more suitable for dense textures and fast finger movements. We hence propose a new rendering technique, called textscLocalized textscHaptic textscTexture (textscLHT), which is based on the concept of textittaxel considered as an elementary tactile information that is rendered on the screen. By using a grid of taxels to encode a texture, textscLHT is shown to provide a consistent tactile rendering across different velocities for high density textures, and is found to reduce user textiterror rate by up to 77.68\% compared to textscSHO.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rekik, Yosra and Vezzoli, Eric and Grisoni, Laurent and Giraud, Frédéric},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {identification, tactile feedback, density, lht, rendering techniques, sho, sht, taxel, texture, velocity},
	pages = {5006--5015},
}

@inproceedings{gai_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting {Easy} {Physical}-to-{Virtual} {Creation} of {Mobile} {VR} {Maze} {Games}: {A} {New} {Genre}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025494},
	doi = {10.1145/3025453.3025494},
	abstract = {With the fast development of virtual reality games, one of the key research questions is how players may express their creativity and participate in the process of game design. In this paper, we present a new game genre which combines user-controlled game design in physical space with game play in virtual space on a mobile device. The new system supports authoring by anyone, creating virtual reality games that can be easily modified or developed for physical space, and be used anywhere by novice end-users without any knowledge of tracking technology. We present the design and implementation of the system, as well as a user experiment. Findings illustrate that the proposed system promotes participation and provides a richer, more interactive and engaging experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gai, Wei and Yang, Chenglei and Bian, Yulong and Shen, Chia and Meng, Xiangxu and Wang, Lu and Liu, Juan and Dong, Mingda and Niu, Chengjie and Lin, Cheng},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, head-mounted display, mobile 3d, natural interaction},
	pages = {5016--5028},
}

@inproceedings{mcarthur_ux_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {UX} of {Avatar} {Customization}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026020},
	doi = {10.1145/3025453.3026020},
	abstract = {Avatar customization is a feature that is offered in many computer and video games. Customization options are presented to users via Character Creation Interfaces or CCIs. CCIs differ greatly between games, independent of genre, with regard to the quantity and quality of customization options available. In addition, the way in which these options are presented to users differs from game to game. Research on avatar customization is typically focused on user-avatar identity or self-representation. In general, we have found that the User Experience (UX) of avatar customization has been greatly overlooked in academic literature. As such, we look to existing research on UX in order to propose how its methodologies may be used to study the impact of CCI affordances on player experience in games.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McArthur, Victoria},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user experience, interface, affordances, identity, avatars},
	pages = {5029--5033},
}

@inproceedings{gaston_three_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {To {Three} or {Not} to {Three}: {Improving} {Human} {Computation} {Game} {Onboarding} with a {Three}-{Star} {System}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025997},
	doi = {10.1145/3025453.3025997},
	abstract = {While many popular casual games use three-star systems, which give players up to three stars based on their performance in a level, this technique has seen limited application in human computation games (HCGs). This gives rise to the question of what impact, if any, a three-star system will have on the behavior of players in HCGs. In this work, we examined the impact of a three-star system implemented in the protein folding HCG Foldit. We compared the basic game's introductory levels with two versions using a three-star system, where players were rewarded with more stars for completing levels in fewer moves. In one version, players could continue playing levels for as many moves as they liked, and in the other, players were forced to reset the level if they used more moves than required to achieve at least one star on the level. We observed that the three-star system encouraged players to use fewer moves, take more time per move, and replay completed levels more often. We did not observe an impact on retention. This indicates that three-star systems may be useful for re-enforcing concepts introduced by HCG levels, or as a flexible means to encourage desired behaviors.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gaston, Jacqueline and Cooper, Seth},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design, games, analytics, human computation},
	pages = {5034--5039},
}

@inproceedings{hornbaek_what_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {What {Is} {Interaction}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025765},
	doi = {10.1145/3025453.3025765},
	abstract = {The term interaction is field-defining, yet surprisingly confused. This essay discusses what interaction is. We first argue that only few attempts to directly define interaction exist. Nevertheless, we extract from the literature distinct and highly developed concepts, for instance viewing interaction as dialogue, transmission, optimal behavior, embodiment, and tool use. Importantly, these concepts are associated with different scopes and ways of construing the causal relationships between the human and the computer. This affects their ability to inform empirical studies and design. Based on this discussion, we list desiderata for future work on interaction, emphasizing the need to improve scope and specificity, to better account for the effects and agency that computers have in interaction, and to generate strong propositions about interaction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hornbæk, Kasper and Oulasvirta, Antti},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {human-computer interaction, interaction, concepts, models, scientific progress, theories},
	pages = {5040--5052},
}

@inproceedings{maudet_beyond_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Beyond {Grids}: {Interactive} {Graphical} {Substrates} to {Structure} {Digital} {Layout}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025718},
	doi = {10.1145/3025453.3025718},
	abstract = {Traditional graphic design tools emphasize the grid for structuring layout. Interviews with professional graphic designers revealed that they use surprisingly sophisticated structures that go beyond the grid, which we call graphical substrates. We present a framework to describe how designers establish graphical substrates based on properties extracted from concepts, content and context, and use them to compose layouts in both space and time. We developed two technology probes to explore how to embed graphical substrates into tools. Contextify lets designers tailor layouts according to each reader's intention and context; while Linkify lets designers create dynamic layouts based on relationships among content properties. We tested the probes with professional graphic designers, who all identified novel uses in their current projects. We incorporated their suggestions into StyleBlocks, a prototype that reifies CSS declarations into interactive graphical substrates. Graphical substrates offer an untapped design space for tools that can help graphic designers generate personal layout structures.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Maudet, Nolwenn and Jalal, Ghita and Tchernavskij, Philip and Beaudouin-Lafon, Michel and Mackay, Wendy E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {graphic design, creativity support tools., layout},
	pages = {5053--5064},
}

@inproceedings{takahashi_expressive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Expressive {Fused} {Deposition} {Modeling} by {Controlling} {Extruder} {Height} and {Extrusion} {Amount}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025933},
	doi = {10.1145/3025453.3025933},
	abstract = {Fused deposition modeling (FDM) 3D printers form objects by stacking layers having a linear structure. To print fine structures, an appropriate choice of parameters is necessary, or printing error occurs. On the other hand, the printing error is exploited as an expression technique. However, the relation between the printed structure and the parameters causing the printing error is unclear. In this paper, we focus on the height position of the extruder and the amount of extruded material, and explore the combination of these parameters to enhance the capability of FDM. By extending an equation that calculates the amount of material from the layer height, we investigate the behavior and structure of material extruded from various height positions. On the basis of experimental results, the printed structure is classified into six categories according to the structural feature. We describe these structural features and demonstrate examples with new inherent expressions for FDM.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Takahashi, Haruki and Miyashita, Homei},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {3D printing, expression, fabrication, fused deposition modeling, printing error},
	pages = {5065--5074},
}

@inproceedings{strasnick_shiftio_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ShiftIO}: {Reconfigurable} {Tactile} {Elements} for {Dynamic} {Affordances} and {Mobile} {Interaction}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025988},
	doi = {10.1145/3025453.3025988},
	abstract = {Currently, virtual (i.e. touchscreen) controls are dynamic, but lack the advantageous tactile feedback of physical controls. Similarly, devices may also have dedicated physical controls, but they lack the flexibility to adapt for different contexts and applications. On mobile and wearable devices in particular, space constraints further limit our input and output capabilities. We propose utilizing reconfigurable tactile elements around the edge of a mobile device to enable dynamic physical controls and feedback. These tactile elements can be used for physical touch input and output, and can reposition according to the application both around the edge of and hidden within the device. We present shiftIO, two implementations of such a system which actuate physical controls around the edge of a mobile device using magnetic locomotion. One version utilizes PCB-manufactured electromagnetic coils, and the other uses switchable permanent magnets. We perform a technical evaluation of these prototypes and compare their advantages in various applications. Finally, we demonstrate several mobile applications which leverage shiftIO to create novel mobile interactions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Strasnick, Evan and Yang, Jackie and Tanner, Kesler and Olwal, Alex and Follmer, Sean},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {dynamic affordance, magnetically-actuated buttons, mobile haptics, tactile display},
	pages = {5075--5086},
}

@inproceedings{petralito_good_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {A {Good} {Reason} to {Die}: {How} {Avatar} {Death} and {High} {Challenges} {Enable} {Positive} {Experiences}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026047},
	doi = {10.1145/3025453.3026047},
	abstract = {Appropriate challenges and challenge-skill balance are usually key to positive player experiences. However, some games such as the successful series Dark Souls are notorious for their excessive difficulty. Yet, there has been little empirical investigation of why players enjoy games they constantly struggle and fail with. We surveyed 95 participants right after the release of Dark Souls III about their experiences with the game, employing both open questions and different player experience measures. Players generally enjoyed challenging play sessions and mostly reported positive experiences, with achievement and learning moments strongly contributing to positive experiences. However, these factors themselves were enabled by negative events such as difficulties and avatar death. Our findings showcase that negative events bear a potential for forming positive and meaningful experiences, thus expanding previous knowledge about the role of challenge and failing in games. Moreover, the significance of hard-earned achievements extends present design conventions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Petralito, Serge and Brühlmann, Florian and Iten, Glena and Mekler, Elisa D. and Opwis, Klaus},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {games, player experience, avatar death, challenge, enjoyment, failure},
	pages = {5087--5097},
}

@inproceedings{mustafa_how_2017,
	address = {New York, NY, USA},
	title = {How {Human} {Am} {I}? {EEG}-{Based} {Evaluation} of {Virtual} {Characters}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026043},
	doi = {10.1145/3025453.3026043},
	abstract = {There is a continuous effort by animation experts to create increasingly realistic and more human-like digital characters. However, as virtual characters become more human they risk evoking a sense of unease in their audience. This sensation, called the Uncanny Valley effect, is widely acknowledged both in the popular media and scientific research but empirical evidence for the hypothesis has remained inconsistent. In this paper, we investigate the neural responses to computer-generated faces in a cognitive neuroscience study. We record brain activity from participants (N = 40)},
	publisher = {Association for Computing Machinery},
	author = {Mustafa, Maryam and Guthe, Stefan and Tauscher, Jan-Philipp and Goesele, Michael and Magnor, Marcus},
	year = {2017},
}

@inproceedings{wehbe_testing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Testing {Incremental} {Difficulty} {Design} in {Platformer} {Games}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025697},
	doi = {10.1145/3025453.3025697},
	abstract = {Designing difficulty levels in platformer games is a challenge for game designers. It is important because design decisions that affect difficulty also directly affect player experience. Consequently, design strategies for balancing game difficulty are discussed by both academics and game designers. In this paper, we study how manipulating the following design decisions, commonly found in platformers, moderates difficulty: Scroll Speed, Target Size, Jump Task Complexity, and Perspective. Results for Scroll Speed and Target Size indicate that errors increase as speed increases and platform size decreases. However, results for jump task complexity demonstrate a separation of errors from task complexity. Specifically, while double-jump tasks are harder than single-jump tasks, triple-jump tasks appear to be as difficult as double-jump tasks. Additionally, the study demonstrates how changes in perspective affect the errors made by players in gameplay. The study results are applicable both to automatic level generation and dynamic difficulty adjustment in platformer games.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wehbe, Rina R. and Mekler, Elisa D. and Schaekermann, Mike and Lank, Edward and Nacke, Lennart E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {game design, difficulty, games user research (gur)},
	pages = {5109--5113},
}

@inproceedings{hassib_engagemeter_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{EngageMeter}: {A} {System} for {Implicit} {Audience} {Engagement} {Sensing} {Using} {Electroencephalography}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025669},
	doi = {10.1145/3025453.3025669},
	abstract = {Obtaining information about audience engagement in presentations is a valuable asset for presenters in many domains. Prior literature mostly utilized explicit methods of collecting feedback which induce distractions, add workload on audience and do not provide objective information to presenters. We present EngageMeter - a system that allows fine-grained information on audience engagement to be obtained implicitly from multiple brain-computer interfaces (BCI) and to be fed back to presenters for real time and post-hoc access. Through evaluation during an HCI conference (Naudience=11, Npresenters=3) we found that EngageMeter provides value to presenters (a) in real-time, since it allows reacting to current engagement scores by changing tone or adding pauses, and (b) in post-hoc, since presenters can adjust their slides and embed extra elements. We discuss how EngageMeter can be used in collocated and distributed audience sensing as well as how it can aid presenters in long term use.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hassib, Mariam and Schneegass, Stefan and Eiglsperger, Philipp and Henze, Niels and Schmidt, Albrecht and Alt, Florian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {eeg, physiological sensing, audience feedback, bci},
	pages = {5114--5119},
}

@inproceedings{evain_can_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Can {I} {Think} of {Something} {Else} {When} {Using} a {BCI}? {Cognitive} {Demand} of an {SSVEP}-{Based} {BCI}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026037},
	doi = {10.1145/3025453.3026037},
	abstract = {BCIs are presumably supposed to require the full attention of their users and to lose accuracy if they pay attention to another task. This assertion has been verified with several BCI paradigms (e.g. P300). But the cognitive demand of the promising SSVEP paradigm had never been specifically assessed yet. We measured the accuracy of an SSVEP-based BCI used by 26 participants in various conditions of mental workload. Our analysis revealed that surprisingly, for this type of BCI, little attention is actually needed from participants to reach optimal accuracy: participants were able to successfully perform a complex secondary task (N-back) without degrading the BCI accuracy. The same observation was made whether visual or auditive attention was solicited. These results indicate that SSVEP is a low-demanding paradigm in terms of cognitive resources, and are encouraging for its use in complex interaction settings.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Evain, Andéol and Argelaguet, Ferran and Roussel, Nicolas and Casiez, Géry and Lécuyer, Anatole},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {cognitive load, bci, n-back task, ssvep},
	pages = {5120--5125},
}

@inproceedings{spiel_not_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {"{Not} {Another} {Z} {Piece}!": {Adaptive} {Difficulty} in {TETRIS}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025721},
	doi = {10.1145/3025453.3025721},
	abstract = {Difficulty in TETRIS is adjusted by adapting the speed with which blocks fall. In this contribution, we describe results of an exploratory study in which we investigated relationships between players' performance and their subjective assessment of difficulty and fun. We tested five different algorithms that, instead of adjusting game speed, adjust difficulty by choosing blocks based on the current game state. With our results, we establish pile height and bumpiness as parameters that indicate the performance of a player during a live game, discuss the inherent difficulty of different block choosing algorithms and show how the relationship between fun and perceived difficulty varies for distinct player groups. With regard to adapting difficulty, we argue that one can still teach an old dog such a TETRIS a lot of new tricks.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Spiel, Katta and Bertel, Sven and Kayali, Fares},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user study, fun, perceived difficulty, tetris},
	pages = {5126--5131},
}

@inproceedings{malinverni_world-as-support_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {World}-as-{Support}: {Embodied} {Exploration}, {Understanding} and {Meaning}-{Making} of the {Augmented} {World}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025955},
	doi = {10.1145/3025453.3025955},
	abstract = {Current technical capabilities of mobile technologies are consolidating the interest in developing context-aware Augmented/Mixed Reality applications. Most of these applications are designed based on the Window-on-the-World (WoW) interaction paradigm. A significant decrease in cost of projection technology and advances in pico-sized projectors have spurred applications of Projective Augmented Reality. This research has focused mainly on technological development. However, there is still a need to fully understand its communicational and expressive potential. Hence, we define a conceptual paradigm that we call World-as-Support (WaS). We compare the WaS and WoW paradigms by contrasting their assumptions and cultural values, as well as through a study of an application aimed at supporting the collaborative improvisation of site-specific narratives by children. Our analysis of children's understanding of the physical and social environment and of their imaginative play allowed us to identify the affordances, strengths and weaknesses of these two paradigms.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Malinverni, Laura and Maya, Julian and Schaper, Marie-Monique and Pares, Narcis},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {augmented reality, mixed reality, embodied interaction, embodied cognition, meaning making, window-on-the-world, world-as-support.},
	pages = {5132--5144},
}

@incollection{feuchtner_extending_2017,
	address = {New York, NY, USA},
	title = {Extending the {Body} for {Interaction} with {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025689},
	abstract = {In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Feuchtner, Tiare and Müller, Jörg},
	year = {2017},
	pages = {5145--5157},
}

@inproceedings{wilde_embodied_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Embodied {Design} {Ideation} {Methods}: {Analysing} the {Power} of {Estrangement}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025873},
	doi = {10.1145/3025453.3025873},
	abstract = {Embodied design ideation practices work with relationships between body, material and context to enliven design and research potential. Methods are often idiosyncratic and due to their physical nature not easily transferred. This presents challenges for designers wishing to develop and share techniques or contribute to research. We present a framework that enables designers to understand, describe and contextualise their embodied design ideation practices in ways that can be understood by peers, as well as those new to embodied ideation. Our framework developed over two conference workshops provides a frame for discussion of embodied design actions that leverage the power of estrangement. We apply our framework to eight embodied design ideation methods. Our contribution is thus twofold: (1) a framework to understand and leverage the power of estrangement in embodied design ideation, and (2) an inspirational catalogue demonstrating the diversity of ideas that embodied design ideation methods can foster.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wilde, Danielle and Vallgårda, Anna and Tomico, Oscar},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design methods, ideation, design research, disruption, embodiment, estrangement},
	pages = {5158--5170},
}

@incollection{francoise_designing_2017,
	address = {New York, NY, USA},
	title = {Designing for {Kinesthetic} {Awareness}: {Revealing} {User} {Experiences} through {Second}-{Person} {Inquiry}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025714},
	abstract = {We consider kinesthetic awareness, the perception of our own body position and movement in space, as a critical value for embodied design within third wave HCI. We designed an interactive sound installation that supports kinesthetic awareness of a participant's micro-movements. The installation's interaction design uses continuous auditory feedback and leverages an adaptive mapping strategy, refining its sensitivity to increase sonic resolution at lower levels of movement activity. The installation uses field recordings as rich source materials to generate a sound environment that attunes to a participant's micro-movements. Through a qualitative study using a second-person interview technique, we gained nuanced insights into the participants' subjective experiences of the installation. These reveal consistent temporal patterns, as participants build on a gradual process of integration to increase the complexity and capacity of their kinesthetic awareness during interaction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Françoise, Jules and Candau, Yves and Fdili Alaoui, Sarah and Schiphorst, Thecla},
	year = {2017},
	pages = {5171--5183},
}

@inproceedings{pearson_chameleon_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Chameleon {Devices}: {Investigating} {More} {Secure} and {Discreet} {Mobile} {Interactions} via {Active} {Camouflaging}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025482},
	doi = {10.1145/3025453.3025482},
	abstract = {Many users value the ability to have quick and frequent sight of their mobiles when in public settings. However, in doing so, they expose themselves to potential risks, ranging from being targets of robbery to the more subtle social losses through being seen to be rude or inattentive to those around them. In nature, some animals can blend into their environments to avoid being eaten or to reduce their impact on the ecosystem around them. Taking inspiration from these evolved systems we investigate the notion of chameleon approaches for mobile interaction design. Our probes were motivated, inspired and refined through extended interactions with people drawn from contexts with differing ranges of security and privacy concerns. Through deployments on users' own devices, our prototypes show the value of the concept. The encouraging results motivate further research in materials and form factors that can provide more effective automatic plain-sight hiding.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pearson, Jennifer and Robinson, Simon and Jones, Matt and Joshi, Anirudha and Ahire, Shashank and Sahoo, Deepak and Subramanian, Sriram},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {camouflaging devices, mobiles, subtle notifications},
	pages = {5184--5196},
}

@incollection{mcreynolds_toys_2017,
	address = {New York, NY, USA},
	title = {Toys {That} {Listen}: {A} {Study} of {Parents}, {Children}, and {Internet}-{Connected} {Toys}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025735},
	abstract = {Hello Barbie, CogniToys Dino, and Amazon Echo are part of a new wave of connected toys and gadgets for the home that listen. Unlike the smartphone, these devices are always on, blending into the background until needed. We conducted interviews with parent-child pairs in which they interacted with Hello Barbie and CogniToys Dino, shedding light on children's expectations of the toys' "intelligence'" and parents' privacy concerns and expectations for parental controls. We find that children were often unaware that others might be able to hear what was said to the toy, and that some parents draw connections between the toys and similar tools not intended as toys (e.g., Siri, Alexa) with which their children already interact. Our findings illuminate people's mental models and experiences with these emerging technologies and will help inform the future designs of interactive, connected toys and gadgets. We conclude with recommendations for parents, designers, and policy makers.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McReynolds, Emily and Hubbard, Sarah and Lau, Timothy and Saraf, Aditya and Cakmak, Maya and Roesner, Franziska},
	year = {2017},
	pages = {5197--5207},
}

@incollection{van_kleek_better_2017,
	address = {New York, NY, USA},
	title = {Better the {Devil} {You} {Know}: {Exposing} the {Data} {Sharing} {Practices} of {Smartphone} {Apps}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025556},
	abstract = {Most users of smartphone apps remain unaware of what data about them is being collected, by whom, and how these data are being used. In this mixed methods investigation, we examine the question of whether revealing key data collection practices of smartphone apps may help people make more informed privacy-related decisions. To investigate this question, we designed and prototyped a new class of privacy indicators, called Data Controller Indicators (DCIs), that expose previously hidden information flows out of the apps. Our lab study of DCIs suggests that such indicators do support people in making more confident and consistent choices, informed by a more diverse range of factors, including the number and nature of third-party companies that access users' data. Furthermore, personalised DCIs, which are contextualised against the other apps an individual already uses, enable them to reason effectively about the differential impacts on their overall information exposure.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Van Kleek, Max and Liccardi, Ilaria and Binns, Reuben and Zhao, Jun and Weitzner, Daniel J. and Shadbolt, Nigel},
	year = {2017},
	pages = {5208--5220},
}

@inproceedings{moser_parents_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Parents? {And} {Children}?{S} {Preferences} about {Parents} {Sharing} about {Children} on {Social} {Media}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025587},
	doi = {10.1145/3025453.3025587},
	abstract = {Prior research shows that parents receive a number of benefits through sharing about their children online, but little is known about children?s perspectives about parent sharing. We conducted a survey with 331 parent-child pairs to examine parents? and children?s preferences about what parents share about their children on social media. We find that parents and children are in agreement in their perception of how often and how much information parents share about their children on social media. However, there is disagreement about the permission-seeking process: children believe their parents should ask permission more than parents think they should, and parents believe they should ask for permission more often than they actually do, especially younger parents. We describe two categories of content that children are okay, or not okay, with their parents sharing about them. We offer design directions for managing parent sharing.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Moser, Carol and Chen, Tianying and Schoenebeck, Sarita Y.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {social media, privacy, child, parent, permission, sharing},
	pages = {5221--5225},
}

@incollection{sailaja_challenges_2017,
	address = {New York, NY, USA},
	title = {Challenges of {Using} {Personal} {Data} to {Drive} {Personalised} {Electronic} {Programme} {Guides}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025986},
	abstract = {Media researchers are adopting personalisation in diverse ways to deliver increasingly context-sensitive and customised media experiences. This paper explores user attitudes towards a personalised Electronic Programme Guide which tailors media recommendations based on users' personal data. We used scenario based exploration enabled by the use of probes to convey the functionalities of data-driven personalised EPGs and to facilitate user discussions around its potential use. Users preferred personalised EPGs over current popular EPGs but expressed a significant lack of trust in the personal data collection that drives personalisation. Users appreciated the functionalities afforded by personalisation of media but were apprehensive about the implications of the personal data being collected about them, particularly in the context of their homes. This calls for the need to design future personalised media experiences that help enhance trust in these socio-technical settings.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sailaja, Neelima and Crabtree, Andy and Stenton, Phil},
	year = {2017},
	pages = {5226--5231},
}

@inproceedings{saksono_reflective_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Reflective {Informatics} {Through} {Family} {Storytelling}: {Self}-{Discovering} {Physical} {Activity} {Predictors}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025651},
	doi = {10.1145/3025453.3025651},
	abstract = {HCI research has increasingly examined how sensing technologies can help people capture and visualize data about their health-related behaviors. Yet, few systems help people reflect more fundamentally on the factors that influence behaviors such as physical activity (PA). To address this research gap, we take a novel approach, examining how such reflections can be stimulated through a medium that generations of families have used for reflection and teaching: storytelling. Through observations and interviews, we studied how 13 families interacted with a low-fidelity prototype, and their attitudes towards this tool. Our prototype used storytelling and interactive prompts to scaffold reflection on factors that impact children's PA. We contribute to HCI research by characterizing how families interacted with a story-driven reflection tool, and how such a tool can encourage critical processes for behavior change. Informed by the Transtheoretical Model, we present design implications for reflective informatics systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Saksono, Herman and Parker, Andrea G.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, storytelling, physical activity, families, personal health informatics, reflective informatics, sensemaking, technology-mediated reflection},
	pages = {5232--5244},
}

@inproceedings{hong_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting {Families} in {Reviewing} and {Communicating} about {Radiology} {Imaging} {Studies}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025754},
	doi = {10.1145/3025453.3025754},
	abstract = {Diagnostic radiology reports are increasingly being made available to patients and their family members. However, these reports are not typically comprehensible to lay recipients, impeding effective communication about report findings. In this paper, we present three studies informing the design of a prototype to foster patient-clinician communication about radiology report content. First, analysis of questions posted in online health forums helped us identify patients' information needs. Findings from an elicitation study with seven radiologists provided necessary domain knowledge to guide prototype design. Finally, a clinical field study with 14 pediatric patients, their parents and clinicians, revealed positive responses of each stakeholder when using the prototype to interact with and discuss the patient's current CT or MRI report and allowed us to distill three use cases: co-located communication, preparing for the consultation, and reviewing radiology data. We draw on our findings to discuss design considerations for supporting each of these use cases.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hong, Matthew K. and Feustel, Clayton and Agnihotri, Meeshu and Silverman, Max and Simoneaux, Stephen F. and Wilcox, Lauren},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {families, adolescents, patient-doctor communication, radiology report},
	pages = {5245--5256},
}

@inproceedings{berry_how_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {How {Values} {Shape} {Collaboration} {Between} {Patients} with {Multiple} {Chronic} {Conditions} and {Spousal} {Caregivers}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025923},
	doi = {10.1145/3025453.3025923},
	abstract = {Individuals with multiple chronic conditions (MCC) collaborate with spousal caregivers daily to pursue what is most important to their health and well-being. Previous research in human-computer interaction has supported individuals with chronic conditions or their caregivers, but little has supported both as a unit. We conducted a field study with 12 patient-caregiver dyads, all married and living together, to identify partners' values and how they shape collaborative management of MCC. Partners' coinciding values motivated them to empathize with and support each other in the face of challenges related to health and well-being. When their values were asymmetric, they perceived tensions between individual autonomy and their ability to coordinate with their partner. Systems to support partners in this context could help them overcome asymmetric values, but should balance this with support for individual autonomy.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Berry, Andrew B. L. and Lim, Catherine and Hartzler, Andrea L. and Hirsch, Tad and Wagner, Edward H. and Ludman, Evette and Ralston, James D.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {caregiver, collaboration, coordination, multiple chronic conditions, patient, self-care, self-management},
	pages = {5257--5270},
}

@inproceedings{miller_through_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Through the {Looking} {Glass}: {The} {Effects} of {Feedback} on {Self}-{Awareness} and {Conversational} {Behaviour} during {Video} {Chat}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025548},
	doi = {10.1145/3025453.3025548},
	abstract = {Video chat is a popular form of computer-mediated communication in a range of contexts from online job interviews to chatting with friends. Although seeing your own video feedback is the predominant interface design, self-awareness research suggests that seeing oneself could induce self-consciousness and affect interaction. We created a custom video chat application and asked pairs of strangers to engage in an online personal information exchange task with or without video feedback. Feedback increased self-awareness and the use of socially-focused words, and decreased the use of words expressing certainty. In addition, mixed-gender dyads rated themselves as more socially orientated with feedback than without, which was reflected in an increased use of inclusive pronouns and affiliation words, and fewer words expressing discrepancy. However, with feedback, same-gender dyads reported greater task orientation than mixed-gender dyads reflected in increased use of task-relevant words. We discuss design implications in contexts from remote therapy to online dating.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Miller, Matthew K. and Mandryk, Regan L. and Birk, Max V. and Depping, Ansgar E. and Patel, Tushita},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gender, self-awareness, cmc, cscw, feedback, video chat},
	pages = {5271--5283},
}

@inproceedings{krekhov_gestures_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gestures {From} the {Point} of {View} of an {Audience}: {Towards} {Anticipatable} {Interaction} of {Presenters} {With} {3D} {Content}.},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025641},
	doi = {10.1145/3025453.3025641},
	abstract = {Presenting content to an audience is important in several fields, including education, marketing, and entertainment. Therefore, the main goal of the presenter is to transport messages to the audience.The paper aims to improve the process of message transportation by providing audience-friendly and anticipatable gestures for the presenter to be used for 3D interaction with the content. For this purpose, we first gathered input from a potential audience through a Wizard of Oz experiment and implemented three coherent gesture sets using the Kinect. We conducted an online survey to evaluate the hypotheses regarding the anticipation rate and perceived user experience. In particular, two of our three gesture sets show tendencies to be intuitively predictable by an untrained, uninformed audience. As the three sets differ significantly in the anticipation level, we conclude that future improvements of such gestures would enhance the audience's ability to predict the intended actions even further.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Krekhov, Andrey and Emmerich, Katharina and Babinski, Maxim and Krüger, Jens},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {kinect, gestures, audience, presentation},
	pages = {5284--5294},
}

@incollection{licoppe_showing_2017,
	address = {New York, NY, USA},
	title = {Showing {Objects}: {Holding} and {Manipulating} {Artefacts} in {Video}-{Mediated} {Collaborative} {Settings}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025848},
	abstract = {In this paper we report on a pervasive practice in video-mediated communication: where participants show one another one or more objects. This is a distinct activity from others considered by researchers of video-mediated technologies that focus on a face-to-face orientation, or just on the support necessary to help people to refer to objects. We first present examples of this pervasive phenomenon in naturally occurring Skype conversations, revealing how this conduct is configured and organized within the interaction between participants. We reveal how the subtle adjustment of the position of the body, the head and gaze with respect to the handheld objects offers crucial resources for participants to achieve joint seeing. Then we report on a quite different setting, a naturalistic experiment where participants collaborate on a collective task with remote colleagues through maneuverable, orientable devices (Kubis). Again, in these experiments participants frequently show objects, and at times the devices provide additional resources to support these activities. But at other times they also involve some difficulties. We conclude by suggesting possible technological developments, some quite simple, others more radical, that might support participants to show objects, whether they are in domestic settings or undertaking work activities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Licoppe, Christian and Luff, Paul K. and Heath, Christian and Kuzuoka, Hideaki and Yamashita, Naomi and Tuncer, Sylvaine},
	year = {2017},
	pages = {5295--5306},
}

@inproceedings{otsuki_thirdeye_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ThirdEye}: {Simple} {Add}-on {Display} to {Represent} {Remote} {Participant}'s {Gaze} {Direction} in {Video} {Communication}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025681},
	doi = {10.1145/3025453.3025681},
	abstract = {A long-standing challenge in video-mediated communication systems is to represent a remote participant's gaze direction in local environments correctly. To address this issue, we developed ThirdEye, an add-on eye-display for a video communication system. This display is made from an artificial ulexite (TV rock) that is cut into a hemispherical shape, enabling light from the bottom surface to be projected onto the hemisphere surface. By drawing an appropriate ellipse on an LCD and placing ThirdEye over it, this system simulates an eyeball. Our experiment proved that an observer could perceive a remote Looker's gaze direction more precisely when the gaze was presented using ThirdEye compared to the case in which the gaze was presented using the Looker's face on a flat display.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Otsuki, Mai and Kawano, Taiki and Maruyama, Keita and Kuzuoka, Hideaki and Suzuki, Yusuke},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gaze awareness, telecommunication},
	pages = {5307--5312},
}

@inproceedings{suzuki_faceshare_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{FaceShare}: {Mirroring} with {Pseudo}-{Smile} {Enriches} {Video} {Chat} {Communications}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025574},
	doi = {10.1145/3025453.3025574},
	abstract = {"Mirroring" refers to the unconscious mimicry of another person's behaviors, such as their facial expressions. Mirroring has many positive effects, such as enhancing closeness and improving the flow of a conversation, which enriches the quality of communication. Our study set out to devise a means of evoking these positive effects in a video chat without any conscious effort of participants. We constructed a videophone system, called FaceShare, which can deform the user's face into a smile in response to their partner's smiling. That is, our system generates mirroring by producing a pseudo-smile through image processing. We conducted an experiment in which pairs of participants had brief conversations via FaceShare. The results implied that mirroring using the pseudo-smile lets the mimicker, whose face is deformed according to the expressions of their partner, feel a closeness, and improves the flow of the conversation for both the mimicker and the mimickee, who sees the mimicker's deformed face.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Suzuki, Keita and Yokoyama, Masanori and Yoshida, Shigeo and Mochizuki, Takayoshi and Yamada, Tomohiro and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {facial expression, cscw, telepresence, mirroring, transcendent telepresence},
	pages = {5313--5317},
}

@inproceedings{pohl_squeezeback_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Squeezeback: {Pneumatic} {Compression} for {Notifications}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025526},
	doi = {10.1145/3025453.3025526},
	abstract = {Current mobile devices commonly use vibration feedback to signal incoming notifications. However, vibration feedback exhibits strong attention capture, limiting its use to short periods and prominent notifications. Instead, we investigate the use of compression feedback for notifications, which scales from subtle stimuli to strong ones and can provide sustained stimuli over longer periods. Compression feedback utilizes inflatable straps around a user's limbs, a form factor allowing for easy integration into many common wearables. We explore technical aspects of compression feedback and investigate its psychophysical properties with several lab and in situ studies. Furthermore, we show how compression feedback enables reactive feedback. Here, deflation patterns are used to reveal further information on a user's query. We also compare compression and vibrotactile feedback and find that they have similar performance.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pohl, Henning and Brandes, Peter and Ngo Quang, Hung and Rohs, Michael},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {notifications, wearable, mobile haptics, blood pressure, compression feedback, pneumatics, pressure feedback},
	pages = {5318--5330},
}

@inproceedings{gong_cito_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Cito: {An} {Actuated} {Smartwatch} for {Extended} {Interactions}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025568},
	doi = {10.1145/3025453.3025568},
	abstract = {We propose and explore actuating a smartwatch face to enable extended interactions. Five face movements are defined: rotation, hinging, translation, rising, and orbiting. These movements are incorporated into interaction techniques to address limitations of a fixed watch face. A 20-person study uses concept videos of a passive low fidelity prototype to confirm the usefulness of the actuated interaction techniques. A second 20-person study uses 3D rendered animations to access social acceptability and perceived comfort for different actuation dynamics and usage contexts. Finally, we present Cito, a high-fidelity proof-of-concept hardware prototype that investigates technical challenges.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gong, Jun and Li, Lan and Vogel, Daniel and Yang, Xing-Dong},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {interaction techniques, smartwatch, actuated ui},
	pages = {5331--5345},
}

@inproceedings{yuan_how_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {How {Busy} {Are} {You}? {Predicting} the {Interruptibility} {Intensity} of {Mobile} {Users}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025946},
	doi = {10.1145/3025453.3025946},
	abstract = {Smartphones frequently notify users about newly available messages or other notifications. It can be very disruptive when these notifications interrupt users while they are busy. Our work here is based on the observation that people usually exhibit different levels of busyness at different contexts. This means that classifying users' interruptibility as a binary status, interruptible or not interruptible, is not sufficient to accurately measure their availability towards smartphone interruptions. In this paper, we propose, implement and evaluate a two-stage hierarchical model to predict people's interruptibility intensity. Our work is the first to introduce personality traits into interruptibility prediction model, and we found that personality data improves the prediction significantly. Our model bootstraps the prediction with similar people's data, and provides a good initial prediction for users whose individual models have not been trained on their own data yet. Overall prediction accuracy of our model can reach 66.1\%.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Fengpeng and Gao, Xianyi and Lindqvist, Janne},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {notifications, interruptibility, context, predictive models},
	pages = {5346--5360},
}

@inproceedings{widdicks_demand_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Demand {Around} the {Clock}: {Time} {Use} and {Data} {Demand} of {Mobile} {Devices} in {Everyday} {Life}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025730},
	doi = {10.1145/3025453.3025730},
	abstract = {Motivated by mobile devices' growing demand for connectivity, and concern in HCI with the energy intensity and sustainability of networked services, in this paper we reveal the impact of applications on smartphones and tablets in terms of network demand and time use. Using a detailed mixed methods study with eight participants, we first provide an account of how data demand has meaning and utility in our participants' social practices, and the timing and relative impacts of these. We then assess the scale of this demand by drawing comparison between our fine-grained observations and a more representative dataset of 398 devices from the Device Analyzer corpus. Our results highlight the significant categories of data demanding practice, and the identification of where changes in app time and duration of use might reduce or shift demand to reduce services' impacts.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Widdicks, Kelly and Bates, Oliver and Hazas, Mike and Friday, Adrian and Beresford, Alastair R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {sustainability, data demand, demand designed into practices, ict},
	pages = {5361--5372},
}

@inproceedings{khovanskaya_reworking_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Reworking the {Gaps} between {Design} and {Ethnography}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026051},
	doi = {10.1145/3025453.3026051},
	abstract = {Since Dourish's critique of 'implications for design' [15], researchers have asked how design and ethnography should or could relate in HCI. Here we reflect on two experiences with cross-informing ongoing ethnographic investigation with the early stages of research through design. One uses speculative design to reflect on and inform ethnographic fieldwork on busyness in middle-class familes; the other uses speculative design to complement late-stage analysis of a historical ethnography of rural technological infrastructure. Rather than trying to do away with the gap between ethnography and design by seamlessly integrating the two processes, we reworked the relationship between ethnography and design by closing the gap in the temporal workflows while simultaneously maintaining a distinction in the performance of the two roles. We found that this new gap resulted in a series of misunderstandings; but by putting the two roles in active dialogue, we were able leverage misunderstandings into mutual benefit.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Khovanskaya, Vera and Sengers, Phoebe and Mazmanian, Melissa and Darrah, Charles},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ethnography, speculative design, inventive methods},
	pages = {5373--5385},
}

@inproceedings{elsden_speculative_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {On {Speculative} {Enactments}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025503},
	doi = {10.1145/3025453.3025503},
	abstract = {Speculative Enactments are a novel approach to speculative design research with participants. They invite the empirical analysis of participants acting amidst speculative but consequential circumstances. HCI as a broadly pragmatic, experience-centered, and participant-focused field is well placed to innovate methods that invite first-hand interaction and experience with speculative design projects. We discuss three case studies of this approach in practice, based on our own work: Runner Spotters, Metadating and a Quantified Wedding. In distinguishing Speculative Enactments we offer not just practical guidelines, but a set of conceptual resources for researchers and practitioners to critique the different contributions that speculative approaches make to HCI discourse.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Elsden, Chris and Chatting, David and Durrant, Abigail C. and Garbett, Andrew and Nissen, Bettina and Vines, John and Kirk, David S.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design methods, research through design, speculative design, design fiction, critical futures, data-driven life},
	pages = {5386--5399},
}

@inproceedings{blythe_research_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Research {Fiction}: {Storytelling}, {Plot} and {Design}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026023},
	doi = {10.1145/3025453.3026023},
	abstract = {What kind of stories and plots do researchers of Human Computer Interaction draw on when they make fictions? This paper applies the "basic plots" identified in the study of literature to scenarios, speculative design and design fiction. Traditional HCI scenarios employ the plot of "Overcoming the Monster" where the monster is some problem to be solved. Much of the commentary on critical, speculative or adversarial design also draws on this plot as it attempts to overcome monsters like public apathy or a lack of debate. Design Fiction more frequently takes the form of a "Voyage and Return" or a "Quest". The paper argues that a better understanding of plot and storytelling could contribute to more reflective research fiction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Blythe, Mark},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {personas, speculative design, design fiction, adversarial design, critical design, scenarios, solutionism},
	pages = {5400--5411},
}

@inproceedings{schlesinger_intersectional_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Intersectional {HCI}: {Engaging} {Identity} through {Gender}, {Race}, and {Class}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025766},
	doi = {10.1145/3025453.3025766},
	abstract = {Understanding users becomes increasingly complicated when we grapple with various overlapping attributes of an individual's identity. In this paper we introduce intersectionality as a framework for engaging with the complexity of users' "and authors" "identities", and situating these identities in relation to their contextual surroundings. We conducted a meta-review of identity representation in the CHI proceedings, collecting a corpus of 140 manuscripts on gender, ethnicity, race, class, and sexuality published between 1982-2016. Drawing on this corpus, we analyze how identity is constructed and represented in CHI research to examine intersectionality in a human-computer interaction (HCI) context. We find that previous identity-focused research tends to analyze one facet of identity at a time. Further, research on ethnicity and race lags behind research on gender and socio-economic class. We conclude this paper with recommendations for incorporating intersectionality in HCI research broadly, encouraging clear reporting of context and demographic information, inclusion of author disclosures, and deeper engagement with identity complexities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Schlesinger, Ari and Edwards, W. Keith and Grinter, Rebecca E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gender, race, identity, intersectionality, class, ethnicity, intersectional hci, socio-economic status},
	pages = {5412--5427},
}

@inproceedings{nguyen_vremiere_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Vremiere: {In}-{Headset} {Virtual} {Reality} {Video} {Editing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025675},
	doi = {10.1145/3025453.3025675},
	abstract = {Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Cuong and DiVerdi, Stephen and Hertzmann, Aaron and Liu, Feng},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, video editing},
	pages = {5428--5438},
}

@inproceedings{wood_theyre_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {"{They}'re {Just} {Tixel} {Pits}, {Man}": {Disputing} the '{Reality}' of {Virtual} {Reality} {Pornography} through the {Story} {Completion} {Method}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025762},
	doi = {10.1145/3025453.3025762},
	abstract = {Pornography is a substantial part of humans' everyday interaction with computers, yet to date the topic has been underconsidered by HCI. Here, we examine some of the common cultural ideals non-experts constructed of a "new" pornographic experience - Virtual Reality (VR) Porn - through use of the "Story Completion Method". Forty five participants completed a story stem about a male character who was about to have his "very first virtual reality porn experience". Through our analysis, we demonstrate a narrative of a "perfect", idealised sexual experience, as well as one which emphasised the imagined "precarious" and dangerous consequences around this technology use. We indicate how the stories reproduced ideals around heteronormativity and hegemonic masculinity, suggesting an agenda of "Designing for Eroticism" as a tactic which could avoid such problematic discourses. We also suggest the opportunities and challenges presented through use of the "Story Completion Method".},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wood, Matthew and Wood, Gavin and Balaam, Madeline},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, thematic analysis, speculative design, design fiction, porn, pornography},
	pages = {5439--5451},
}

@inproceedings{peiris_thermovr_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ThermoVR}: {Exploring} {Integrated} {Thermal} {Haptic} {Feedback} with {Head} {Mounted} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025824},
	doi = {10.1145/3025453.3025824},
	abstract = {Head Mounted Displays (HMDs) provide a promising opportunity for providing haptic feedback on the head for an enhanced immersive experience. In ThermoVR, we integrated five thermal feedback modules on the HMD to provide thermal feedback directly onto the user's face. We conducted evaluations with 15 participants using two approaches: Firstly, we provided simultaneously actuated thermal stimulations (hot and cold) as directional cues and evaluated the accuracy of recognition; secondly, we evaluated the overall immersive thermal experience that the users experience when provided with thermal feedback on the face. Results indicated that the recognition accuracy for cold stimuli were of approx. 89.5\% accuracy while the accuracy for hot stimuli were 68.6\%. Also, participants reported that they felt a higher level of immersion on the face when all modules were simultaneously stimulated (hot and cold). The presented applications demonstrate the ThermoVR's directional cueing and immersive experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Peiris, Roshan Lalintha and Peng, Wei and Chen, Zikun and Chan, Liwei and Minamizawa, Kouta},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {head mounted display, thermal display, thermal haptics},
	pages = {5452--5456},
}

@inproceedings{walker_efficient_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Efficient {Typing} on a {Visually} {Occluded} {Physical} {Keyboard}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025783},
	doi = {10.1145/3025453.3025783},
	abstract = {The rise of affordable head-mounted displays (HMDs) has raised questions about how to best design user interfaces for this technology. This paper focuses on the use of HMDs for home and office applications that require substantial text input. A physical keyboard is a familiar and effective text input device in normal desktop computing. But without additional camera technology, an HMD occludes all visual feedback about a user's hand position over the keyboard. We describe a system that assists HMD users in typing on a physical keyboard. Our system has a virtual keyboard assistant that provides visual feedback inside the HMD about a user's actions on the physical keyboard. It also provides powerful automatic correction of typing errors by extending a state-of-the-art touchscreen decoder. In a study with 24 participants, we found our virtual keyboard assistant enabled users to type more accurately on a visually-occluded keyboard. We found users wearing an HMD could type at over 40 words-per-minute while obtaining an error rate of less than 5\%.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Walker, James and Li, Bochao and Vertanen, Keith and Kuhl, Scott},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {head-mounted display, text entry, decoder, physical keyboard},
	pages = {5457--5461},
}

@inproceedings{boy_showing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Showing {People} {Behind} {Data}: {Does} {Anthropomorphizing} {Visualizations} {Elicit} {More} {Empathy} for {Human} {Rights} {Data}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025512},
	doi = {10.1145/3025453.3025512},
	abstract = {We investigate the impact of using anthropomorphized data graphics over standard charts on viewers' empathy for, and prosocial behavior toward suffering populations, in the context of human rights narratives. We present a series of experiments conducted on Amazon Mechanical Turk, in which we compare various forms of anthropomorphized data graphics-ranging from a single human figure that "fills up" to show proportional data, to separated groups of individual human beings-with a standard chart baseline. Each experiment uses two carefully crafted human rights data-driven stories to present the graphics. Contrary to our expectations, we consistently find that anthropomorphized data graphics and standard charts have very similar effects on empathy and prosocial behavior.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Boy, Jeremy and Pandey, Anshul Vikram and Emerson, John and Satterthwaite, Margaret and Nov, Oded and Bertini, Enrico},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {empathy, anthropographics, human rights, information visualization for the people, prosocial behavior},
	pages = {5462--5474},
}

@inproceedings{dimara_narratives_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Narratives in {Crowdsourced} {Evaluation} of {Visualizations}: {A} {Double}-{Edged} {Sword}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025870},
	doi = {10.1145/3025453.3025870},
	abstract = {We explore the effects of providing task context when evaluating visualization tools using crowdsourcing. We gave crowdsource workers i) abstract information visualization tasks without any context, ii) tasks where we added semantics to the dataset, and iii) tasks with two types of backstory narratives: an analytic narrative and a decision-making narrative. Contrary to our expectations, we did not find evidence that adding data semantics increases accuracy, and further found that our backstory narratives can even decrease accuracy. Adding dataset semantics can however increase attention and provide subjective benefits in terms of confidence, perceived easiness, task enjoyability and perceived usefulness of the visualization. Nevertheless, our backstory narratives did not appear to provide additional subjective benefits. These preliminary findings suggest that narratives may have complex and unanticipated effects, calling for more studies in this area.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dimara, Evanthia and Bezerianos, Anastasia and Dragicevic, Pierre},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {evaluation, narrative, information visualization, crowdsource, decision making, instructions},
	pages = {5475--5484},
}

@inproceedings{alper_visualization_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Visualization {Literacy} at {Elementary} {School}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025877},
	doi = {10.1145/3025453.3025877},
	abstract = {This work advances our understanding of children's visualization literacy, and aims to improve it through a novel approach for teaching visualization at elementary school. We first contribute an analysis of data graphics and activities employed in grade K to 4 educational materials, and the results of a survey conducted with 16 elementary school teachers. We find that visualization education could benefit from integrating pedagogical strategies for teaching abstract concepts with established interactive visualization techniques. Building on these insights, we develop and study design principles for novel interactive teaching material aimed at increasing children's visualization literacy. We specifically contribute C'est La Vis, an online platform for teachers and students to respectively teach and learn about pictographs and bar charts, and report on our initial observations of its use in grades K and 2.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alper, Basak and Riche, Nathalie Henry and Chevalier, Fanny and Boy, Jeremy and Sezgin, Metin},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {visualization literacy, qualitative analysis},
	pages = {5485--5497},
}

@inproceedings{du_finding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Finding {Similar} {People} to {Guide} {Life} {Choices}: {Challenge}, {Design}, and {Evaluation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025777},
	doi = {10.1145/3025453.3025777},
	abstract = {People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {visual analytics, decision making, similarity, temporal event analytics, temporal visualization},
	pages = {5498--5544},
}

@inproceedings{shannon_better_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Better {Organization} or a {Source} of {Distraction}? {Introducing} {Digital} {Peer} {Feedback} to a {Paper}-{Based} {Classroom}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025564},
	doi = {10.1145/3025453.3025564},
	abstract = {Peer feedback is a central activity for project-based design education. The prevalence of devices carried by students and the emergence of novel peer feedback systems enables the possibility of collecting and sharing feedback immediately between students during class. However, pen and paper is thought to be more familiar, less distracting for students, and easier for instructors to implement and manage. To evaluate the efficacy of in-class digital feedback systems, we conducted a within-subjects study with 73 students during two weeks of a game design course. After short student presentations, while instructors provided verbal feedback, peers provided feedback either on paper or through a device. The study found that both methods yielded comments of similar quality and quantity, but the digital approach provided additional ways for students to participate and required less effort from the instructors. While both methods produced similar behaviors, students held inaccurate perceptions about their behavior with each method. We discuss design implications for technologies to support in-class feedback exchange.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Shannon, Amy and Sciuto, Alex and Hu, Danielle and Dow, Steven P. and Hammer, Jessica},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {peer feedback, computer-supported collaborative learning, in-class activities, interactive learning techniques},
	pages = {5545--5555},
}

@inproceedings{kharrufa_group_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Group {Spinner}: {Recognizing} and {Visualizing} {Learning} in the {Classroom} for {Reflection}, {Communication}, and {Planning}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025679},
	doi = {10.1145/3025453.3025679},
	abstract = {Group Spinner is a digital visual tool intended to help teachers observe and reflect on children's collaborative technology-enhanced learning activities in the classroom. We describe the design of Group Spinner, which was informed by activity theory, previous work and teachers' focus group feedback. Based on a radar chart and a set of indicators, Group Spinner allows teachers to record in-class observations as to different aspects of group learning and learning behaviors, beyond the limited knowledge acquisition measures. Our exploratory study involved 6 teachers who used the tool for a total of 23 classes in subjects ranging from Maths and Geography to Sociology and Art. Semi-structured interviews with these teachers revealed a number of different uses of the tool. Depending on their experience and pedagogy, teachers considered Group Spinner to be a valuable tool to support awareness, reflection, communication, and/or planning.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kharrufa, Ahmed and Rix, Sally and Osadchiy, Timur and Preston, Anne and Olivier, Patrick},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {reflection, collaboration, schools/educational setting, observation, radar-chart, technology enhanced learning},
	pages = {5556--5567},
}

@inproceedings{zhu_vivo_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ViVo}: {Video}-{Augmented} {Dictionary} for {Vocabulary} {Learning}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025779},
	doi = {10.1145/3025453.3025779},
	abstract = {Research on Computer-Assisted Language Learning (CALL) has shown that the use of multimedia materials such as images and videos can facilitate interpretation and memorization of new words and phrases by providing richer cues than text alone. We present ViVo, a novel video-augmented dictionary that provides an inexpensive, convenient, and scalable way to exploit huge online video resources for vocabulary learning. ViVo automatically generates short video clips from existing movies with the target word highlighted in the subtitles. In particular, we apply a word sense disambiguation algorithm to identify the appropriate movie scenes with adequate contextual information for learning. We analyze the challenges and feasibility of this approach and describe our interaction design. A user study showed that learners were able to retain nearly 30\% more new words with ViVo than with a standard bilingual dictionary days after learning. They preferred our video-augmented dictionary for its benefits in memorization and enjoyable learning experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Yeshuang and Wang, Yuntao and Yu, Chun and Shi, Shaoyun and Zhang, Yankai and He, Shuang and Zhao, Peijun and Ma, Xiaojuan and Shi, Yuanchun},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {subtitles, dictionary, movie clips, vocabulary learning},
	pages = {5568--5579},
}

@inproceedings{wauck_class_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {From in the {Class} or in the {Wild}? {Peers} {Provide} {Better} {Design} {Feedback} {Than} {External} {Crowds}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025477},
	doi = {10.1145/3025453.3025477},
	abstract = {As demand for design education increases, instructors are struggling to provide timely, personalized feedback for student projects. Gathering feedback from classroom peers and external crowds offer scalable approaches, but there is little evidence of how they compare. We report on a study in which students (n=127) created early- and late-stage prototypes as part of nine-week projects. At each stage, students received feedback from peers and external crowds: their own social networks, online communities, and a task market. We measured the quality, quantity and valence of the feedback and the actions taken on it, and categorized its content using a taxonomy of critique discourse. The study found that peers produced feedback that was of higher perceived quality, acted upon more, and longer compared to the crowds. However, crowd feedback was found to be a viable supplement to peer feedback and students preferred it for projects targeting specialized audiences. Feedback from all sources spanned only a subset of the critique categories. Instructors may fill this gap by further scaffolding feedback generation. The study contributes insights for how to best utilize different feedback sources in project-based courses.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wauck, Helen and Yen, Yu-Chun (Grace) and Fu, Wai-Tat and Gerber, Elizabeth and Dow, Steven P. and Bailey, Brian P.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {learning, design methods, crowdsourcing, feedback},
	pages = {5580--5591},
}

@inproceedings{xia_collection_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Collection {Objects}: {Enabling} {Fluid} {Formation} and {Manipulation} of {Aggregate} {Selections}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025554},
	doi = {10.1145/3025453.3025554},
	abstract = {Despite the long development of Graphical User Interfaces, working with multiple graphical objects remains a challenge, due to the difficulties of forming complex selections, ambiguities of operations, and tediousness of repetitively unselect-reselect or ungroup-regroup objects. Instead of tackling them as individual problems, we attribute it to the lack of system support to the general selection-action cycles. We propose Collection Objects to not only support a single fast selection-action cycle but also allow multiple cycles to be chained together into a fluid workflow. Collection Objects unifies selection, grouping, and manipulation of aggregate selections into a single object, with which selection can be composed with various techniques, modified for later actions, grouped with objects inside still directly accessible, and quasi-moded for less context switching. We implemented Collection Object in the context of a vector drawing application with simultaneous pen and touch input. Results of an expert evaluation show that Collection Objects holds considerable promises for fluid interaction with multiple objects.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xia, Haijun and Araujo, Bruno and Wigdor, Daniel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {collection object, object-oriented interaction, pen and touch},
	pages = {5592--5604},
}

@inproceedings{klamka_illumipaper_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{IllumiPaper}: {Illuminated} {Interactive} {Paper}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025525},
	doi = {10.1145/3025453.3025525},
	abstract = {Due to their simplicity and flexibility, digital pen-and-paper solutions have a promising potential to become a part of our daily work. Unfortunately, they lack dynamic visual feedback and thereby restrain advanced digital functionalities. In this paper, we investigate new forms of paper-integrated feedback, which build on emerging paper-based electronics and novel thin-film display technologies. Our approach focuses on illuminated elements, which are seamlessly integrated into standard paper. For that, we introduce an extended design space for paper-integrated illuminations. As a major contribution, we present a systematic feedback repertoire for real-world applications including feedback components for innovative paper interaction tasks in five categories. Furthermore, we contribute a fully-functional research platform including a paper-controller, digital pen and illuminated, digitally controlled papers that demonstrate the feasibility of our techniques. Finally, we report on six interviews, where experts rated our approach as intuitive and very usable for various applications, in particular educational ones.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Klamka, Konstantin and Dachselt, Raimund},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {pen interaction, anoto, augmented paper, digital pen and paper, electro-luminescence, thin-film display, visual feedback},
	pages = {5605--5618},
}

@inproceedings{cattan_does_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Does {Practice} {Make} {Perfect}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025585},
	doi = {10.1145/3025453.3025585},
	abstract = {Touch latency has been shown to deteriorate users' performances at levels as low as 25 ms, but this was tested only in short experimental sessions. Real life usage of touchscreens covers much longer periods. It provides training which could lead to reduce the impact of latency.We investigate users' ability to compensate for touch latency with training. Two groups of participants were trained on a tracking task during ten different days over two weeks with either high or low latency. The gap of performances between the two groups, observed at the beginning of the experiment, was reduced by 54 \% after training. Users can thus compensate for latency, at least partially. These results nuance the negative effects of touch latency reported in previous work. They suggest that long-term studies could provide better insights on users' behaviors when dealing with touch latency.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cattan, Elie and Rochet-Capellan, Amélie and Perrier, Pascal and Bérard, François},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {learning, training, direct-touch, latency, tracking task, user performances},
	pages = {5619--5629},
}

@inproceedings{fruchard_markpad_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{MarkPad}: {Augmenting} {Touchpads} for {Command} {Selection}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025486},
	doi = {10.1145/3025453.3025486},
	abstract = {We present MarkPad, a novel interaction technique taking advantage of the touchpad. MarkPad allows creating a large number of size-dependent gestural shortcuts that can be spatially organized as desired by the user. It relies on the idea of using visual or tactile marks on the touchpad or a combination of them. Gestures start from a mark on the border and end on another mark anywhere. MarkPad does not conflict with standard interactions and provides a novice mode that acts as a rehearsal of the expert mode. A first study showed that an accuracy of 95\% could be achieved for a dense configuration of tactile and/or visual marks allowing many gestures. Performance was 5\% lower in a second study where the marks were only on the borders. A last study showed that borders are rarely used, even when the users are unaware of the technique. Finally, we present a working prototype and briefly report on how it was used by two users for a few months.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fruchard, Bruno and Lecolinet, Eric and Chapuis, Olivier},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {tactile feedback, spatial memory, bezel gestures, gestural interaction, marking menus, touchpad, user-defined gestures},
	pages = {5630--5642},
}

@inproceedings{arora_experimental_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Experimental {Evaluation} of {Sketching} on {Surfaces} in {VR}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025474},
	doi = {10.1145/3025453.3025474},
	abstract = {Sketching in immersive 3D virtual reality (VR) environments has great potential for a variety of interactive 3D design applications. Precisely sketching the intended strokes in mid-air, however, can be a challenge. In this paper, we present a set of controlled studies to analyze the factors affecting human ability to sketch freely in a 3D VR environment. In our first study, we directly compare traditional sketching on a physical surface to sketching in VR, with and without a physical surface to rest the stylus on. Our results indicate that the lack of a physical drawing surface is a major cause of inaccuracies in VR drawing, and that the effect is dependent on the orientation of the drawing surface. In a second experiment, we evaluate the extent to which visual guidance can compensate for the loss of sketching precision in VR. We found that while additional visual guidance improves positional accuracy, it can be detrimental to the aesthetic quality of strokes. We conclude by distilling our experimental findings into design guidelines for sketching tools in immersive 3D environments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Arora, Rahul and Kazi, Rubaiat Habib and Anderson, Fraser and Grossman, Tovi and Singh, Karan and Fitzmaurice, George},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, 3d drawing, motor ability, visual factors},
	pages = {5643--5654},
}

@inproceedings{mcgill_i_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {I {Am} {The} {Passenger}: {How} {Visual} {Motion} {Cues} {Can} {Influence} {Sickness} {For} {In}-{Car} {VR}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026046},
	doi = {10.1145/3025453.3026046},
	abstract = {This paper explores the use of VR Head Mounted Displays (HMDs) in-car and in-motion for the first time. Immersive HMDs are becoming everyday consumer items and, as they offer new possibilities for entertainment and productivity, people will want to use them during travel in, for example, autonomous cars. However, their use is confounded by motion sickness caused in-part by the restricted visual perception of motion conflicting with physically perceived vehicle motion (accelerations/rotations detected by the vestibular system). Whilst VR HMDs restrict visual perception of motion, they could also render it virtually, potentially alleviating sensory conflict. To study this problem, we conducted the first on-road and in motion study to systematically investigate the effects of various visual presentations of the real-world motion of a car on the sickness and immersion of VR HMD wearing passengers. We established new baselines for VR in-car motion sickness, and found that there is no one best presentation with respect to balancing sickness and immersion. Instead, user preferences suggest different solutions are required for differently susceptible users to provide usable VR in-car. This work provides formative insights for VR designers and an entry point for further research into enabling use of VR HMDs, and the rich experiences they offer, when travelling.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McGill, Mark and Ng, Alexander and Brewster, Stephen},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, mixed reality, hmd, automobile, autonomous car, in-car, in-motion, motion sickness, passenger},
	pages = {5655--5668},
}

@inproceedings{rietzler_vair_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{VaiR}: {Simulating} {3D} {Airflows} in {Virtual} {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026009},
	doi = {10.1145/3025453.3026009},
	abstract = {The integration of multi-sensory stimuli, e.g. haptic airflow, in virtual reality (VR) has become an important topic of VR research and proved to enhance the feeling of presence. VaiR focuses on an accurate and realistic airflow simulation that goes far beyond wind. While previous works on the topic of airflow in VR are restricted to wind, while focusing on the feeling of presence, there is to the best of our knowledge no work considering the conceptual background or on the various application areas. Our pneumatic prototype emits short and long term flows with a minimum delay and is able to animate wind sources in 3D space around the user's head. To get insights on how airflow can be used in VR and how such a device should be designed, we arranged focus groups and discussed the topic. Based on the gathered knowledge, we developed a prototype which proved to increase presence, as well as enjoyment and realism, while not disturbing the VR experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rietzler, Michael and Plaumann, Katrin and Kränzle, Taras and Erath, Marcel and Stahl, Alexander and Rukzio, Enrico},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, presence, evaluation, airflow},
	pages = {5669--5677},
}

@inproceedings{lee_collaborative_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Collaborative {Map} {Making}: {A} {Reflexive} {Method} for {Understanding} {Matters} of {Concern} in {Design} {Research}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025535},
	doi = {10.1145/3025453.3025535},
	abstract = {HCI researchers investigating the politics of technology design have recently focused on how design practice can tackle "Matters of Concern" - complex social issues perceived and experienced in multiple ways. These researchers suggest design research can generate new networks of human and non-human actors to express and act on these issues. Prior studies, however, tend to restrict their networks within traditional boundaries (e.g. existing organizations, local communities) and categories (e.g. human/nonhuman binary) without examining their significance for participants. We suggest collaborative map making as a reflexive method for understanding current Matters of Concern from the perspectives of diverse actors, not just researchers. As case studies of the method's use, we present two studies of domestic computing technologies in the US and South Korea, which show how collaborative map making allows salient networks to expand beyond the individual actors in the home to local and global power issues outside of boundaries (e.g. physical house) and categories (e.g. private/public space) commonly recognized in HCI. Our methodology provides HCI researchers with a way to understand existing Matters of Concern, so they can position themselves to address and act on these issues.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Hee Rin and Šabanović, Selma and Kwak, Sonya S.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {home, actor network, collaborative mapping, matters of concern, reflexivity, situational analysis},
	pages = {5678--5689},
}

@inproceedings{henley_toward_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Toward {Principles} for the {Design} of {Navigation} {Affordances} in {Code} {Editors}: {An} {Empirical} {Investigation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025645},
	doi = {10.1145/3025453.3025645},
	abstract = {Design principles are a key tool for creators of interactive systems; however, a cohesive set of principles has yet to emerge for the design of code editors. In this paper, we conducted a between-subjects empirical study comparing the navigation behaviors of 32 professional LabVIEW programmers using two different code-editor interfaces: the ubiquitous tabbed editor and the experimental Patchworks editor. Our analysis focused on how the programmers arranged and navigated among open information patches (i.e., code modules and program output). Key findings of our study included that Patchworks users made significantly fewer click actions per navigation, juxtaposed patches side by side significantly more, and exhibited significantly fewer navigation mistakes than tabbed-editor users. Based on these findings and more, we propose five general principles for the design of effective navigation affordances in code editors.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Henley, Austin Z. and Fleming, Scott D. and Luong, Maria V.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user study, design principles, navigation, programming environments, visual programming languages},
	pages = {5690--5702},
}

@inproceedings{roy_follow-my-lead_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Follow-{My}-{Lead}: {Intuitive} {Indoor} {Path} {Creation} and {Navigation} {Using} {Interactive} {Videos}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025976},
	doi = {10.1145/3025453.3025976},
	abstract = {We present Follow-My-Lead, an alternative indoor navigation technique that uses visual information recorded on an actual navigation path as a navigational guide. Its design revealed a trade-off between the fidelity of information provided to users and their effort to acquire it. Our first experiment revealed that scrolling through a continuous image stream of the navigation path is highly informative, but it becomes tedious with constant use. Discrete image checkpoints require less effort, but can be confusing. A balance may be struck by adding fast video transitions between image checkpoints, but precise control is required to handle difficult situations. Authoring still image checkpoints is also difficult, and this inspired us to invent a new technique using video checkpoints. We conducted a second experiment on authoring and navigation performance and found video checkpoints plus fast video transitions to be better than both image checkpoints plus fast video transitions and traditional written instructions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Roy, Quentin and Perrault, Simon T. and Zhao, Shengdong and Davis, Richard C. and Pattena Vaniyar, Anuroop and Vechev, Velko and Lee, Youngki and Misra, Archan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {video, indoor navigation, wearable computing, egocentric visual navigation., leader-follower, mobile computing, smartglasses},
	pages = {5703--5715},
}

@inproceedings{stein_mobility_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Mobility in {Later} {Life}: {Appropriation} of an {Integrated} {Transportation} {Platform}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025672},
	doi = {10.1145/3025453.3025672},
	abstract = {We present the results of a design case study focusing on supporting the daily transportation of elderly in Germany. We conceptualized, developed and studied the appropriation of a transportation information system intended to ease switching between different transportation modes. Based on a literature review and a context study with 21 interviews we explored routinized transport mode usage and barriers when switching between modes. Iteratively, we co-designed a transport platform accessible via a website, a mobile app, and an iTV app. We further looked at the appropriation of the platform into the daily lives of 19 persons. Studying the appropriation highlighted different factors that facilitate the adoption of alternative transport options. The factors included reducing uncertainty, complementing transport information with context information (e.g. weather) and providing informational access based on the user's preferences as well as fitting in with the situational needs (activity related).},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Stein, Martin and Meurer, Johanna and Boden, Alexander and Wulf, Volker},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {participatory design, qualitative research, mobility, elderly, transportation},
	pages = {5716--5729},
}

@inproceedings{mcnally_gains_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gains from {Participatory} {Design} {Team} {Membership} as {Perceived} by {Child} {Alumni} and {Their} {Parents}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025622},
	doi = {10.1145/3025453.3025622},
	abstract = {The direct gains children perceive from their membership on Participatory Design (PD) teams are seldom the focus of research studies. Yet, how HCI practitioners choose to include children in PD methods may influence the value participants see in their participation, and thereafter the outcomes of PD processes. To understand what gains former child members of a PD team perceive from their participation we conducted a two-part study. In Study 1 we surveyed and interviewed child alumni of a PD team to determine gains that are perceived first-hand. In Study 2 we obtained a secondary perspective by surveying and interviewing parents of alumni. We report on the perceived gains to former participants that were identified and described in these two studies-including collaboration, communication, design process knowledge, and confidence. We reflect on our findings through discussions of the continued applicability of gains, new opportunities, and implications for PD practitioners and methods.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McNally, Brenna and Mauriello, Matthew Louis and Guha, Mona Leigh and Druin, Allison},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, co-design, participatory design, gains, participant perspective},
	pages = {5730--5741},
}

@inproceedings{yip_examining_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Examining {Adult}-{Child} {Interactions} in {Intergenerational} {Participatory} {Design}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025787},
	doi = {10.1145/3025453.3025787},
	abstract = {Prior studies have focused on child interactions in participatory design (PD) with adults and children, but less is known about what specific adult-child interactions constitute a partnership. In this study, we unpack what constitutes an "equal partnership" in PD between adults and children. On the basis of prior literature, we created a new framework that examines the complementary roles between children and adults. Next, we analyzed a case study of a year-long intergenerational design team of children (ages 7-11) and adults. From this analysis, we determined that design partnerships are composed of four dimensions that span from unbalanced to balanced interactions: facilitation, relationship building, design-by-doing, and elaborating together. Finally, to demonstrate its utility, we analyzed two focal co-design sessions using our framework. Our analysis suggests that equal partnership in PD is not a single static interaction but a development over time of design interactions influenced by context, experience, and participants.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yip, Jason C. and Sobel, Kiley and Pitt, Caroline and Lee, Kung Jin and Chen, Sijin and Nasu, Kari and Pina, Laura R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, co-design, cooperative inquiry, participatory design, design methods, youth},
	pages = {5742--5754},
}

@inproceedings{spiel_participatory_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Participatory {Evaluation} with {Autistic} {Children}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025851},
	doi = {10.1145/3025453.3025851},
	abstract = {Participatory Design (PD) has become a standard methodology in HCI, however, the evaluation of the outcomes of participatory processes is often exclusively driven by researcher defined measures of success. Through our work with autistic children, who have radically different life worlds from our own, it became evident that their criteria for the success of a project are most likely also very different. In order to address the limitations of researcher defined and led evaluations in this context, we developed an approach for participatory evaluation called PEACE (Participatory Evaluation with Autistic ChildrEn). Using this approach, we were able to include autistic children in dedicated evaluation phases through the co-definition of goals and methods, joint processes of data gathering and the co-interpretation of results. We discuss three case studies in which we successfully applied our approach and conclude with a reflection on the novel insights created through participatory evaluation and researchers' roles in such a process.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Spiel, Katta and Malinverni, Laura and Good, Judith and Frauenberger, Christopher},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {children, autism, participatory evaluation},
	pages = {5755--5766},
}

@inproceedings{hiniker_co-designing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Co-{Designing} with {Preschoolers} {Using} {Fictional} {Inquiry} and {Comicboarding}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025588},
	doi = {10.1145/3025453.3025588},
	abstract = {In this case study, we describe a design workshop with 7 children age 4-6 using existing co-design techniques known to elicit design insights in older individuals. We found that our 5- and 6-year-old participants successfully generated design ideas using these methods, while 4-year-olds were unable to use create solutions in a traditional format. How-ever, these younger children enthusiastically offered opportunities where, with methodological guidance, the research-er could have followed the child's lead and shifted the design question to one that was potentially more meaningful for the participant. We propose a future work to examine the effectiveness of giving these younger participants great-er authority in defining and scoping the problem space.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hiniker, Alexis and Sobel, Kiley and Lee, Bongshin},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {participatory design, comicboarding, design workshop, early childhood, fictional inquiry},
	pages = {5767--5772},
}

@inproceedings{zhu_making_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Making {Space} for the {Quality} {Care}: {Opportunities} for {Technology} in {Cognitive} {Behavioral} {Therapy} for {Insomnia}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025549},
	doi = {10.1145/3025453.3025549},
	abstract = {Insomnia can drastically affect individuals' overall well-being and work performance, with substantial costs to society and industry. Cognitive behavioral therapy for insomnia (CBT-I) is a psychotherapeutic treatment, which requires patients to track sleep and share the data with CBT-I clinicians. However, the number of specialists who can provide CBT-I limits the number of patients who can receive it. In this paper, we aim to identify opportunities to leverage technology to assist clinicians in delivering quality and effective CBT-I services to broader populations. Toward this goal, we conducted formative studies, including 11 CBT-I clinic observations and 17 semi-structured interviews, to understand the current workflow of CBT-I and associated challenges. We discuss how technology can assist clinicians and patients throughout the various steps of CBT-I workflow while addressing some of the identified challenges, and more broadly, how technology can make space for clinicians and patients to build quality therapeutic relationships.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Haining and Luo, Yuhan and Choe, Eun Kyoung},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {interview, observation, clinical workflow, cognitive behavioral therapy for insomnia (cbt-i), patient engagement, patient-generated data, self- monitoring data},
	pages = {5773--5786},
}

@inproceedings{kim_prescribing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Prescribing 10,000 {Steps} {Like} {Aspirin}: {Designing} a {Novel} {Interface} for {Data}-{Driven} {Medical} {Consultations}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025570},
	doi = {10.1145/3025453.3025570},
	abstract = {Due to the prevalence of personal health tracking, cases of self-logged data being utilized in the clinic are gradually increasing. However, obstacles to clinicians' ability to further adopt such data-driven medical consultations in the existing workflow remain, such as lack of time and poor interoperability. In this paper, we conducted a workshop to design a clinician interface supporting the integration of data-driven consultation into the existing workflow and investigate the role of the interface in situ. After implementing the clinician interface designed based on the workshop results, we observed 32 cases of actual use within the clinical context. We found that our interface, DataMD, helped the clinician construct a new workflow, enhanced the clinician's counseling skills, and facilitated more in-depth conversation. This paper contributes to empirically identifying the role of a clinician interface through a user-centered design approach.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Yoojung and Heo, Eunyoung and Lee, Hyunjeong and Ji, Sookyoung and Choi, Jueun and Kim, Jeong-Whun and Lee, Joongseek and Yoo, Sooyoung},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design workshop, patient-generated data, clinician interface, data-driven medical consultation, self-logged data},
	pages = {5787--5799},
}

@inproceedings{mentis_crafting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Crafting a {View} of {Self}-{Tracking} {Data} in the {Clinical} {Visit}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025589},
	doi = {10.1145/3025453.3025589},
	abstract = {When self-tracking encounters clinical practices, the data is reshaped by goals and expertise that exist within a healthcare framework. To uncover these shaping practices, we provided a Fitbit Zip step-count sensor to nine patients with Parkinson's disease. Each patient wore the sensor for four weeks and then returned for a clinical visit with their neurologist. Our analysis focuses on this first clinical visit after four weeks of data had been collected. Our use of conversation analysis of both talk and action makes visible the practices engaged in by both collaborative members to 'craft a view' of the data toward shared decision making. Our findings reveal the deliberate guiding of attention to specific interpretations of the data through both talk and actions and we explain how our systematic analysis has uncovered tools for the mutually beneficial crafting practices of the clinician and patient.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mentis, Helena M. and Komlodi, Anita and Schrader, Katrina and Phipps, Michael and Gruber-Baldini, Ann and Yarbrough, Karen and Shulman, Lisa},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {self-tracking, quantified self, perception, activity tracker},
	pages = {5800--5812},
}

@inproceedings{dow_what_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {What {Happens} to {Digital} {Feedback}? {Studying} the {Use} of a {Feedback} {Capture} {Platform} by {Care} {Organisations}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025943},
	doi = {10.1145/3025453.3025943},
	abstract = {In this paper we report on a four-month long field trial of ThoughtCloud, a feedback collection platform that allows people to leave ratings and audio or video responses to simple prompts. ThoughtCloud was trialled with four organisations providing care services for people with disabilities. We conducted interviews with staff and volunteers that used ThoughtCloud before, during and after its deployment, and workshops with service users and staff. While the collection of feedback was high, only one organisation regularly reviewed and responded to collected opinions. Furthermore, tensions arose around data access and sharing, and the mismatch of values between "giving voice" and the capacity for staff to engage in feedback practices. We contribute insights into the challenges faced in using novel technologies in resource constrained organisations, and discuss opportunities for designs that give greater agency to service users to engage those that care for them in reflecting and responding to their opinions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dow, Andy and Vines, John and Lowe, Toby and Comber, Rob and Wilson, Rob},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {health, social care, democracy, feedback},
	pages = {5813--5825},
}

@inproceedings{guo_facade_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Facade: {Auto}-{Generating} {Tactile} {Interfaces} to {Appliances}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025845},
	doi = {10.1145/3025453.3025845},
	abstract = {Common appliances have shifted toward flat interface panels, making them inaccessible to blind people. Although blind people can label appliances with Braille stickers, doing so generally requires sighted assistance to identify the original functions and apply the labels. We introduce Facade - a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel. Facade users capture a photo of the appliance with a readily available fiducial marker (a dollar bill) for recovering size information. This image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface. Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls. Finally, a home 3D printer or commercial service fabricates the layer, which is then aligned and attached to the interface by the blind person. We demonstrate the viability of Facade in a study with 11 blind participants.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Anhong and Kim, Jeeeun and Chen, Xiang 'Anthony' and Yeh, Tom and Hudson, Scott E. and Mankoff, Jennifer and Bigham, Jeffrey P.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, crowdsourcing, 3d printing, fabrication, blind, visually impaired, computer vision, non-visual interfaces},
	pages = {5826--5838},
}

@inproceedings{kacorri_people_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {People with {Visual} {Impairment} {Training} {Personal} {Object} {Recognizers}: {Feasibility} and {Challenges}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025899},
	doi = {10.1145/3025453.3025899},
	abstract = {Blind people often need to identify objects around them, from packages of food to items of clothing. Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter, scale, viewpoints, occlusion, and image quality than in photos taken by blind users. We explore personal object recognizers, where visually impaired people train a mobile application with a few snapshots of objects of interest and provide custom labels. We adopt transfer learning with a deep learning system for user-defined multi-label k-instance classification. Experiments with blind participants demonstrate the feasibility of our approach, which reaches accuracies over 90\% for some participants. We analyze user data and feedback to explore effects of sample size, photo-quality variance, and object shape; and contrast models trained on photos by blind participants to those by sighted participants and generic recognizers.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kacorri, Hernisa and Kitani, Kris M. and Bigham, Jeffrey P. and Asakawa, Chieko},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, object recognition, blind, computer vision, photographs},
	pages = {5839--5849},
}

@inproceedings{taranta_ii_jackknife_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Jackknife: {A} {Reliable} {Recognizer} with {Few} {Samples} and {Many} {Modalities}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026002},
	doi = {10.1145/3025453.3026002},
	abstract = {Despite decades of research, there is yet no general rapid prototyping recognizer for dynamic gestures that can be trained with few samples, work with continuous data, and achieve high accuracy that is also modality-agnostic. To begin to solve this problem, we describe a small suite of accessible techniques that we collectively refer to as the Jackknife gesture recognizer. Our dynamic time warping based approach for both segmented and continuous data is designed to be a robust, go-to method for gesture recognition across a variety of modalities using only limited training samples. We evaluate pen and touch, Wii Remote, Kinect, Leap Motion, and sound-sensed gesture datasets as well as conduct tests with continuous data. Across all scenarios we show that our approach is able to achieve high accuracy, suggesting that Jackknife is a capable recognizer and good first choice for many endeavors.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Taranta II, Eugene M. and Samiei, Amirreza and Maghoumi, Mehran and Khaloo, Pooya and Pittman, Corey R. and LaViola Jr., Joseph J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gesture recognition, rapid prototyping, dynamic time warping, gesture customization, user evaluation},
	pages = {5850--5861},
}

@inproceedings{billah_ubiquitous_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Ubiquitous {Accessibility} for {People} with {Visual} {Impairments}: {Are} {We} {There} {Yet}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025731},
	doi = {10.1145/3025453.3025731},
	abstract = {Ubiquitous access is an increasingly common vision of computing, wherein users can interact with any computing device or service from anywhere, at any time. In the era of personal computing, users with visual impairments required special-purpose, assistive technologies, such as screen readers, to interact with computers. This paper investigates whether technologies like screen readers have kept pace with, or have created a barrier to, the trend toward ubiquitous access, with a specific focus on desktop computing as this is still the primary way computers are used in education and employment. Towards that, the paper presents a user study with 21 visually-impaired participants, specifically involving the switching of screen readers within and across different computing platforms, and the use of screen readers in remote access scenarios. Among the findings, the study shows that, even for remote desktop access - an early forerunner of true ubiquitous access - screen readers are too limited, if not unusable. The study also identifies several accessibility needs, such as uniformity of navigational experience across devices, and recommends potential solutions. In summary, assistive technologies have not made the jump into the era of ubiquitous access, and multiple, inconsistent screen readers create new practical problems for users with visual impairments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Billah, Syed Masum and Ashok, Vikas and Porter, Donald E. and Ramakrishnan, I.V.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {mobile computing, multiple screen readers, remote access, ubiquitous accessibility, visually impaired users},
	pages = {5862--5868},
}

@inproceedings{liu_bignav_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{BIGnav}: {Bayesian} {Information} {Gain} for {Guiding} {Multiscale} {Navigation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025524},
	doi = {10.1145/3025453.3025524},
	abstract = {This paper introduces BIGnav, a new multiscale navigation technique based on Bayesian Experimental Design where the criterion is to maximize the information-theoretic concept of mutual information, also known as information gain. Rather than simply executing user navigation commands, BIGnav interprets user input to update its knowledge about the user's intended target. Then it navigates to a new view that maximizes the information gain provided by the user's expected subsequent input. We conducted a controlled experiment demonstrating that BIGnav is significantly faster than conventional pan and zoom and requires fewer commands for distant targets, especially in non-uniform information spaces. We also applied BIGnav to a realistic application and showed that users can navigate to highly probable points of interest on a map with only a few steps. We then discuss the tradeoffs of BIGnav–including efficiency vs. increased cognitive load–and its application to other interaction tasks.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Wanyu and D'Oliveira, Rafael Lucas and Beaudouin-Lafon, Michel and Rioul, Olivier},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {bayesian experimental design, guided navigation, multiscale navigation, mutual information},
	pages = {5869--5880},
}

@inproceedings{grandi_design_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Design and {Evaluation} of a {Handheld}-{Based} {3D} {User} {Interface} for {Collaborative} {Object} {Manipulation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025935},
	doi = {10.1145/3025453.3025935},
	abstract = {Object manipulation in 3D virtual environments demands a combined coordination of rotations, translations and scales, as well as the camera control to change the user's viewpoint. Then, for many manipulation tasks, it would be advantageous to share the interaction complexity among team members. In this paper we propose a novel 3D manipulation interface based on a collaborative action coordination approach. Our technique explores a smartphone – the touchscreen and inertial sensors – as input interface, enabling several users to collaboratively manipulate the same virtual object with their own devices. We first assessed our interface design on a docking and an obstacle crossing tasks with teams of two users. Then, we conducted a study with 60 users to understand the influence of group size in collaborative 3D manipulation. We evaluated teams in combinations of one, two, three and four participants. Experimental results show that teamwork increases accuracy when compared with a single user. The accuracy increase is correlated with the number of individuals in the team and their work division strategy.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Grandi, Jerônimo Gustavo and Debarba, Henrique Galvan and Nedel, Luciana and Maciel, Anderson},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user studies, 3D user interfaces, collaborative manipulation},
	pages = {5881--5891},
}

@inproceedings{saidi_tdome_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{TDome}: {A} {Touch}-{Enabled} {6DOF} {Interactive} {Device} for {Multi}-{Display} {Environments}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025661},
	doi = {10.1145/3025453.3025661},
	abstract = {The rapid evolution of multi-display environments (MDEs) has created a vacuum in need of novel input devices to optimize interaction in MDEs. In this paper, we propose TDome, a novel touch-enabled 6DOF input and output device to facilitate interactions in MDEs. TDome offers a private display as output, and multiple degrees of freedom as input by combining touch gestures on the display with physical rotation, roll and translation manipulations of the device. TDome allows versatile interactions that address major MDE tasks, which we illustrate through various proof-of-concept implementations: detect surrounding displays, select one display, transfer data across displays, reach distant displays and perform private interactions. We explore TDome's usability and suitability for MDEs through three user studies. First we explore combined physical+touch gestures from which we discard uncomfortable combinations. We experimentally validate their feasibility and come up with a set of 71 combined gestures that are comfortable and ensure a high success rate, i.e. that can be easily performed and efficiently detected. Finally, we collect user feedback to identify natural mappings between gestures and MDE interactions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Saidi, Houssem and Serrano, Marcos and Irani, Pourang and Dubois, Emmanuel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {touch input, input device, multi-display environments, rolling device},
	pages = {5892--5904},
}

@inproceedings{gutwin_field_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {A {Field} {Experiment} of {Spatially}-{Stable} {Overviews} for {Document} {Navigation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025905},
	doi = {10.1145/3025453.3025905},
	abstract = {Finding (and re-finding) locations in text documents is a common activity for most computer users – but tools for document navigation are still limited in many ways. Previous research has shown that a spatially-stable overview of the entire document can be substantially faster than any other navigation technique – particularly when revisiting previous locations. However, the overview technique has only been tested in a limited laboratory study, so little is known about whether it works in more realistic contexts. To answer this question, we developed a PDF viewer that incorporates several document-navigation techniques, and carried out two studies. First, we ran a field experiment in which users carried out search tasks using an overview and other techniques – on their own computers in a non-laboratory environment. Second, we ran a smaller field study in which people used our viewer (with choice of navigation techniques) for their own PDF tasks. In the field experiment, the overview was significantly and substantially faster than other techniques, and in the field study, the technique was frequently used for a wide variety of documents. Our work provides confirmation of the value of spatially stable overviews as a basis for document navigation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gutwin, Carl and Cockburn, Andy and Gough, Nickolas},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {document navigation, scrolling, space-filling thumbnails},
	pages = {5905--5916},
}

@inproceedings{sambasivan_imagined_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Imagined {Connectivities}: {Synthesized} {Conceptions} of {Public} {Wi}-{Fi} in {Urban} {India}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025545},
	doi = {10.1145/3025453.3025545},
	abstract = {India and other economies in the Global South are undergoing a proliferation in public Wi-Fi, with large-scale deployments from industry and government. In this paper, we report on a qualitative study on public Wi-Fi conceptions as held by urban Indians, textita priori to connecting to a network. Our findings show that prior public Wi-Fi users and non-users alike raised a surprising range and depth of conceptions—ranging from suspicion of operators' intentions to monetize, to concerns about sexual image morphing, to fears of phone wipeouts, to aspiration—which were informed by popular media, BlueTooth cultures, and social learning. We found these conceptions of Wi-Fi networks to significantly influence adoption of public Wi-Fi. With enormous investments in public Wi-Fi initiatives, we call for network providers to address these deep conceptions among emerging users; by suggesting ways to build public awareness, better user experiences, and business model innovation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sambasivan, Nithya and Aoki, Paul M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ictd, hci4d, india, internet, access, imaginaries, media studies, public wi-fi},
	pages = {5917--5928},
}

@inproceedings{pal_agency_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Agency in {Assistive} {Technology} {Adoption}: {Visual} {Impairment} and {Smartphone} {Use} in {Bangalore}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025895},
	doi = {10.1145/3025453.3025895},
	abstract = {Studies on technology adoption typically assume that a user's perception of usability and usefulness of technology are central to its adoption. Specifically, in the case of accessibility and assistive technology, research has traditionally focused on the artifact rather than the individual, arguing that individual technologies fail or succeed based on their usability and fit for their users. Using a mixed-methods field study of smartphone adoption by 81 people with visual impairments in Bangalore, India, we argue that these positions are dated in the case of accessibility where a non-homogeneous population must adapt to technologies built for sighted people. We found that many users switch to smartphones despite their awareness of significant usability challenges with smartphones. We propose a nuanced understanding of perceived usefulness and actual usage based on need-related social and economic functions, which is an important step toward rethinking technology adoption for people with disabilities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pal, Joyojeet and Viswanathan, Anandhi and Chandra, Priyank and Nazareth, Anisha and Kameswaran, Vaishnav and Subramonyam, Hariharan and Johri, Aditya and Ackerman, Mark S. and O'Modhrain, Sile},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, mobile phones, India, android, bangalore, iOS},
	pages = {5929--5940},
}

@inproceedings{rifat_money_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Money, {God}, and {SMS}: {Explorations} in {Supporting} {Social} {Action} {Through} a {Bangladeshi} {Mosque}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025960},
	doi = {10.1145/3025453.3025960},
	abstract = {Religious institutions hold a significant place in daily life for the vast majority of people in the world, especially in developing countries. Yet despite their social prominence, and despite HCI's emphasis on the social context of technology, organized religion is neglected in both the HCI and ICTD literature. This paper explores the relationship that mosques in Bangladesh have with their constituencies and with technology, with an eye toward the integration of technology with existing religious institutions as a way to achieve positive social ends. We first describe a qualitative exploration of several mosque communities in Bangladesh, where we find that skepticism and pragmatism about modern technology interact in a complex way that nevertheless leaves room for technical interventions. We then describe a randomized controlled trial to study the relative value of SMS messages infused with overtly religious or secularly altruistic frames for the purpose of mosque fundraising. We find that SMS messages increase donations overall, but that their framing is significant. Messages with secular altruistic framing increased donations by 9.5\%, while those with religious sentiment increased donations by 57.3\%. Our findings demonstrate how technologies like SMS amplify underlying religious forces and suggest the possibility of working with religious institutions in applying positive ICT interventions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rifat, Md. Rashidujjaman and Chen, Jay and Toyama, Kentaro},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ict4d, techno-spirituality, donation, religion, sms},
	pages = {5941--5953},
}

@inproceedings{kandathil_negotiating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Negotiating {Absent} {Practices} and {Dormant} {Features}: {Discourse} as a {Means} of {Shaping} the {Implementation} of a {Global} {Enterprise} {System} to {Meet} {Local} {Work} {Culture}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026039},
	doi = {10.1145/3025453.3026039},
	abstract = {The introduction of a new enterprise system to an organization often necessitates the accommodation of standardized practices, which may be in conflict with local users' practices and their work culture. We explore such a conflict in an India-based multinational organization using an eight-month interpretive case study. Based on grounded analysis, we present a narrative account of how consultants, on contract for managing the deployment and making necessary adjustments, used discourse as a means of shaping user understanding about the features and practices embedded in the underlying system, which were not initially realized through the interface. Sustained user resistance to this shaping led to a negotiated compromise and adaptation of the system to incorporate local work culture. Our findings allow us to explore the under-theorized role of discursive power within an implementer-user-technology trio, and illustrate the feedback utility of user resistance in developing culturally-inclusive designs.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kandathil, George and Wagner, Erica L.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {culture, hci, hci4d, appropriation, workplace studies, cscw, critical design, affordance, cross-cultural design, discursive, enterprise system},
	pages = {5954--5965},
}

@inproceedings{lu_hybrid_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Hybrid {HFR} {Depth}: {Fusing} {Commodity} {Depth} and {Color} {Cameras} to {Achieve} {High} {Frame} {Rate}, {Low} {Latency} {Depth} {Camera} {Interactions}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025478},
	doi = {10.1145/3025453.3025478},
	abstract = {The low frame rate and high latency of consumer depth cameras limits their use in interactive applications. We propose combining the Kinect depth camera with an ordinary color camera to synthesize a high frame rate and low latency depth image. We exploit common CMOS camera region of interest (ROI) functionality to obtain a high frame rate image over a small ROI. Motion in the ROI is computed by a fast optical flow implementation. The resulting flow field is used to extrapolate Kinect depth images to achieve high frame rate and low latency depth, and optionally predict depth to further reduce latency. Our "Hybrid HFR Depth" prototype generates useful depth images at maximum 500Hz with minimum 20ms latency. We demonstrate Hybrid HFR Depth in tracking fast moving objects, handwriting in the air, and projecting onto moving hands. Based on commonly available cameras and image processing implementations, Hybrid HFR Depth may be useful to HCI practitioners seeking to create fast, fluid depth camera-based interactions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Jiajun and Benko, Hrvoje and Wilson, Andrew D.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {kinect, latency, configurable, depth camera, frame rate},
	pages = {5966--5975},
}

@inproceedings{chen_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding the {Aesthetic} {Evolution} of {Websites}: {Towards} a {Notion} of {Design} {Periods}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025607},
	doi = {10.1145/3025453.3025607},
	abstract = {In art and music, time periods like "classical" and "impressionist" are powerful means for academics and practitioners to compare and contrast artifacts that share aesthetics or philosophies. While web designs have undergone changes for 25 years, we lack theories to describe or explain these changes. In this paper, we take a first step towards identifying and understanding the design periods of websites. Drawing from humanistic HCI methods, we asked subject experts of web design to critically analyze a dataset of prominent websites whose lifetimes span over a decade. These informed judgments reveal a set of key markers that signal shifts in design periods. For instance, advances in display technologies and changes in company strategies help explain how design periods demarcated by particular layout templates and navigation models arise. We suggest that designers and marketers can draw inspiration from website designs curated into design periods. Future work should examine the utility of applying design periods to any computationally embedded artifact that is an interaction design.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Wen and Crandall, David J. and Su, Norman Makoto},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {art, design periods, interaction design criticism, websites},
	pages = {5976--5987},
}

@inproceedings{macleod_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding {Blind} {People}'s {Experiences} with {Computer}-{Generated} {Captions} of {Social} {Media} {Images}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025814},
	doi = {10.1145/3025453.3025814},
	abstract = {Research advancements allow computational systems to automatically caption social media images. Often, these captions are evaluated with sighted humans using the image as a reference. Here, we explore how blind and visually impaired people experience these captions in two studies about social media images. Using a contextual inquiry approach (n=6 blind/visually impaired), we found that blind people place a lot of trust in automatically generated captions, filling in details to resolve differences between an image's context and an incongruent caption. We built on this in-person study with a second, larger online experiment (n=100 blind/visually impaired) to investigate the role of phrasing in encouraging trust or skepticism in captions. We found that captions emphasizing the probability of error, rather than correctness, encouraged people to attribute incongruence to an incorrect caption, rather than missing details. Where existing research has focused on encouraging trust in intelligent systems, we conclude by challenging this assumption and consider the benefits of encouraging appropriate skepticism.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {MacLeod, Haley and Bennett, Cynthia L. and Morris, Meredith Ringel and Cutrell, Edward},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, social media, twitter, blindness, alt text, automatic image captioning},
	pages = {5988--5999},
}

@inproceedings{peng_time_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Time {Travel} with {One} {Click}: {Effects} of {Digital} {Filters} on {Perceptions} of {Photographs}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025810},
	doi = {10.1145/3025453.3025810},
	abstract = {Today's digital photographs are being heavily "filtered." By simple clicks on mobile apps like Hipstamatic and Instagram, users can easily apply digital filters to their pictures to create effects such as faux-vintage and light leaks. To understand the potential impacts of photo filters, we conducted an online experiment and investigated how the use of the black-and-white and film-style photo filters changed viewers' perceptions and descriptions of photographs. We found that photo filters substantially increased viewers' perceived temporal distances to photographs. Participants also tended to describe analogue-style photos more interpretively and tentatively than unfiltered ones, indicating an increase in construal levels. We suggest that the widely used photo filter is not just a tool to change aesthetics; it also adds a layer of history, meaning, and defamiliarization to photographs, allowing users to construct a mental distance in images that deviates from everyday experiences. We offer insights into the psychology of visual styles and implications for designing filter apps and photo-sharing platforms.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Peng, Yilang},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {computational text analysis, construal level theory, digital filter, mobile app, visual style},
	pages = {6000--6011},
}

@inproceedings{jamieson_forgetmenot_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ForgetMeNot}: {Active} {Reminder} {Entry} {Support} for {Adults} with {Acquired} {Brain} {Injury}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025888},
	doi = {10.1145/3025453.3025888},
	abstract = {Smartphone reminding apps can compensate for memory impairment after acquired brain injury (ABI). In the absence of a caregiver, users must enter reminders themselves if the apps are going to help them. Poor memory and apathy associated with ABI can result in failure to initiate such configuration behaviour and the benefits of reminder apps are lost. ForgetMeNot takes a novel approach to address this problem by periodically encouraging the user to enter reminders with unsolicited prompts (UPs). An in situ case study investigated the experience of using a reminding app for people with ABI and tested UPs as a potential solution to initiating reminder entry. Three people with severe ABI living in a post-acute rehabilitation hospital used the app in their everyday lives for four weeks to collect real usage data. Field observations illustrated how difficulties with motivation, insight into memory difficulties and anxiety impact reminder app use in a rehabilitation setting. Results showed that when 6 UPs were presented throughout the day, reminder-setting increased, showing UPs are an important addition to reminder applications for people with ABI. This study demonstrates that barriers to technology use can be resolved in practice when software is developed with an understanding of the issues experienced by the user group.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jamieson, Matthew and O'Neill, Brian and Cullen, Breda and Lennon, Marilyn and Brewster, Stephen and Evans, Jonathan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {assistive technology, field study, acquired brain injury, in situ study, memory rehabilitation, smartphone reminding},
	pages = {6012--6023},
}

@inproceedings{zhang_interaction_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Interaction {Proxies} for {Runtime} {Repair} and {Enhancement} of {Mobile} {Application} {Accessibility}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025846},
	doi = {10.1145/3025453.3025846},
	abstract = {We introduce interaction proxies as a strategy for runtime repair and enhancement of the accessibility of mobile applications. Conceptually, interaction proxies are inserted between an application's original interface and the manifest interface that a person uses to perceive and manipulate the application. This strategy allows third-party developers and researchers to modify an interaction without an application's source code, without rooting the phone, without otherwise modifying an application, while retaining all capabilities of the system (e.g., Android's full implementation of the TalkBack screen reader). This paper introduces interaction proxies, defines a design space of interaction re-mappings, identifies necessary implementation abstractions, presents details of implementing those abstractions in Android, and demonstrates a set of Android implementations of interaction proxies from throughout our design space. We then present a set of interviews with blind and low-vision people interacting with our prototype interaction proxies, using these interviews to explore the seamlessness of interaction, the perceived usefulness and potential of interaction proxies, and visions of how such enhancements could gain broad usage. By allowing third-party developers and researchers to improve an interaction, interaction proxies offer a new approach to personalizing mobile application accessibility and a new approach to catalyzing development, deployment, and evaluation of mobile accessibility enhancements.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xiaoyi and Ross, Anne Spencer and Caspi, Anat and Fogarty, James and Wobbrock, Jacob O.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, interaction proxies, runtime modification},
	pages = {6024--6037},
}

@inproceedings{li_sugilite_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{SUGILITE}: {Creating} {Multimodal} {Smartphone} {Automation} by {Demonstration}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025483},
	doi = {10.1145/3025453.3025483},
	abstract = {SUGILITE is a new programming-by-demonstration (PBD) system that enables users to create automation on smartphones. SUGILITE uses Android's accessibility API to support automating arbitrary tasks in any Android app (or even across multiple apps). When the user gives verbal commands that SUGILITE does not know how to execute, the user can demonstrate by directly manipulating the regular apps' user interface. By leveraging the verbal instructions, the demonstrated procedures, and the apps? UI hierarchy structures, SUGILITE can automatically generalize the script from the recorded actions, so SUGILITE learns how to perform tasks with different variations and parameters from a single demonstration. Extensive error handling and context checking support forking the script when new situations are encountered, and provide robustness if the apps change their user interface. Our lab study suggests that users with little or no programming knowledge can successfully automate smartphone tasks using SUGILITE.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Toby Jia-Jun and Azaria, Amos and Myers, Brad A.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {end-user development, programming by demonstration, smartphone automation},
	pages = {6038--6049},
}

@inproceedings{ahmed_automated_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Automated {Detection} of {Facial} {Expressions} during {Computer}-{Assisted} {Instruction} in {Individuals} on the {Autism} {Spectrum}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025472},
	doi = {10.1145/3025453.3025472},
	abstract = {It has been suggested that computer-assisted instruction (CAI) is a promising method for educating students on the autism spectrum. We aimed to determine whether automated recognition of facial expressions aided in predicting CAI engagement and learning performance. Seven youth with autism (mean age = 12.7, SD = 4.2) interacted with a CAI program, TeachTown Basics, for 15 consecutive sessions. Video recordings of the participants' faces were collected during these sessions and facial expressions from these videos were analyzed using CERT, an algorithm that automatically outputs intensity values for each facial action unit (AU). Using these data, we attempted to operationally define two engagement indices: (1) behavioral engagement, the proportion of time a participant had their face oriented to the computer screen; and (2) emotional engagement, the activation of AUs previously associated with CAI. Our results suggest that both indices strongly correlated with one another, but that emotional (not behavioral) engagement predicted test performance. CAI knowledge domain, participant sex, and developmental age also contributed to the prediction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ahmed, Alex A. and Goodwin, Matthew S.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {assistive technologies, individuals with disabilities \&amp, education/learning, computer vision, emotion/affective computing, quantitative methods},
	pages = {6050--6055},
}

@inproceedings{findlater_comparing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Comparing {Touchscreen} and {Mouse} {Input} {Performance} by {People} {With} and {Without} {Upper} {Body} {Motor} {Impairments}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025603},
	doi = {10.1145/3025453.3025603},
	abstract = {Controlled studies of touchscreen input performance for users with upper body motor impairments remain relatively sparse. To address this gap, we present a controlled lab study of mouse vs. touchscreen performance with 32 participants (16 with upper body motor impairments and 16 without). Our study examines: (1) how touch input compares to an indirect pointing device (a mouse); (2) how performance compares across a range of standard interaction techniques; and (3) how these answers differ for users with and without motor impairments. While the touchscreen was faster than the mouse overall, only participants without motor impairments benefited from a lower error rate on the touchscreen. Indeed, participants with motor impairments had a three-fold increase in pointing (tapping) errors on the touchscreen compared to the mouse. Our findings also highlight the high frequency of spurious touches for users with motor impairments and update past accessibility recommendations for minimum touchscreen target sizes to at least 18mm.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Findlater, Leah and Moffatt, Karyn and Froehlich, Jon E. and Malu, Meethu and Zhang, Joan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, mouse, touchscreen, motor impairments, human performance, input devices},
	pages = {6056--6061},
}

@inproceedings{muntean_designing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Designing {Cultural} {Values} into {Interaction}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025908},
	doi = {10.1145/3025453.3025908},
	abstract = {In this paper, we highlight possibilities for designing intangible cultural values into interactions with technologies in heritage spaces. We do this specifically through the design of elwkw – Belongings, an interactive tangible table installed in a cultural heritage museum. The tabletop was collaboratively designed to communicate complex and narrative information and values about Musqueam culture. Rather than focusing only on content and interface design, we wanted visitors to also experience Musqueam values through their interactions with the system. We describe our value-sensitive design process, present five interdependent design goals, discuss the design strategies that enabled us to meet these goals, and evaluate our approach through a user study. From our design process and evaluation we offer recommendations for designing values into interactions more generally and for tangible interactions specifically in ways that support visitors' experience and understanding of specific cultural values through technology.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Muntean, Reese and Antle, Alissa N. and Matkin, Brendan and Hennessy, Kate and Rowley, Susan and Wilson, Jordan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {museums, intangible cultural heritage, digital tabletops, indigenous heritage, tangibles, value sensitive design},
	pages = {6062--6074},
}

@inproceedings{webber_kinecting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Kinecting with {Orangutans}: {Zoo} {Visitors}' {Empathetic} {Responses} to {Animals}' {Use} of {Interactive} {Technology}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025729},
	doi = {10.1145/3025453.3025729},
	abstract = {Animal conservation organisations occasionally harness depictions of animals using digital technology to inspire interest in, and concern for animals. To better understand the forms of empathy experienced by people observing animal-computer interaction, we designed and studied an interactive installation for orangutans at a zoo. Through collaborative design we established an understanding of zoos' objectives and strategies related to empathy in the zoo context. We deployed a prototype installation, and observed and interviewed visitors who watched orangutans use the installation. Analysis of observations and interviews revealed that visitors responded with cognitive, affective and motor empathy for the animals. We propose that these empathetic responses are prompted by the visibility of orangutans' bodily movements, by the "anthropic frame" provided by digital technology, and by prompting reflection on animals' cognitive processes and affective states. This paper contributes new evidence and understanding of people's empathetic responses to observing animal-computer interaction and confirms the value of designing for empathy in its various forms},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Webber, Sarah and Carter, Marcus and Sherwen, Sally and Smith, Wally and Joukhadar, Zaher and Vetere, Frank},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {empathy, animal-computer interaction, conservation, primates, zoos},
	pages = {6075--6088},
}

@inproceedings{su_reconsidering_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Reconsidering {Nature}: {The} {Dialectics} of {Fair} {Chase} in the {Practices} of {American} {Midwest} {Hunters}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025966},
	doi = {10.1145/3025453.3025966},
	abstract = {In this paper, we describe an ethnographic study consisting of 14 interviews with hunters and participant observations in the American Midwest. We find that the ethos of "fair chase" serves to unite an eclectic group of hunters under a single moral compass. Fair chase posits, for example, that hunters must not have an improper advantage over animals. The actual practices of hunters in different communities (e.g., communities revolving around different weapons or professions), however, reveals a series of opposing points of view among hunters at large on what actually constitutes fair chase. We suggest that an understanding of fair chase and its dialectics can constructively problematize nature for human-computer interaction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Su, Norman Makoto and Cheon, EunJeong},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ethics, dialectics, fair chase, hunters, hunting, nature, rural, values},
	pages = {6089--6100},
}

@inproceedings{mcgookin_exploring_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Exploring {Seasonality} in {Mobile} {Cultural} {Heritage}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025803},
	doi = {10.1145/3025453.3025803},
	abstract = {We present results of an investigation into the role of seasonality in mobile cultural heritage applications. 45 participants in 26 groups used one of two applications when visiting the Finnish recreational island of Seurasaari. Each provided summer and winter content, but varied in how this was presented. We uncovered how users consider seasonality in content, seasonal preferences, as well as how different media becomes more or less interesting if shown in or out of season. We present design considerations for future researchers to consider seasonality in cultural heritage applications.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McGookin, David and Tahiroălu, Koray and Vaittinen, Tuomas and Kytö, Mikko and Monastero, Beatrice and Vasquez, Juan Carlos},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {cultural heritage, mixed reality, location-based interaction, open-air museum, outdoor heritage site, seasonality},
	pages = {6101--6105},
}

@inproceedings{yin_where_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Where {No} {One} {Has} {Gone} {Before}: {A} {Meta}-{Dataset} of the {World}'s {Largest} {Fanfiction} {Repository}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025720},
	doi = {10.1145/3025453.3025720},
	abstract = {With its roots dating to popular television shows of the 1960s such as Star Trek, fanfiction has blossomed into an extremely widespread form of creative expression. The transition from printed zines to online fanfiction repositories has facilitated this growth in popularity, with millions of fans writing stories and adding daily to sites such as Archive Of Our Own, Fanfiction.net, FIMfiction.net, and many others. Enthusiasts are sharing their writing, reading stories written by others, and helping each other to grow as writers. Yet, this domain is often undervalued by society and understudied by researchers. To facilitate the study of this large but often marginalized community, we present a fully anonymized data release (via differential privacy) of the metadata from a large fanfiction site (to protect author privacy, story, profile, and review text is excluded, and only metadata is provided). We use visual analytics techniques to draw several intriguing insights from the data and show the potential for future research. We hope other researchers can use this data to explore further questions related to online fanfiction communities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yin, Kodlee and Aragon, Cecilia and Evans, Sarah and Davis, Katie},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {online communities, youth, fanfiction},
	pages = {6106--6110},
}

@inproceedings{torres_illumination_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Illumination {Aesthetics}: {Light} as a {Creative} {Material} within {Computational} {Design}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025466},
	doi = {10.1145/3025453.3025466},
	abstract = {Recent digital fabrication tools have enabled new form-giving using a wide range of physical materials. However, light as a first class creative material has been largely ignored within the design of our electronic objects. Our work expands the illumination design space by treating light as a physical material. We introduce a digital design tool that simulates and visualizes physical light interactions with a variety of materials for creating custom luminaires. We further develop a computational design and fabrication process for creating custom secondary optics elements (SOEs), which provides additional handles for users to physically shape and redirect light to compose, fill, and evenly diffuse planar and volumetric geometries. Through a workshop study with novice electronic designers, we show how incorporating physical techniques to shape light alters how users view the role and function of LEDs and electronics. We produce example pieces that showcase how our approach expands the electronics aesthetic and discuss how viewing light as material can engender novel, expressive artifacts.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Torres, Cesar and O'Leary, Jasper and Nicholas, Molly and Paulos, Eric},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {digital fabrication, displays, lamp, lighting, luminaire, new media},
	pages = {6111--6122},
}

@inproceedings{wang_transformative_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Transformative {Appetite}: {Shape}-{Changing} {Food} {Transforms} from {2D} to {3D} by {Water} {Interaction} through {Cooking}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026019},
	doi = {10.1145/3025453.3026019},
	abstract = {We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the 'flat packaging' concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided - 2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wen and Yao, Lining and Zhang, Teng and Cheng, Chin-Yi and Levine, Daniel and Ishii, Hiroshi},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {2d-to-3d, anisotropic swelling, autonomous shape-changing, dish design, interactive edibles, transformable food, water interaction},
	pages = {6123--6132},
}

@inproceedings{hassib_emotion_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Emotion {Actuator}: {Embodied} {Emotional} {Feedback} through {Electroencephalography} and {Electrical} {Muscle} {Stimulation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025953},
	doi = {10.1145/3025453.3025953},
	abstract = {The human body reveals emotional and bodily states through measurable signals, such as body language and electroencephalography. However, such manifestations are difficult to communicate to others remotely. We propose EmotionActuator, a proof-of-concept system to investigate the transmission of emotional states in which the recipient performs emotional gestures to understand and interpret the state of the sender.We call this kind of communication embodied emotional feedback, and present a prototype implementation. To realize our concept we chose four emotional states: amused, sad, angry, and neutral. We designed EmotionActuator through a series of studies to assess emotional classification via EEG, and create an EMS gesture set by comparing composed gestures from the literature to sign-language gestures. Through a final study with the end-to-end prototype interviews revealed that participants like implicit sharing of emotions and find the embodied output to be immersive, but want to have control over shared emotions and with whom. This work contributes a proof of concept system and set of design recommendations for designing embodied emotional feedback systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hassib, Mariam and Pfeiffer, Max and Schneegass, Stefan and Rohs, Michael and Alt, Florian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {affective computing, emotion, affect display, eeg., emotion sharing, ems},
	pages = {6133--6146},
}

@inproceedings{boyd_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding the {Role} {Fluidity} of {Stakeholders} {During} {Assistive} {Technology} {Research} "{In} the {Wild}"},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025493},
	doi = {10.1145/3025453.3025493},
	abstract = {Deploying novel technologies requires the coordinated efforts of the research team, research participants, and a variety of community members and project stakeholders. To ensure that the project is completed successfully, these disparate groups of people engage in articulation work, which is the meta-work that supports the use of collaborative systems. In this paper, we examine the articulation work surrounding the deployment of systems that have found limited long-term adoption: assistive technology. Specifically, we examine three research deployments of a collaborative game for children with autism. Analysis of the articulation work performed during these studies demonstrates how research deployments of technologies create conditions in which stakeholders must take on additional roles to make the deployment work. By understanding the articulation work surrounding deployment studies engendered in this role fluidity, we can improve both research design and the analysis of data emergent from these studies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Boyd, LouAnne E. and Rector, Kyle and Profita, Halley and Stangl, Abigale J. and Zolyomi, Annuska and Kane, Shaun K. and Hayes, Gillian R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {collaboration, assistive technology, autism, articulation work, deployment, role fluidity},
	pages = {6147--6158},
}

@inproceedings{molapo_video_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Video {Consumption} {Patterns} for {First} {Time} {Smartphone} {Users}: {Community} {Health} {Workers} in {Lesotho}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025616},
	doi = {10.1145/3025453.3025616},
	abstract = {There is already strong evidence that mobile videos are a good vehicle for public health information dissemination, but there remain open questions around sustainability, appropriate target users, consumption patterns, content, and usage models. We analyse log and interview data of 42 community health workers (who were first time smartphone users) from a longitudinal 17-month deployment to better understand how the utility of mobile videos played out over time in rural Lesotho. During the study period, videos were viewed at an average of 170 times per month, for a total of 2898 views. Through this data we draw these primary findings: a) pausing is not contextually necessary, b) age is not a barrier to usage, c) the primary predictor of popularity of a given video is topical relevance and national campaigns, d) there is no apparent relationship between video length, popularity and completion rates, and e) new videos have only a short-lived novelty effect. Furthermore, we affirm that regular engagement with CHWs has an impact on continued usage, in addition to being important for reducing attrition due to technical issues.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Molapo, Maletsabisa and Densmore, Melissa and DeRenzi, Brian},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {community health workers, consumption patterns, lesotho, mobile health video, understanding users.},
	pages = {6159--6170},
}

@inproceedings{simpson_experiences_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Experiences of {Delivering} a {Public} {Health} {Data} {Service}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025881},
	doi = {10.1145/3025453.3025881},
	abstract = {The turn to in-the-wild within HCI has given rise to an increasing concern around designing technologies which are available at large scale. Uniquely, at the intersection of public health and HCI, our work has supported the deployment of a mobile application, FeedFinder, over the last three years. We delineate the ground-work that was required to sustain this mobile application over the long-term. Focussing in particular on efforts made to engage institutions in taking ownership over FeedFinder and the data it provides, we reflect on the tensions that arose between users and civic institutions, particularly around "what matters". We provide a reflection on key requirements when designing a health data service and provide three lessons learnt which can guide researchers toward their own successful and productive long-term research deployments.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Simpson, Emma and Comber, Rob and Garbett, Andrew and Jenkins, Ed Ian and Balaam, Madeline},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {public health, breastfeeding, citizen-led data, user-centred design},
	pages = {6171--6183},
}

@inproceedings{parry-hill_understanding_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Understanding {Volunteer} {AT} {Fabricators}: {Opportunities} and {Challenges} in {DIY}-{AT} for {Others} in e-{NABLE}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026045},
	doi = {10.1145/3025453.3026045},
	abstract = {We present the results of a study of e-NABLE, a distributed, collaborative volunteer effort to design and fabricate upper-limb assistive technology devices for limb-different users. Informed by interviews with 14 stakeholders in e-NABLE, including volunteers and clinicians, we discuss differences and synergies among each group with respect to motivations, skills, and perceptions of risks inherent in the project. We found that both groups are motivated to be involved in e-NABLE by the ability to use their skills to help others, and that their skill sets are complementary, but that their different perceptions of risk may result in uneven outcomes or missed expectations for end users. We offer four opportunities for design and technology to enhance the stakeholders' abilities to work together.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Parry-Hill, Jeremiah and Shih, Patrick C. and Mankoff, Jennifer and Ashbrook, Daniel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {accessibility, diy, digital fabrication, assistive technology, 3d printing, making, limb difference, prosthetics},
	pages = {6184--6194},
}

@inproceedings{foong_vita_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{VITA}: {Towards} {Supporting} {Volunteer} {Interactions} with {Long}-{Term} {Care} {Residents} with {Dementia}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025776},
	doi = {10.1145/3025453.3025776},
	abstract = {Volunteers are an important resource at long-term care homes because they can supply services, such as engagement activities, that over-burdened care staff struggle to provide. However, volunteers without sufficient training are often challenged in responding to dementia-linked behaviors, which can lead to frustrating difficulties during interaction. Additionally, short-staffed care homes have difficulties in training and maintaining volunteers. To better support volunteers in providing engagement activities for people with dementia without a high training burden, we created VITA, a tablet-based system that supplies carefully designed profiling and guidance using our dementia-appropriate engagement activity kit. Our evaluation indicated that the instructional guide supplied by VITA significantly improves volunteers' ability to facilitate engagement activities with people with dementia, approaching the level of engagement achievable by professional therapists.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Foong, Pin Sym and Zhao, Shengdong and Carlson, Kelsey and Liu, Zhe},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {dementia, tablets, dementia care., engagement activity, nursing homes, volunteers},
	pages = {6195--6207},
}

@inproceedings{mikami_micro-versioning_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Micro-{Versioning} {Tool} to {Support} {Experimentation} in {Exploratory} {Programming}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025597},
	doi = {10.1145/3025453.3025597},
	abstract = {Experimentation plays an essential role in exploratory programming, and programmers apply version control operations when switching the part of the source code back to the past state during experimentation. However, these operations, which we refer to as micro-versioning, are not well supported in current programming environments. We first examined previous studies to clarify the requirements for a micro-versioning tool. We then developed a micro-versioning tool that displays visual cues representing possible micro-versioning operations in a textual code editor. Our tool includes a history model that generates meaningful candidates by combining a regional undo model and tree-structured undo model. The history model uses code executions as a delimiter to segment text edit operations into meaning groups. A user study involving programmers indicated that our tool satisfies the above-mentioned requirements and that it is useful for exploratory programming.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mikami, Hiroaki and Sakamoto, Daisuke and Igarashi, Takeo},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {develpment environment, micro-versioning, version control system},
	pages = {6208--6219},
}

@inproceedings{chen_codeon_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Codeon: {On}-{Demand} {Software} {Development} {Assistance}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025972},
	doi = {10.1145/3025453.3025972},
	abstract = {Software developers rely on support from a variety of resources—including other developers—but the coordination cost of finding another developer with relevant experience, explaining the context of the problem, composing a specific help request, and providing access to relevant code is prohibitively high for all but the largest of tasks. Existing technologies for synchronous communication (e.g. voice chat) have high scheduling costs, and asynchronous communication tools (e.g. forums) require developers to carefully describe their code context to yield useful responses. This paper introduces Codeon, a system that enables more effective task hand-off between end-user developers and remote helpers by allowing asynchronous responses to on-demand requests. With Codeon, developers can request help by speaking their requests aloud within the context of their IDE. Codeon automatically captures the relevant code context and allows remote helpers to respond with high-level descriptions, code annotations, code snippets, and natural language explanations. Developers can then immediately view and integrate these responses into their code. In this paper, we describe Codeon, the studies that guided its design, and our evaluation that its effectiveness as a support tool. In our evaluation, developers using Codeon completed nearly twice as many tasks as those who used state-of-the-art synchronous video and code sharing tools, by reducing the coordination costs of seeking assistance from other developers.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Yan and Lee, Sang Won and Xie, Yin and Yang, YiWei and Lasecki, Walter S. and Oney, Steve},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, development support, intelligent assistants},
	pages = {6220--6231},
}

@inproceedings{ragavan_pfis-v_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{PFIS}-{V}: {Modeling} {Foraging} {Behavior} in the {Presence} of {Variants}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025818},
	doi = {10.1145/3025453.3025818},
	abstract = {Foraging among similar variants of the same artifact is a common activity, but computational models of Information Foraging Theory (IFT) have not been developed to take such variants into account. Without being able to computationally predict people's foraging behavior with variants, our ability to harness the theory in practical ways–such as building and systematically assessing tools for people who forage different variants of an artifact–is limited. Therefore, in this paper, we introduce a new predictive model, PFIS-V, that builds upon PFIS3, the most recent of the PFIS family of modeling IFT in programming situations. Our empirical results show that PFIS-V is up to 25\% more accurate than PFIS3 in predicting where a forager will navigate in a variationed information space.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ragavan, Sruti Srinivasa and Pandya, Bhargav and Piorkowski, David and Hill, Charles and Kuttal, Sandeep Kaur and Sarma, Anita and Burnett, Margaret},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {information foraging theory, variants},
	pages = {6232--6244},
}

@inproceedings{dangelo_improving_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Improving {Communication} {Between} {Pair} {Programmers} {Using} {Shared} {Gaze} {Awareness}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025573},
	doi = {10.1145/3025453.3025573},
	abstract = {Remote collaboration can be more difficult than collocated collaboration for a number of reasons, including the inability to easily determine what your collaborator is looking at. This impedes a pair's ability to efficiently communicate about on-screen locations and makes synchronous coordination difficult. We designed a novel gaze visualization for remote pair programmers which shows where in the code their partner is currently looking, and changes color when they are looking at the same thing. Our design is unobtrusive, and transparently depicts the imprecision inherent in eye tracking technology. We evaluated our design with an experiment in which pair programmers worked remotely on code refactoring tasks. Our results show that with the visualization, pairs spent a greater proportion of their time concurrently looking at the same code locations. Pairs communicated using a larger ratio of implicit to explicit references, and were faster and more successful at responding to those references.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {D'Angelo, Sarah and Begel, Andrew},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {collaboration, eye-tracking, pair programming},
	pages = {6245--6290},
}

@inproceedings{baumer_post-userism_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Post-{Userism}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025740},
	doi = {10.1145/3025453.3025740},
	abstract = {HCI is focused on improving the interactions we have with technology and innovating new types of interactions, as well as expanding the types of people for whom those interactions are designed. Central to these efforts is the simultaneously empowering and contested construct of the "user." This paper examines what the construct of the user highlights, as well as what it conceals. We introduce post-userism, a perspective that simultaneously acknowledges the limits of, and proposes alternatives to, the central construct of the user as proxy for the "human" in HCI. Drawing on developments across the historical trajectory of HCI, we articulate how the user is enacted across four different levels of representation-systems, interface, design process, and the ideology and identify situations where the user breaks down. Synthesizing prior work, we offer a series of strategies for grappling with such situations. In doing so, we seek to overcome the limitations imposed by the user and develop a language that will aid in evolving the foundations of HCI by asking what, exactly, we place at the center of our scholarship and design.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Baumer, Eric P. S. and Brubaker, Jed R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {hci, interaction design, theory, post-user, post-userism, use},
	pages = {6291--6303},
}

@inproceedings{asad_tap_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Tap the "{Make} {This} {Public}" {Button}: {A} {Design}-{Based} {Inquiry} into {Issue} {Advocacy} and {Digital} {Civics}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026034},
	doi = {10.1145/3025453.3026034},
	abstract = {This paper examines the strategies of cycling advocates when deploying digital tools in their advocacy work as they support and create better cycling infrastructure and policies. Over the course of two years, we interviewed and conducted design-based fieldwork in two large U.S. cities with individuals and advocacy organizations, learning about the goals, motivations, and constraints that inform their work in their respective urban homes. Our design-based investigation and fieldwork advance a deeper, situated understanding of the role that computing technology plays when engaging across multiple sites of advocacy work. From this, we add detail to the connections across resources, identities, and issues and continue to advance the emerging area of digital civics, which seeks to design tools that support relational civic interactions across multiple categories of civic actors.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Asad, Mariam and Le Dantec, Christopher A.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {participatory design, digital civics, civic engagement, community computing, cycling., digital advocacy, publics},
	pages = {6304--6316},
}

@inproceedings{green_enabling_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Enabling {Polyvocality} in {Interactive} {Documentaries} through "{Structural} {Participation}"},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025606},
	doi = {10.1145/3025453.3025606},
	abstract = {Recent innovations in online, social and interactive media have led to the emergence of new forms of documentary, such as interactive documentaries ('i-Docs'), with qualities that lend themselves to more open and inclusive production structures. Still, little is known about the experience of making and/or participating-in these kinds of documentary. Our two-year in-the-wild study engaged a large community-of-interest in the production of an i-Doc to explore the ethically-desirable yet challenging aim of enabling multiple subjects to have agency and control over their representation in a documentary. Our study reveals insights into the experiences of participating in an i-Doc and highlights key sociotechnical challenges. We argue that new sociotechnical infrastructure is needed, that frames both "executory" and "structural" forms of participation as symbiotic elements of a co-design process.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Green, David Philip and Bowen, Simon and Hook, Jonathan and Wright, Peter},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {co-design, narrative, interactivity, participation, authorship, documentary, grassroots, i-docs},
	pages = {6317--6329},
}

@inproceedings{jacobs_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting {Expressive} {Procedural} {Art} {Creation} through {Direct} {Manipulation}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025927},
	doi = {10.1145/3025453.3025927},
	abstract = {Computation is a powerful artistic medium. Artists with experience in programming have demonstrated the unique creative opportunities of using code to make art. Currently, manual artists interested in using procedural techniques must undergo the difficult process of learning to program, and must adopt tools and practices far removed from those to which they are accustomed. We hypothesize that, through the right direct manipulation interface, we can enable accessible and expressive procedural art creation. To explore this, we developed Para, a digital illustration tool that supports the creation of declarative constraints in vector artwork. Para's constraints enable procedural relationships while facilitating live manual control and non-linear editing. Constraints can be combined with duplication behaviors and ordered collections of artwork to produce complex, dynamic compositions. We use the results of two open-ended studies with professional artists and designers to provide guidelines for accessible tools that integrate manual and procedural expression.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jacobs, Jennifer and Gogia, Sumit and Mundefinedch, Radomír and Brandt, Joel R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {programming, generative art, procedural art},
	pages = {6330--6341},
}

@inproceedings{ren_crowd_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Crowd {Diversity} and {Performance} in {Wikipedia}: {The} {Mediating} {Effects} of {Task} {Conflict} and {Communication}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025992},
	doi = {10.1145/3025453.3025992},
	abstract = {Crowd diversity is a key attribute that impacts crowd performance in online collaboration systems. As a structural composition of a crowd, diversity is likely to influence crowd performance through communication processes during collaboration. This study examined how diversity influenced crowd performance under different conditions of task conflict and communication in Wikipedia article production. With a sample of 5,899 articles, we found that contribution diversity positively predicted crowd performance, whereas experience diversity was negatively related to performance. In addition, task communication and conflict partially mediated the relationship between crowd diversity and performance. Task communication positively predicted performance for both forms of diversity. Task conflict, on the other hand, was positively predicted by expertise diversity, but had negative associations with contribution diversity and performance. The findings help unpack the reasons for differential effects of diversity on crowd performance, and demonstrate the importance of including communication variables when studying online crowd collaboration.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ren, Ruqin and Yan, Bei},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {diversity, wikipedia, communication, conflict, crowd performance},
	pages = {6342--6351},
}

@inproceedings{hall_freedom_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Freedom versus {Standardization}: {Structured} {Data} {Generation} in a {Peer} {Production} {Community}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025940},
	doi = {10.1145/3025453.3025940},
	abstract = {In addition to encyclopedia articles and software, peer production communities produce structured data, e.g., Wikidata and OpenStreetMap's metadata. Structured data from peer production communities has become increasingly important due to its use by computational applications, such as CartoCSS, MapBox, and Wikipedia infoboxes. However, this structured data is usable by applications only if it follows standards. We did an interview study focused on OpenStreetMap's knowledge production processes to investigate how – and how successfully – this community creates and applies its data standards. Our study revealed a fundamental tension between the need to produce structured data in a standardized way and OpenStreetMap's tradition of contributor freedom. We extracted six themes that manifested this tension and three overarching concepts, correctness, community, and code, which help make sense of and synthesize the themes. We also offered suggestions for improving OpenStreetMap's knowledge production processes, including new data models, sociotechnical tools, and community practices (e.g. stronger leadership).},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hall, Andrew and McRoberts, Sarah and Thebault-Spieker, Jacob and Lin, Yilun and Sen, Shilad and Hecht, Brent and Terveen, Loren},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {openstreetmap, peer-production communities, standardization, structured data},
	pages = {6352--6362},
}

@inproceedings{yang_commitment_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Commitment of {Newcomers} and {Old}-{Timers} to {Online} {Health} {Support} {Communities}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026008},
	doi = {10.1145/3025453.3026008},
	abstract = {For online communities to be successful, they must retain an adequate number of members who contribute to the community. The amount and type of communication members receive can play an important role in generating and sustaining members' commitment to it. However, the communication that members find valuable may change with their tenure in the community. This paper examines how the communication members receive in an health support community influences their commitment and how this influence changes with their tenure in the community. Commitment was operationalized with three measures: self-reported attachment, continued participation in the community, and responding to others. Results show that receiving communication was generally associated with increased commitment across the three measures, with its impact increasing with members' tenure. However, the average amount of informational and emotional support members received per message was associated with decreased commitment. Results have implications for interventions to encourage members' commitment to their communities throughout their history in the community.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Diyi and Kraut, Robert and Levine, John M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {communication, commitment, group socialization, onlinehealth support communities, social support},
	pages = {6363--6375},
}

@inproceedings{foote_starting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Starting {Online} {Communities}: {Motivations} and {Goals} of {Wiki} {Founders}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025639},
	doi = {10.1145/3025453.3025639},
	abstract = {Why do people start new online communities? Previous research has studied what helps communities to grow and what motivates contributors, but the reasons that people create new communities in the first place remain unclear. We present the results of a survey of over 300 founders of new communities on the online wiki hosting site Wikia.com. We analyze the motivations and goals of wiki creators, finding that founders have diverse reasons for starting wikis and diverse ways of defining their success. Many founders see their communities as occupying narrow topics, and neither seek nor expect a large group of contributors. We also find that founders with differing goals approach community building differently. We argue that community platform designers can create interfaces that support the diverse goals of founders more effectively.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Foote, Jeremy and Gergle, Darren and Shaw, Aaron},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {survey, motivation, peer production, online communities, wikis},
	pages = {6376--6380},
}

@inproceedings{balestra_investigating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Investigating the {Motivational} {Paths} of {Peer} {Production} {Newcomers}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026057},
	doi = {10.1145/3025453.3026057},
	abstract = {Maintaining participation beyond the initial period of engagement is critical for peer production systems. Theory suggests that an increase in motivation is expected with contributors' movement from the community periphery to the core. Less is known, however, about how specific motivations change over time. We fill this gap by focusing on individual motivational paths in the formative periods of engagement, exploring which motivations change and how. We collected data on various instrumental and non-instrumental motivations at two points in study participants? Wikipedia career: when they started editing and again after six months. We found that non-instrumental motivations (including collective and intrinsic motives) decreased significantly over time, in contrast with socially-driven motivations such as norm-oriented motivates which did not change and social motives which increased marginally. The findings offer new insights into newcomers' evolving motivations, with implications for designing and managing peer-production systems.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Balestra, Martina and Cheshire, Coye and Arazy, Ofer and Nov, Oded},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {motivation, peer production, wikipedia, newcomers},
	pages = {6381--6385},
}

@inproceedings{nassir_traversing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Traversing {Boundaries}: {Understanding} the {Experiences} of {Ageing} {Saudis}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025618},
	doi = {10.1145/3025453.3025618},
	abstract = {This is a methods paper that draws from our fieldwork conducted in Saudi Arabia to understand ageing people's experiences. This paper focuses on insights gained when using qualitative methods to understand the experiences of ageing Saudis. The aim is to highlight some of the cultural considerations, opportunities, challenges, and issues that influenced our approach and deployment of interviews and probes. Influences of social-cultural practices and religion led to interesting challenges for recruitment, conducting cross-gender communications, and how participants reported their experiences. This paper offers methodological considerations that include the influences of local culture, gender, religion, etc. We also discuss how we shaped our fieldwork tools based upon considerations of local cultural and religious contexts. In particular, we highlight the usefulness of probes in traversing cultural boundaries.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nassir, Soud and Leong, Tuck Wah},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {hci, gender, social media, privacy, interviews, methods, ageing, cultural considerations, probes, saudi arabia},
	pages = {6386--6397},
}

@inproceedings{durrant_transitions_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Transitions in {Digital} {Personhood}: {Online} {Activity} in {Early} {Retirement}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025913},
	doi = {10.1145/3025453.3025913},
	abstract = {We present findings from a qualitative study about how Internet use supports self-functioning following the life transition of retirement from work. This study recruited six recent retirees and included the deployment of OnLines, a design research artifact that logged and visualized key online services used by participants at home over four-weeks. The deployment was supported by pre- and post-deployment interviews. OnLines prompted participants' reflection on their patterns of Internet use. Position Exchange Theory was used to understand retirees' sense making from a lifespan perspective, informing the design of supportive online services. This paper delivers a three-fold contribution to the field of human-computer interaction, advancing a lifespan-oriented approach by conceptualizing the self as a dialogical phenomenon that develops over time, advancing the ageing discourse by reporting on retirees' complex identities in the context of their life histories, and advancing discourse on research through design by developing OnLines to foster participant-researcher reflection informed by Self Psychology.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Durrant, Abigail and Kirk, David and Trujillo Pisanty, Diego and Moncur, Wendy and Orzech, Kathryn and Schofield, Tom and Elsden, Chris and Chatting, David and Monk, Andrew},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {research through design, ageing, lifespan-oriented research, personhood, position exchange theory, retirement},
	pages = {6398--6411},
}

@inproceedings{oliveira_dissecting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Dissecting {Spear} {Phishing} {Emails} for {Older} vs {Young} {Adults}: {On} the {Interplay} of {Weapons} of {Influence} and {Life} {Domains} in {Predicting} {Susceptibility} to {Phishing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025831},
	doi = {10.1145/3025453.3025831},
	abstract = {Spear phishing emails are key in many cyber attacks. Successful emails employ psychological weapons of influence and relevant life domains. This paper investigates spear phishing susceptibility as a function of Internet user age (old vs young), weapon of influence, and life domain. A 21-day study was conducted with 158 participants (younger and older Internet users). Data collection took place at the participants' homes to increase ecological validity. Our results show that older women were the most vulnerable group to phishing attacks. While younger adults were most susceptible to scarcity, older adults were most susceptible to reciprocation. Further, there was a discrepancy, particularly among older users, between self-reported susceptibility awareness and their behavior during the intervention. Our results show the need for demographic personalization for warnings, training and educational tools in targeting the specifics of the older adult population.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oliveira, Daniela and Rocha, Harold and Yang, Huizi and Ellis, Donovan and Dommaraju, Sandeep and Muradoglu, Melis and Weir, Devon and Soliman, Adam and Lin, Tian and Ebner, Natalie},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {aging, principles of influence, spear phishing, susceptibility},
	pages = {6412--6424},
}

@inproceedings{mcneill_privacy_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Privacy {Considerations} {When} {Designing} {Social} {Network} {Systems} to {Support} {Successful} {Ageing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025861},
	doi = {10.1145/3025453.3025861},
	abstract = {A number of interventions exist to support older adults in ageing well and these typically involve support for an active and sociable ageing process. We set out to examine the privacy implications of an intervention that would monitor mobility and share lifestyle and health data with a community of trusted others. We took a privacy-by-design approach to the system in the early stages of its development, working with older adults to firstly understand their networks of trust and secondly understand their privacy concerns should information be exchanged across that network. We used a Johari Windows framework in the thematic analysis of our data, concluding that the social sharing of information in later life carried significant risk. Our participants worried about the social signaling associated with data sharing and were cautious about a system that had the potential to disrupt established networks.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McNeill, Andrew R. and Coventry, Lynne and Pywell, Jake and Briggs, Pam},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, health, trust, older adults, social networks},
	pages = {6425--6437},
}

@inproceedings{kasahara_malleable_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Malleable {Embodiment}: {Changing} {Sense} of {Embodiment} by {Spatial}-{Temporal} {Deformation} of {Virtual} {Human} {Body}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025962},
	doi = {10.1145/3025453.3025962},
	abstract = {We hypothesize that replacing the visual perception of one's body with a spatial-temporal deformed state would change sensations associated with the body. We developed a system that captures full-body movement and generates estimated past and future body movement by deformation. With a head mounted display, people could see their bodies as slightly deformed. We then investigated 1) how human movement is physically changed, and 2) how humans feel about the change in physical and emotional views of the body due to virtual body deformation. Our results show that spatial-temporal deformation of a virtual body actually changes the sense of body as well as physical movement. For instance, a body image generated at approximately 25-100 ms in the future induced a "lighter weight" sensation. On the basis of our findings, we discuss the design implication of computational control for the physical and emotional sense of body.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kasahara, Shunichi and Konno, Keina and Owaki, Richi and Nishi, Tsubasa and Takeshita, Akiko and Ito, Takayuki and Kasuga, Shoko and Ushiba, Junichi},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {"body ownership", "embodiment", "motion", "perception", "virtual reality"},
	pages = {6438--6448},
}

@inproceedings{krogh_sensitizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Sensitizing {Concepts} for {Socio}-{Spatial} {Literacy} in {HCI}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025756},
	doi = {10.1145/3025453.3025756},
	abstract = {People inherently share spaces with other people. Congenitally, interactive technologies and ubiquitous environments shape our opportunities for enacting social relations. Proxemics and Spatial Sharing have been suggested as foundations for our understanding of the socio-spatial aspects of computing. By tandeming these theoretical perspectives in a set of cases in the office domain, we develop a contribution comprised of 3 key sensitizing concepts: Proxemic Malleability, Proxemic Threshold and Proxemic Gravity articulating socio-spatial qualities at the interplay between interactive systems, spaces, interior elements and co-located people. The sensitizing concepts qualify interaction designers in considering proxemic consequences of technology design; they serve both as analytic lenses and as generative instruments in a design process. The proposed sensitizing concepts and the theoretical work of the paper contribute to enhanced Socio-spatial literacy in HCI.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Krogh, Peter Gall and Petersen, Marianne Graves and O'Hara, Kenton and Groenbaek, Jens Emil},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {interaction design, proxemics, ubiquitous computing, literacy, architecture., socio-spatial, space, spatiality},
	pages = {6449--6460},
}

@inproceedings{taylor_situational_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Situational {When}: {Designing} for {Time} {Across} {Cultures}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025936},
	doi = {10.1145/3025453.3025936},
	abstract = {We propose the concept of "Situational When", an approach to understanding time in interface design not as a point on a calendar or clock, but as a set of converging circumstances that constitute "the time" for happenings to take place. Time is encoded both explicitly and implicitly in designed products. However, many technologies propagate business-centric, modernist values such as scheduling and efficiency, and marginalize broader socio-cultural aspects on which many activities are nonetheless contingent, e.g. the right people, the right weather conditions, and the right vibe. We derive our reflections from a case study of a cross-cultural digital noticeboard designed with an Australian Aboriginal community. Attention to the situational when opens up new possibilities for design that put greater emphasis on the social and relational aspects of time, the situational insights embodied in local narratives, and the tangible (e.g. people) and intangible (e.g. energy) circumstances that together make up the "right" time.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Taylor, Jennyfer Lawrence and Soro, Alessandro and Roe, Paul and Lee Hong, Anita and Brereton, Margot},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {storytelling, temporality, aboriginal, breaching experiments, calendar, cross-cultural, noticeboard, time},
	pages = {6461--6474},
}

@inproceedings{grusky_modeling_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Modeling {Sub}-{Document} {Attention} {Using} {Viewport} {Time}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025916},
	doi = {10.1145/3025453.3025916},
	abstract = {Website measures of engagement captured from millions of users, such as in-page scrolling and viewport position, can provide deeper understanding of attention than possible with simpler measures, such as dwell time. Using data from 1.2M news reading sessions, we examine and evaluate three increasingly sophisticated models of sub-document attention computed from viewport time, the time a page component is visible on the user display. Our modeling incorporates prior eye-tracking knowledge about onscreen reading, and we validate it by showing how, when used to estimate user reading rate, it aligns with known empirical measures. We then show how our models reveal an interaction between article topic and attention to page elements. Our approach supports refined large-scale measurement of user engagement at a level previously available only from lab-based eye-tracking studies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Grusky, Max and Jahani, Jeiran and Schwartz, Josh and Valente, Dan and Artzi, Yoav and Naaman, Mor},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user modeling, attention, news articles, reading, web analytics},
	pages = {6475--6480},
}

@inproceedings{muller_remote_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Remote {Collaboration} {With} {Mixed} {Reality} {Displays}: {How} {Shared} {Virtual} {Landmarks} {Facilitate} {Spatial} {Referencing}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025717},
	doi = {10.1145/3025453.3025717},
	abstract = {HCI research has demonstrated Mixed Reality (MR) as being beneficial for co-located collaborative work. For remote collaboration, however, the collaborators' visual contexts do not coincide due to their individual physical environments. The problem becomes apparent when collaborators refer to physical landmarks in their individual environments to guide each other's attention. In an experimental study with 16 dyads, we investigated how the provisioning of shared virtual landmarks (SVLs) influences communication behavior and user experience. A quantitative analysis revealed that participants used significantly less ambiguous spatial expressions and reported an improved user experience when SVLs were provided. Based on these findings and a qualitative video analysis we provide implications for the design of MRs to facilitate remote collaboration.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Müller, Jens and Rädle, Roman and Reiterer, Harald},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {mixed reality, remote collaboration, virtual landmarks},
	pages = {6481--6486},
}

@inproceedings{jabbar_growing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Growing the {Blockchain} {Information} {Infrastructure}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025959},
	doi = {10.1145/3025453.3025959},
	abstract = {In this paper, we present ethnographic data that unpacks the everyday work of some of the many infrastructuring agents who contribute to creating, sustaining and growing the Blockchain information infrastructure. We argue that this infrastructuring work takes the form of entrepreneurial actions, which are self-initiated and primarily directed at sustaining or increasing the initiator's stake in the emerging information infrastructure. These entrepreneurial actions wrestle against the affordances of the installed base of the Blockchain infrastructure, and take the shape of engaging or circumventing activities. These activities purposefully aim at either influencing or working around the enablers and constraints afforded by the Blockchain information infrastructure, as its installed base is gaining inertia. This study contributes to our understanding of the purpose of infrastructuring, seen from the perspective of heterogeneous entrepreneurial agents. It supplements existing accounts of the "when" and "how" of infrastructure, with a lens for examining the "why" of infrastructure.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jabbar, Karim and Bjørn, Pernille},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {blockchain, bitcoin, entrepreneurship, information infrastructures, open-source, sociomateriality},
	pages = {6487--6498},
}

@inproceedings{sas_design_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Design for {Trust}: {An} {Exploration} of the {Challenges} and {Opportunities} of {Bitcoin} {Users}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025886},
	doi = {10.1145/3025453.3025886},
	abstract = {Bitcoin is a cryptocurrency which has received increasing interest over the last five years. Built upon a decentralized peer to peer system, it supports transparent, fast, cost effective, and irreversible transactions, without the need for trusting third party financial institutions. We know however little about people's motivation and experience with bitcoin currency. This paper reports on interviews with 20 bitcoin users in Malaysia about their experience and trust challenges. Findings show that bitcoins are used more as store of value for speculative investment or savings' protection. The paper advances the HCI theories on trust by identifying main bitcoin characteristics and their impact on trust, such as decentralization, unregulation, embedded expertise, and reputation, as well as transactions' transparency, low cost, and easiness to complete. We discuss insecure transactions, the risk of dishonest traders and its mitigating strategies. The paper concludes with design implications including support for the transparency of two-way transactions, tools for materializing trust, and tools for supporting reversible transactions.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sas, Corina and Khairuddin, Irni Eliana},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {blockchain, trust, bitcoin users, dishonest traders, risks},
	pages = {6499--6510},
}

@inproceedings{jack_infrastructure_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Infrastructure as {Creative} {Action}: {Online} {Buying}, {Selling}, and {Delivery} in {Phnom} {Penh}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025889},
	doi = {10.1145/3025453.3025889},
	abstract = {This paper describes a complex global sales and logistics network based in Phnom Penh, Cambodia, which utilizes Internet tools (particularly Facebook) as well as a suite of offline tools such as feature phones, paper receipts, and motorcycles to facilitate the buying and selling of clothes and other commodities. Against the gap or import models that sometimes limit HCI understandings of computational change in non-Western environments, we argue that the consumers, business owners, delivery drivers, and call center staff play active and formative roles in producing this infrastructure, integrating new tools into older cultural practices and determining how they work within the limits and conventions of the environment. We argue that resourceful and imaginative activities such as these constitute a form of creative infrastructural action and are central to the ways that new tools circulate in the world, though they often go unrecognized by HCI as innovation.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jack, Margaret and Chen, Jay and Jackson, Steven J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {ethnography, ICTD, infrastructure, e-commerce, logistics, postcolonial computing},
	pages = {6511--6522},
}

@inproceedings{bardzell_supporting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Supporting {Cultures} of {Making}: {Technology}, {Policy}, {Visions}, and {Myths}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025975},
	doi = {10.1145/3025453.3025975},
	abstract = {Recent HCI research has linked social policy to design, e.g., in issues such as public safety, privacy, and social justice. One area where policy, technology, and design intersect is in the vision of the creative economy. In that vision, creativity, distinct local/regional cultural practices, technology, and entrepreneurship synergistically produce social innovation on a scale sufficient to drive economies. Culture and creative industries (CCI) policy specifies how governments intervene to support such clusters. Maker cultures are seen as central to this vision, but comparatively little is known about how makers produce culture. We offer a critical analysis of several encounters between CCI policy in Taiwan and its maker scene. These encounters reveal misalignments that undercut efforts intended to support making. We propose that supporting any creative culture, including making, entails a serious commitment to understanding its culture, including its cultural contents and their means of production. We further argue that scholarly rigor in cultivating cultural appreciation is just as fundamental as scholarly rigor in empirically representing cultural practices when it comes to pursuing such a cultural understanding.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bardzell, Shaowen and Bardzell, Jeffrey and Ng, Sarah},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {policy, creativity support, making, critical computing, culture and creative industries, east asia, hackerspaces, maker cultures, social innovation, taiwan},
	pages = {6523--6535},
}

@inproceedings{higuchi_egoscanning_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{EgoScanning}: {Quickly} {Scanning} {First}-{Person} {Videos} with {Egocentric} {Elastic} {Timelines}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025821},
	doi = {10.1145/3025453.3025821},
	abstract = {This work presents EgoScanning, a novel video fast-forwarding interface that helps users to find important events from lengthy first-person videos recorded with wearable cameras continuously. This interface is featured by an elastic timeline that adaptively changes playback speeds and emphasizes egocentric cues specific to first-person videos, such as hand manipulations, moving, and conversations with people, based on computer-vision techniques. The interface also allows users to input which of such cues are relevant to events of their interests. Through our user study, we confirm that users can find events of interests quickly from first-person videos thanks to the following benefits of using the EgoScanning interface: 1) adaptive changes of playback speeds allow users to watch fast-forwarded videos more easily; 2) Emphasized parts of videos can act as candidates of events actually significant to users; 3) Users are able to select relevant egocentric cues depending on events of their interests.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Higuchi, Keita and Yonetani, Ryo and Sato, Yoichi},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {content-aware video fast-forwarding, first-person videos},
	pages = {6536--6546},
}

@inproceedings{mohr_retargeting_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Retargeting {Video} {Tutorials} {Showing} {Tools} {With} {Surface} {Contact} to {Augmented} {Reality}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025688},
	doi = {10.1145/3025453.3025688},
	abstract = {A video tutorial effectively conveys complex motions, but may be hard to follow precisely because of its restriction to a predetermined viewpoint. Augmented reality (AR) tutorials have been demonstrated to be more effective. We bring the advantages of both together by interactively retargeting conventional, two-dimensional videos into three-dimensional AR tutorials. Unlike previous work, we do not simply overlay video, but synthesize 3D-registered motion from the video. Since the information in the resulting AR tutorial is registered to 3D objects, the user can freely change the viewpoint without degrading the experience. This approach applies to many styles of video tutorials. In this work, we concentrate on a class of tutorials which alter the surface of an object.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mohr, Peter and Mandl, David and Tatzgern, Markus and Veas, Eduardo and Schmalstieg, Dieter and Kalkofen, Denis},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {virtual reality, augmented reality, retargeting, video tutorial},
	pages = {6547--6558},
}

@inproceedings{kurzhals_close_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Close to the {Action}: {Eye}-{Tracking} {Evaluation} of {Speaker}-{Following} {Subtitles}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025772},
	doi = {10.1145/3025453.3025772},
	abstract = {The incorporation of subtitles in multimedia content plays an important role in communicating spoken content. For example, subtitles in the respective language are often preferred to expensive audio translation of foreign movies. The traditional representation of subtitles displays text centered at the bottom of the screen. This layout can lead to large distances between text and relevant image content, causing eye strain and even that we miss visual content. As a recent alternative, the technique of speaker-following subtitles places subtitle text in speech bubbles close to the current speaker. We conducted a controlled eye-tracking laboratory study (n = 40) to compare the regular approach (center-bottom subtitles) with content-sensitive, speaker-following subtitles. We compared different dialog-heavy video clips with the two layouts. Our results show that speaker-following subtitles lead to higher fixation counts on relevant image regions and reduce saccade length, which is an important factor for eye strain.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kurzhals, Kuno and Cetinkaya, Emine and Hu, Yongtao and Wang, Wenping and Weiskopf, Daniel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {eye tracking, video, subtitle layout},
	pages = {6559--6568},
}

@inproceedings{ilisescu_responsive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Responsive {Action}-{Based} {Video} {Synthesis}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025880},
	doi = {10.1145/3025453.3025880},
	abstract = {We propose technology to enable a new medium of expression, where video elements can be looped, merged, and triggered, interactively. Like audio, video is easy to sample from the real world, but hard to segment into clean reusable elements. Reusing a video clip means non-linear editing, and compositing with novel footage. The new context dictates how carefully a clip must be prepared, so our end-to-end approach enables previewing and easy iteration. We convert static-camera videos into loopable sequences, synthesizing them in response to simple end-user requests. This is hard because a) users want essentially semantic-level control over the synthesized video content, and b) automatic loop-finding is brittle and leaves users limited opportunity to work through problems. We propose a human-in-the-loop system where adding effort gives the user progressively more creative control. Artists help us evaluate how our trigger interfaces can be used for authoring of videos and video-performances.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ilisescu, Corneliu and Kanaci, Halil Aytac and Romagnoli, Matteo and Campbell, Neill D. F. and Brostow, Gabriel J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {interactive machine learning, video editing, cinemagraphs, sprites, video textures},
	pages = {6569--6580},
}

@inproceedings{piya_co-3deator_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Co-{3Deator}: {A} {Team}-{First} {Collaborative} {3D} {Design} {Ideation} {Tool}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025825},
	doi = {10.1145/3025453.3025825},
	abstract = {We present Co-3Deator, a sketch-based collaborative 3D modeling system based on the notion of "team-first" ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Piya, Cecil and -, Vinayak and Chandrasegaran, Senthil and Elmqvist, Niklas and Ramani, Karthik},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {early-stage design, 3D modeling, collaborative design, creative ideation},
	pages = {6581--6592},
}

@inproceedings{khot_edipulse_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {\textit{{EdiPulse}}: {Investigating} a {Playful} {Approach} to {Self}-{Monitoring} through {3D} {Printed} {Chocolate} {Treats}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025980},
	doi = {10.1145/3025453.3025980},
	abstract = {Self-monitoring offers benefits in facilitating awareness about physical exercise, but such data-centric activity may not always lead to an enjoyable experience. We introduce EdiPulse a novel system that creates activity treats to offer playful reflections on everyday physical activity through the appealing medium of chocolate. EdiPulse translates self-monitored data from physical activity into small 3D printed chocolate treats. These treats (\&lt; 20 grams of chocolate in total) embody four forms: Graph, Flower, Slogan and Emoji. We deployed our system across 7 households and studied its use with 13 participants for 2 weeks per household. The field study revealed positive aspects of our approach along with some open challenges, which we disseminate across five themes: Reflection, Positivity, Determination, Affection, and Co-experience. We conclude by highlighting key implications of our work for future playful food-based technology design in supporting the experience of being physically active},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Khot, Rohit Ashok and Aggarwal, Deepti and Pennings, Ryan and Hjorth, Larissa and Mueller, Florian 'Floyd'},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {physical activity, quantified self, self-monitoring, chocolate printing, food printing, human food interaction, playful interactions},
	pages = {6593--6607},
}

@inproceedings{paay_investigating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Investigating {Cross}-{Device} {Interaction} between a {Handheld} {Device} and a {Large} {Display}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025724},
	doi = {10.1145/3025453.3025724},
	abstract = {There is a growing interest in HCI research to explore cross-device interaction, giving rise to an interest in different approaches facilitating interaction between handheld devices and large displays. Contributing to this, we have investigated the use of four existing approaches combining touch and mid-air gestures, pinching, swiping, swinging and flicking. We look specifically at their relative efficiency, effectiveness and accuracy in bi-directional interaction between a smartphone and large display in a point-click context. We report findings from two user studies, which show that swiping is both most effective, fastest and most accurate, closely followed by swinging. What these two approaches have in common is the ability to keep the pointer steady on the large display, unaffected by concurrent gestures or body movements used to complete the interaction, suggesting that this is an important factor for designing effective cross-device interaction with large displays.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Paay, Jeni and Raptis, Dimitrios and Kjeldskov, Jesper and Skov, Mikael B. and Ruder, Eric V. and Lauridsen, Bjarke M.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {kinect, touch, large displays, cross-device interaction, handheld devices, mid-air gestures},
	pages = {6608--6619},
}

@inproceedings{otterbacher_competent_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Competent {Men} and {Warm} {Women}: {Gender} {Stereotypes} and {Backlash} in {Image} {Search} {Results}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025727},
	doi = {10.1145/3025453.3025727},
	abstract = {There is much concern about algorithms that underlie information services and the view of the world they present. We develop a novel method for examining the content and strength of gender stereotypes in image search, inspired by the trait adjective checklist method. We compare the gender distribution in photos retrieved by Bing for the query "person" and for queries based on 68 character traits (e.g., "intelligent person") in four regional markets. Photos of men are more often retrieved for "person," as compared to women. As predicted, photos of women are more often retrieved for warm traits (e.g., "emotional") whereas agentic traits (e.g., "rational") are represented by photos of men. A backlash effect, where stereotype-incongruent individuals are penalized, is observed. However, backlash is more prevalent for "competent women" than "warm men." Results underline the need to understand how and why biases enter search algorithms and at which stages of the engineering process.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Otterbacher, Jahna and Bates, Jo and Clough, Paul},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {"big two" dimensions of social perception, algorithmic bias, gender stereotypes, image search},
	pages = {6620--6631},
}

@inproceedings{ambe_technology_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Technology {Individuation}: {The} {Foibles} of {Augmented} {Everyday} {Objects}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025770},
	doi = {10.1145/3025453.3025770},
	abstract = {This paper presents the concept of technology individuation and explores its role in design. Individuation expresses how, over time, a technology becomes personal and intimate, unique in purpose, orchestrated in place, and how people eventually come to rely on it to sustain connection with others. We articulate this concept as a critical vantage point for designing augmented everyday objects and the Internet of Things. Individuation foregrounds aspects of habituation, routines and arrangements that through everyday practices reveal unique meaning, reflect self-identity and support agency.The concept is illustrated through three long term case studies of technology in use, involving tangible and embodied interaction with devices that afford communication, monitoring, and awareness in the home setting. The cases are analysed using Hornecker and Buur's Tangible Interaction Framework. We further extend upon this framework to better reveal the role played by personal values, history of use, and arrangements, as they develop over time in the home setting, in shaping tangible and embodied interaction with individuated technologies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ambe, Aloha Hufana and Brereton, Margot and Soro, Alessandro and Roe, Paul},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design, internet of things, tangible, framework, communication, connection, embodied, habituation, individuation, objects, situated., smart, things},
	pages = {6632--6644},
}

@inproceedings{taylor_social_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Social {Consequences} of {Grindr} {Use}: {Extending} the {Internet}-{Enhanced} {Self}-{Disclosure} {Hypothesis}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025775},
	doi = {10.1145/3025453.3025775},
	abstract = {Grindr, a location-based real-time dating application, provides sexual-minority men (SMM) a space through which they can identify, access, and communicate with one another. Although previous research has examined user motivations and public self-disclosure patterns on Grindr, we investigate the effects intimate self-disclosure and sexting via the application's private messaging on internalized homophobia and loneliness. Using the Internet-enhanced self-disclosure hypothesis (ISDH) as a framework, we conducted an online survey of 274 Grindr users. Serial mediation analysis showed support for the ISDH, suggesting that Grindr use was negatively associated with loneliness. Intimate self-disclosure and internalized homophobia mediated the relationship between Grindr use and loneliness, but sexting had no relationship with internalized homophobia or loneliness. We discuss implications for the ISDH, Grindr, self-disclosure, and sexting.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Taylor, Samuel Hardman and Hutson, Jevan Alexander and Alicea, Tyler Richard},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {self-disclosure, grindr, internalized homophobia, loneliness, sexting},
	pages = {6645--6657},
}

@inproceedings{hill_gender-inclusiveness_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gender-{Inclusiveness} {Personas} vs. {Stereotyping}: {Can} {We} {Have} {It} {Both} {Ways}?},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025609},
	doi = {10.1145/3025453.3025609},
	abstract = {Personas often aim to improve product designers' ability to "see through the eyes of" target users through the empathy personas can inspire - but personas are also known to promote stereotyping. This tension can be particularly problematic when personas (who, of course as "people" have genders) are used to promote gender inclusiveness - because reinforcing stereotypical perceptions can run counter to gender inclusiveness. In this paper we explicitly investigate this tension through a new approach to personas: one that includes multiple photos (of males and females) for a single persona. We compared this approach to an identical persona with only one photo using a controlled laboratory study and an eye-tracking study. Our goal was to answer the following question: is it possible for personas to encourage product designers to engage with personas while at the same avoiding promoting gender stereotyping? Our results are encouraging about the use of personas with multiple pictures as a way to expand participants' consideration of multiple genders without reducing their engagement with the persona.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hill, Charles G. and Haag, Maren and Oleson, Alannah and Mendez, Chris and Marsden, Nicola and Sarma, Anita and Burnett, Margaret},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {gender, personas, gendermag, lab study, stereotypes},
	pages = {6658--6671},
}

@inproceedings{hanafi_seer_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{SEER}: {Auto}-{Generating} {Information} {Extraction} {Rules} from {User}-{Specified} {Examples}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025540},
	doi = {10.1145/3025453.3025540},
	abstract = {Time-consuming and complicated best describe the current state of the Information Extraction (IE) field. Machine learning approaches to IE require large collections of labeled datasets that are difficult to create and use obscure mathematical models, occasionally returning unwanted results that are unexplainable. Rule-based approaches, while resulting in easy-to-understand IE rules, are still time-consuming and labor-intensive. SEER combines the best of these two approaches: a learning model for IE rules based on a small number of user-specified examples. In this paper, we explain the design behind SEER and present a user study comparing our system against a commercially available tool in which users create IE rules manually. Our results show that SEER helps users complete text extraction tasks more quickly, as well as more accurately.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hanafi, Maeda F. and Abouzied, Azza and Chiticariu, Laura and Li, Yunyao},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {data extraction, example-driven learning},
	pages = {6672--6682},
}

@inproceedings{banovic_leveraging_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Leveraging {Human} {Routine} {Models} to {Detect} and {Generate} {Human} {Behaviors}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025571},
	doi = {10.1145/3025453.3025571},
	abstract = {An ability to detect behaviors that negatively impact people's wellbeing and show people how they can correct those behaviors could enable technology that improves people's lives. Existing supervised machine learning approaches to detect and generate such behaviors require lengthy and expensive data labeling by domain experts. In this work, we focus on the domain of routine behaviors, where we model routines as a series of frequent actions that people perform in specific situations. We present an approach that bypasses labeling each behavior instance that a person exhibits. Instead, we weakly label instances using people's demonstrated routine. We classify and generate new instances based on the probability that they belong to the routine model. We illustrate our approach on an example system that helps drivers become aware of and understand their aggressive driving behaviors. Our work enables technology that can trigger interventions and help people reflect on their behaviors when those behaviors are likely to negatively impact them.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Banovic, Nikola and Wang, Anqi and Jin, Yanfeng and Chang, Christie and Ramos, Julian and Dey, Anind and Mankoff, Jennifer},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {inverse reinforcement learning, maximum entropy},
	pages = {6683--6694},
}

@inproceedings{xie_interactive_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Interactive {Vectorization}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025872},
	doi = {10.1145/3025453.3025872},
	abstract = {Vectorization turns photographs into vector art. Manual vectorization, where the artist traces over the image by hand, requires skill and time. On the other hand, automatic approaches allow users to generate a result by setting a few global parameters. However, global settings often leave too much detail/complexity in some parts of the image while missing important details in others. We propose interactive vectorization tools that offer more local control than automatic systems, but are more powerful and high-level than simple curve editing. Our system enables novices to vectorize images significantly faster than even experts with state-of-the-art tools.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Jun and Winnemöller, Holger and Li, Wilmot and Schiller, Stephen},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {image analysis, image vectorization, user interaction},
	pages = {6695--6705},
}

@inproceedings{jung_chartsense_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{ChartSense}: {Interactive} {Data} {Extraction} from {Chart} {Images}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025957},
	doi = {10.1145/3025453.3025957},
	abstract = {Charts are commonly used to present data in digital documents such as web pages, research papers, or presentation slides. When the underlying data is not available, it is necessary to extract the data from a chart image to utilize the data for further analysis or improve the chart for more accurate perception. In this paper, we present ChartSense, an interactive chart data extraction system. ChartSense first determines the chart type of a given chart image using a deep learning based classifier, and then extracts underlying data from the chart image using semi-automatic, interactive extraction algorithms optimized for each chart type. To evaluate chart type classification accuracy, we compared ChartSense with ReVision, a system with the state-of-the-art chart type classifier. We found that ChartSense was more accurate than ReVision. In addition, to evaluate data extraction performance, we conducted a user study, comparing ChartSense with WebPlotDigitizer, one of the most effective chart data extraction tools among publicly accessible ones. Our results showed that ChartSense was better than WebPlotDigitizer in terms of task completion time, error rate, and subjective preference.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jung, Daekyoung and Kim, Wonjae and Song, Hyunjoo and Hwang, Jeong-in and Lee, Bongshin and Kim, Bohyoung and Seo, Jinwook},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {deep learning, data extraction, chart classification, chart recognition, mixed-initiative interaction},
	pages = {6706--6717},
}

@inproceedings{avellino_camray_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{CamRay}: {Camera} {Arrays} {Support} {Remote} {Collaboration} on {Wall}-{Sized} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025604},
	doi = {10.1145/3025453.3025604},
	abstract = {Remote collaboration across wall-sized displays creates a key challenge: how to support audio-video communication among users as they move in front of the display. We present CamRay, a platform that uses camera arrays embedded in wall-sized displays to capture video of users and present it on remote displays according to the users' positions. We investigate two settings: in Follow-Remote, the position of the video window follows the position of the remote user; in Follow-Local, the video window always appears in front of the local user. We report the results of a controlled experiment showing that with Follow-Remote, participants are faster, use more deictic instructions, interpret them more accurately, and use fewer words. However, some participants preferred the virtual face-to-face created by Follow-Local when checking for their partners' understanding. We conclude with design recommendations to support remote collaboration across wall-sized displays.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Avellino, Ignacio and Fleury, Cédric and Mackay, Wendy E. and Beaudouin-Lafon, Michel},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {telepresence, remote collaboration, camera array, wall-sized displays},
	pages = {6718--6729},
}

@inproceedings{liu_coreach_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{CoReach}: {Cooperative} {Gestures} for {Data} {Manipulation} on {Wall}-{Sized} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025594},
	doi = {10.1145/3025453.3025594},
	abstract = {Multi-touch wall-sized displays afford collaborative exploration of large datasets and re-organization of digital content. However, standard touch interactions, such as dragging to move content, do not scale well to large surfaces and were not designed to support collaboration, such as passing an object around. This paper introduces CoReach, a set of collaborative gestures that combine input from multiple users in order to manipulate content, facilitate data exchange and support communication. We conducted an observational study to inform the design of CoReach, and a controlled study showing that it reduced physical fatigue and facilitated collaboration when compared with traditional multi-touch gestures. A final study assessed the value of also allowing input through a handheld tablet to manipulate content from a distance.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Can and Chapuis, Olivier and Beaudouin-Lafon, Michel and Lecolinet, Eric},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {co-located collaboration, shared interaction, wall display},
	pages = {6730--6741},
}

@inproceedings{cockburn_turbulent_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Turbulent {Touch}: {Touchscreen} {Input} for {Cockpit} {Flight} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025584},
	doi = {10.1145/3025453.3025584},
	abstract = {Touchscreen input in commercial aircraft cockpits offers potential advantages, including ease of use, modifiability, and reduced weight. However, tolerance to turbulence is a challenge for their deployment. To better understand the impact of turbulence on cockpit input methods we conducted a comparative study of user performance with three input methods – touch, trackball (as currently used in commercial aircraft), and a touchscreen stencil overlay designed to assist finger stabilization. These input methods were compared across a variety of interactive tasks and at three levels of simulated turbulence (none, low, and high). Results showed that performance degrades and subjective workload increases as vibration increases. Touch-based interaction was faster than the trackball when precision requirements were low (at all vibrations), but it was slower and less accurate for more precise pointing, particularly at high vibrations. The stencil did not improve touch selection times, although it did reduce errors on small targets at high vibrations, but only when finger lift-off errors had been eliminated by a timeout. Our work provides new information on the types of tasks affected by turbulence and the input mechanisms that perform best under different levels of vibration.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cockburn, Andy and Gutwin, Carl and Palanque, Philippe and Deleris, Yannick and Trask, Catherine and Coveney, Ashley and Yung, Marcus and MacLean, Karon},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {aviation, touch interaction, turbulence},
	pages = {6742--6753},
}

@inproceedings{dierk_alternail_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{AlterNail}: {Ambient}, {Batteryless}, {Stateful}, {Dynamic} {Displays} at {Your} {Fingertips}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025924},
	doi = {10.1145/3025453.3025924},
	abstract = {Beyond phones, watches, and activity tracking devices, a new ecosystem of functional and fashionable wearable technologies can easily, safely, and economically be designed, prototyped, and integrated directly on the body. In this paper, we present AlterNail, a fingernail form factor, ambient, low-power, stateful, wireless, dynamic display with onboard vibrational sensing. AlterNail integrates a batteryless design using inductive coupling with e-ink technology to enable both quick dynamic and long-term static fingernail based visual designs without the need for power. We also detail the use of simple vibrational signals to uniquely identify everyday objects as they are handled using AlterNails. The intentionally limited interactional functionality of AlterNails, coupled with the rich personal and dynamic expressive potential, combine to present a compelling range of opportunities for designers of new interactive wearable technologies. We detail a range of practical and playful applications using this technology.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dierk, Christine and Vega Gálvez, Tomás and Paulos, Eric},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {wearables, ambient devices, cosmetic computing, fingernails},
	pages = {6754--6759},
}

@inproceedings{wallace_subtle_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Subtle and {Personal} {Workspace} {Requirements} for {Visual} {Search} {Tasks} on {Public} {Displays}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025500},
	doi = {10.1145/3025453.3025500},
	abstract = {We explore how users approach and define personal space on large, public displays. Our results show that users of public displays use one of two strategies for visual search tasks: minimizers create a small window and work up close to the display, and maximizers expand content to its full resolution and work at a distance. We show that these interaction styles match predicted 'personal' and 'subtle' interaction zones, characterize typical width and height requirements for these interactions, and show that these requirements are independent of the on-screen content's dimensions. Finally, we suggest practical guidelines for defining workspaces during personal and subtle interaction on large, public displays.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wallace, James R. and Weingarten, Ariel and Lank, Edward},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {visual search, large public displays, personal workspace},
	pages = {6760--6764},
}

@inproceedings{chang_spiders_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Spiders in the {Sky}: {User} {Perceptions} of {Drones}, {Privacy}, and {Security}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025632},
	doi = {10.1145/3025453.3025632},
	abstract = {Drones are increasingly being used for various purposes from recording footage in inaccessible areas to delivering packages. A rise in drone usage introduces privacy and security concerns about flying boundaries, what data drones collect in public and private spaces, and how that data is stored and disseminated. However, commercial and personal drone regulations focusing on privacy and security have been fairly minimal in the USA. To inform privacy and security guidelines for drone design and regulation, we need to understand users' perceptions about drones, privacy and security. In this paper, we describe a laboratory study with 20 participants who interacted with a real or model drone to elicit user perceptions of privacy and security issues around drones. We present our results, discuss the implications of our work and make recommendations to improve drone design and regulations that enhance individual privacy and security.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Victoria and Chundury, Pramod and Chetty, Marshini},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, drones, usable security, quadcopter, users},
	pages = {6765--6776},
}

@inproceedings{yao_privacy_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Privacy {Mechanisms} for {Drones}: {Perceptions} of {Drone} {Controllers} and {Bystanders}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025907},
	doi = {10.1145/3025453.3025907},
	abstract = {Drones pose privacy concerns such as surveillance and stalking. Many technology-based or policy-based mechanisms have been proposed to mitigate these concerns. However, it is unclear how drone controllers and bystanders perceive these mechanisms and whether people intend to adopt them. In this paper, we report results from two rounds of online survey with 169 drone controllers and 717 bystanders in the U.S. We identified respondents' perceived pros and cons of eight privacy mechanisms. We found that owner registration and automatic face blurring individually received most support from both controllers and bystanders. Our respondents also suggested using varied combinations of mechanisms under different drone usage scenarios, highlighting their context-dependent preferences. We outline a set of important questions for future privacy designs and public policies of drones.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yao, Yaxing and Xia, Huichuan and Huang, Yun and Wang, Yang},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {drone, perceptions, privacy mechanisms, UAS, UAV},
	pages = {6777--6788},
}

@inproceedings{yao_free_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Free to {Fly} in {Public} {Spaces}: {Drone} {Controllers}' {Privacy} {Perceptions} and {Practices}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026049},
	doi = {10.1145/3025453.3026049},
	abstract = {Prior research has discovered various privacy concerns that bystanders have about drones. However, little is known about drone controllers' privacy perceptions and practices of drones. Understanding controllers' perspective is important because it will inform whether controllers' current practices protect or infringe on bystanders' privacy and what mechanisms could be designed to better address the potential privacy issues of drones. In this paper, we report results from interviews of 12 drone controllers in the US. Our interviewees treated safety as their top priority but considered privacy issues of drones exaggerated. Our results also highlight many significant differences in how controllers and bystanders think about drone privacy, for instance, how they determine public vs. private spaces and whether notice and consent of bystanders are needed.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yao, Yaxing and Xia, Huichuan and Huang, Yun and Wang, Yang},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {privacy, surveillance, drone, perceptions, uas, uav},
	pages = {6789--6793},
}

@inproceedings{e_drone_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Drone \&amp; {Wo}: {Cultural} {Influences} on {Human}-{Drone} {Interaction} {Techniques}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025755},
	doi = {10.1145/3025453.3025755},
	abstract = {As drones become ubiquitous, it is important to understand how cultural differences impact human-drone interaction. A previous elicitation study performed in the USA illustrated how users would intuitively interact with drones. We replicated this study in China to gain insight into how these user-defined interactions vary across the two cultures. We found that as per the US study, Chinese participants chose to interact primarily using gesture. However, Chinese participants used multi-modal interactions more than their US counterparts. Agreement for many proposed interactions was high within each culture. Across cultures, there were notable differences despite similarities in interaction modality preferences. For instance, culturally-specific gestures emerged in China, such as a T-shape gesture for stopping the drone. Participants from both cultures anthropomorphized the drone, and welcomed it into their personal space. We describe the implications of these findings on designing culturally-aware and intuitive human-drone interaction.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {E, Jane L. and E, Ilene L. and Landay, James A. and Cauchard, Jessica R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {elicitation study, human-drone interaction, cross-cultural design, quadcopter, drone, uav, gesture},
	pages = {6794--6799},
}

@inproceedings{oliveira_citizen_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Citizen {Science} {Opportunities} in {Volunteer}-{Based} {Online} {Experiments}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025473},
	doi = {10.1145/3025453.3025473},
	abstract = {Online experimentation with volunteers could be described as a form of citizen science in which participants take part in behavioral studies without financial compensation. However, while citizen science projects aim to improve scientific understanding, volunteer-based online experiment platforms currently provide minimal possibilities for research involvement and learning. The goal of this paper is to uncover opportunities for expanding participant involvement and learning in the research process. Analyzing comments from 8,288 volunteers who took part in four online experiments on LabintheWild, we identified six themes that reveal needs and opportunities for closer interaction between researchers and participants. Our findings demonstrate opportunities for research involvement, such as engaging participants in refining experiment implementations, and learning opportunities, such as providing participants with possibilities to learn about research aims. We translate these findings into ideas for the design of future volunteer-based online experiment platforms that are more mutually beneficial to citizen scientists and researchers.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Oliveira, Nigini and Jun, Eunice and Reinecke, Katharina},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {citizen science, online experimentation, open science},
	pages = {6800--6812},
}

@inproceedings{findlater_differences_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Differences in {Crowdsourced} vs. {Lab}-{Based} {Mobile} and {Desktop} {Input} {Performance} {Data}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025820},
	doi = {10.1145/3025453.3025820},
	abstract = {Research on the viability of using crowdsourcing for HCI performance experiments has concluded that online results are similar to those achieved in the lab—at least for desktop interactions. However, mobile devices, the most popular form of online access today, may be more problematic due to variability in the user's posture and in movement of the device. To assess this possibility, we conducted two experiments with 30 lab-based and 303 crowdsourced participants using basic mouse and touchscreen tasks. Our findings show that: (1) separately analyzing the crowd and lab data yields different study conclusions-touchscreen input was significantly less error prone than mouse input in the lab but more error prone online; (2) age-matched crowdsourced participants were significantly faster and less accurate than their lab-based counterparts, contrasting past work; (3) variability in mobile device movement and orientation increased as experimenter control decreased–a potential factor affecting the touchscreen error differences. This study cautions against assuming that crowdsourced data for performance experiments will directly reflect lab-based data, particularly for mobile devices.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Findlater, Leah and Zhang, Joan and Froehlich, Jon E. and Moffatt, Karyn},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, mobile, human performance, input devices},
	pages = {6813--6824},
}

@inproceedings{pandey_gut_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Gut {Instinct}: {Creating} {Scientific} {Theories} with {Online} {Learners}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025769},
	doi = {10.1145/3025453.3025769},
	abstract = {Learners worldwide collectively spend millions of hours per week testing their skills on assignments with known answers. Might some of this time fruitfully be spent posing and exploring novel questions? This paper investigates an approach for learners to contribute scientific ideas. The Gut Instinct system embodies this approach, hosting online learning materials and invites learners to collaboratively brainstorm potential influences on people's microbiome. A between-subjects experiment compared the performance of participants who engaged in just learning, just contributing, or a combination. Participants in the learning condition scored highest on a summative test. Participants in both the contribution and combined conditions generated novel, useful questions; there was not a significant difference between the two. Though participants in the combined condition both learned and contributed, this setting did not exhibit an additive benefit, such as better learning in the combined condition. These results highlight the promise and difficulty of double-bottom-line learning experiences.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pandey, Vineet and Amir, Amnon and Debelius, Justine and Hyde, Embriette R. and Kosciolek, Tomasz and Knight, Rob and Klemmer, Scott},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {crowdsourcing, citizen science, online learning, social computing systems},
	pages = {6825--6836},
}

@inproceedings{lee_self-experimentation_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Self-{Experimentation} for {Behavior} {Change}: {Design} and {Formative} {Evaluation} of {Two} {Approaches}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026038},
	doi = {10.1145/3025453.3026038},
	abstract = {Desirable outcomes such as health are tightly linked to behaviors, thus inspiring research on technologies that support people in changing those behaviors. Many behavior-change technologies are designed by HCI experts but this approach can make it difficult to personalize support to each user's unique goals and needs. This paper reports on the iterative design of two complementary support strategies for helping users create their own personalized behavior-change plans via self-experimentation: One emphasized the use of interactive instructional materials, and the other additionally introduced context-aware computing to enable user creation of "just in time" home-based interventions. In a formative trial with 27 users, we compared these two approaches to an unstructured sleep education control. Results suggest great promise in both strategies and provide insights on how to develop personalized behavior-change technologies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lee, Jisoo and Walker, Erin and Burleson, Winslow and Kay, Matthew and Buman, Matthew and Hekler, Eric B.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {behavior change, context-aware computing, just-in-time interventions, self-experimentation},
	pages = {6837--6849},
}

@inproceedings{karkar_tummytrials_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{TummyTrials}: {A} {Feasibility} {Study} of {Using} {Self}-{Experimentation} to {Detect} {Individualized} {Food} {Triggers}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025480},
	doi = {10.1145/3025453.3025480},
	abstract = {Diagnostic self-tracking, the recording of personal information to diagnose or manage a health condition, is a common practice, especially for people with chronic conditions. Unfortunately, many who attempt diagnostic self tracking have trouble accomplishing their goals. People often lack knowledge and skills needed to design and conduct scientifically rigorous experiments, and current tools provide little support. To address these shortcomings and explore opportunities for diagnostic self tracking, we designed, developed, and evaluated a mobile app that applies a self experimentation framework to support patients suffering from irritable bowel syndrome (IBS) in identifying their personal food triggers. TummyTrials aids a person in designing, executing, and analyzing self experiments to evaluate whether a specific food triggers their symptoms. We examined the feasibility of this approach in a field study with 15 IBS patients, finding that participants could use the tool to reliably undergo a self-experiment. However, we also discovered an underlying tension between scientific validity and the lived experience of self experimentation. We discuss challenges of applying clinical research methods in everyday life, motivating a need for the design of self experimentation systems to balance rigor with the uncertainties of everyday life.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Karkar, Ravi and Schroeder, Jessica and Epstein, Daniel A. and Pina, Laura R. and Scofield, Jeffrey and Fogarty, James and Kientz, Julie A. and Munson, Sean A. and Vilardaga, Roger and Zia, Jasmine},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {self-tracking, personal informatics, self-experimentation, food, irritable bowel syndrome, symptom triggers},
	pages = {6850--6863},
}

@inproceedings{ravichandran_making_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Making {Sense} of {Sleep} {Sensors}: {How} {Sleep} {Sensing} {Technologies} {Support} and {Undermine} {Sleep} {Health}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025557},
	doi = {10.1145/3025453.3025557},
	abstract = {Sleep is an important aspect of our health, but it is difficult for people to track manually because it is an unconscious activity. The ability to sense sleep has aimed to lower the barriers of tracking sleep. Although sleep sensors are widely available, their usefulness and potential to promote healthy sleep behaviors has not been fully realized. To understand people's perspectives on sleep sensing devices and their potential for promoting sleep health, we surveyed 87 and interviewed 12 people who currently use or have previously used sleep sensors, interviewed 5 sleep medical experts, and conducted an in-depth qualitative analysis of 6986 reviews of the most popular commercial sleep sensing technologies. We found that the feedback provided by current sleep sensing technologies affects users' perceptions of their sleep and encourages goals that are in tension with evidence-based methods for promoting good sleep health. Our research provides design recommendations for improving the feedback of sleep sensing technologies by bridging the gap between expert and user goals.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ravichandran, Ruth and Sien, Sang-Wha and Patel, Shwetak N. and Kientz, Julie A. and Pina, Laura R.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {behavior change, quantified self, personal informatics, health monitoring, sleep, sleep sensing, sleep tracking},
	pages = {6864--6875},
}

@inproceedings{epstein_examining_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Examining {Menstrual} {Tracking} to {Inform} the {Design} of {Personal} {Informatics} {Tools}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025635},
	doi = {10.1145/3025453.3025635},
	abstract = {We consider why and how women track their menstrual cycles, examining their experiences to uncover design opportunities and extend the field's understanding of personal informatics tools. To understand menstrual cycle tracking practices, we collected and analyzed data from three sources: 2,000 reviews of popular menstrual tracking apps, a survey of 687 people, and follow-up interviews with 12 survey respondents. We find that women track their menstrual cycle for varied reasons that include remembering and predicting their period as well as informing conversations with healthcare providers. Participants described six methods of tracking their menstrual cycles, including use of technology, awareness of their premenstrual physiological states, and simply remembering. Although women find apps and calendars helpful, these methods are ineffective when predictions of future menstrual cycles are inaccurate. Designs can create feelings of exclusion for gender and sexual minorities. Existing apps also generally fail to consider life stages that women experience, including young adulthood, pregnancy, and menopause. Our findings encourage expanding the field's conceptions of personal informatics.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Epstein, Daniel A. and Lee, Nicole B. and Kang, Jennifer H. and Agapie, Elena and Schroeder, Jessica and Pina, Laura R. and Fogarty, James and Kientz, Julie A. and Munson, Sean},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {personal informatics, inclusivity, lived informatics, menstrual cycle, menstrual tracking, period, women's health},
	pages = {6876--6888},
}

@inproceedings{ayobi_quantifying_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Quantifying the {Body} and {Caring} for the {Mind}: {Self}-{Tracking} in {Multiple} {Sclerosis}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025869},
	doi = {10.1145/3025453.3025869},
	abstract = {Consumer health technologies have an enormous potential to transform the self-management of chronic conditions. However, it is unclear how individuals use self-tracking technologies to manage them. This in-depth interview study explores self-tracking practices in multiple sclerosis (MS), a complex neurological disease that causes physical, cognitive, and psychological symptoms. Our findings illustrate that when faced the unpredictable and degenerative nature of MS, individuals regained a sense of control by intertwining self-care practices with different self-tracking technologies. They engaged in disease monitoring, fitness tracking, and life journaling to quantify the body and care for the mind. We focus attention on the role of emotional wellbeing and the experience of control in self-tracking and managing MS. Finally, we discuss in which ways self-tracking technologies could support the experiential nature of control and foster mindful experiences rather than focusing only on tracking primary disease indicators.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ayobi, Amid and Marshall, Paul and Cox, Anna L. and Chen, Yunan},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {self-tracking, perceived control, personal informatics, chronic conditions, multiple sclerosis, self-care technologies},
	pages = {6889--6901},
}

@inproceedings{mcroberts_share_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Share {First}, {Save} {Later}: {Performance} of {Self} through {Snapchat} {Stories}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025771},
	doi = {10.1145/3025453.3025771},
	abstract = {As the third most popular social network among millennials, Snapchat is well known for its picture and video messaging system that deletes content after it is viewed. However, the Stories feature of Snapchat offers a different perspective of ephemeral content sharing, with pictures and videos that are available for friends to watch an unlimited number of times for 24 hours. We conduct-ed an in-depth qualitative investigation by interviewing 18 participants and reviewing 14 days of their Stories posts. We identify five themes focused on how participants perceive and use the Stories feature, and apply a Goffmanesque metaphor to our analysis. We relate the Stories medium to other research on self-presentation and identity curation in social media.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McRoberts, Sarah and Ma, Haiwei and Hall, Andrew and Yarosh, Svetlana},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {social media, ephemerality, millennials, presentation of self, snapchat},
	pages = {6902--6911},
}

@inproceedings{schlesinger_situated_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Situated {Anonymity}: {Impacts} of {Anonymity}, {Ephemerality}, and {Hyper}-{Locality} on {Social} {Media}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025682},
	doi = {10.1145/3025453.3025682},
	abstract = {Anonymity, ephemerality, and hyper-locality are an uncommon set of features in the design of online communities. However, these features were key to Yik Yak's initial success and popularity. In an interview-based study, we found that these three features deeply affected the identity of the community as a whole, the patterns of use, and the ways users committed to this community. We conducted interviews with 18 Yik Yak users on an urban American university campus and found that these three focal design features contributed to casual commitment, transitory use, and emergent community identity. We describe situated anonymity, which is the result of anonymity, ephemerality, and hyper-locality coexisting as focal design features of an online community. This work extends our understanding of use and identity-versus-bond based commitment, which has implications for the design and study of other atypical online communities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Schlesinger, Ari and Chandrasekharan, Eshwar and Masden, Christina A. and Bruckman, Amy S. and Edwards, W. Keith and Grinter, Rebecca E.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {online communities, anonymity, commitment, ephemerality, community identity, hyper-locality, transitory use},
	pages = {6912--6924},
}

@inproceedings{carpenter_relational_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Relational {Distancing} and {Termination} between {Online} {Friends}: {An} {Application} of the {Investment} {Model}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026026},
	doi = {10.1145/3025453.3026026},
	abstract = {This research examined the relational maintenance versus termination of online friendships in Facebook. Guided by Rusbult's [33] investment model (IM), the study constructed a model to examine 55 matched pairs of Facebook friends consisting of one "primary user" and one "annoyer." Results indicated that primary users' judgments of relational satisfaction with annoyers were influenced by annoyers' narcissistic personality and their overall propensity for posting overly self-focused content. Commitment affected primary users' use of both passive "unfollowing" and active "unfriending" in response to annoyers' behavior. Decisions to maintain or terminate online friendships are related to judgments and actions of both partners. Overall, these results emphasize the dyadic nature of relational maintenance and termination processes in online environments, and the importance of studying them as such.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Carpenter, Christopher J. and Tong, Stephanie Tom},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {investment model, narcissism, relational termination, social network websites, unfriending},
	pages = {6925--6935},
}

@inproceedings{puussaar_enhancing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Enhancing {Personal} {Informatics} {Through} {Social} {Sensemaking}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025804},
	doi = {10.1145/3025453.3025804},
	abstract = {Personal informatics practices are increasingly common, with a range of consumer technologies available to support, largely individual, interactions with data (e.g., performance measurement and activity/health monitoring). In this paper, we explore the concept of social sensemaking. In contrast to high-level statistics, we posit that social networking and reciprocal sharing of fine-grained self-tracker data can provide valuable context for individuals in making sense of their data. We present the design of an online platform called Citizense Makers (CM), which facilitates group sharing, annotating and discussion of self-tracker data. In a field trial of CM, we explore design issues around willingness to share data reciprocally; the importance of familiarity between individuals; and understandings of common activities in contextualising one's own data.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Puussaar, Aare and Clear, Adrian K. and Wright, Peter},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {data sharing, personal informatics, social sensemaking},
	pages = {6936--6942},
}

@inproceedings{samory_sizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Sizing {Up} the {Troll}: {A} {Quantitative} {Characterization} of {Moderator}-{Identified} {Trolling} in an {Online} {Forum}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026007},
	doi = {10.1145/3025453.3026007},
	abstract = {A few troublemakers often spoil online environments for everyone else. An extremely disruptive type of abuser is the troll, whose malicious activities are relatively non-obvious, and thus difficult to detect and contain – particularly by automated systems. A growing corpus of qualitative research focuses on trolling, and differentiates it from other forms of abuse; however, its findings are not directly actionable into automated systems. On the other hand, quantitative research uses definitions of "troll" that mostly fail to capture what moderators and users consider trolling. We address this gap by giving a quantitative analysis of posts, conversations, and users, specifically sanctioned for trolling in an online forum. Although trolls (unlike most other abusers) hardly stand out in a conversation e.g. in terms of vocabulary, textithow they interact, rather than textitwhat they contribute, provides cues of their malicious intent.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Samory, Mattia and Peserico, Enoch},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {social computing, online forums, trolls},
	pages = {6943--6947},
}

@inproceedings{morreale_building_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Building a {Maker} {Community} {Around} an {Open} {Hardware} {Platform}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3026056},
	doi = {10.1145/3025453.3026056},
	abstract = {This paper reflects on the dynamics and practices of building a maker community around a new hardware platform. We examine the factors promoting the successful uptake of a maker platform from two perspectives: first, we investigate the technical and user experience considerations that users identify as the most important. Second, we explore the specific activities that help attract a community and encourage sustained participation. We present an inductive approach based on the case study of Bela, an embedded platform for creating interactive audio systems. The technical design and community building processes are detailed, culminating in a successful crowdfunding campaign. To further understand the community dynamics, the paper also presents an intensive three-day workshop with eight digital musical instrument designers. From observations and interviews, we reflect on the relationship between the platform and the community and offer suggestions for HCI researchers and practitioners interested in establishing their own maker communities.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Morreale, Fabio and Moro, Giulio and Chamberlain, Alan and Benford, Steve and McPherson, Andrew P.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {diy, crowdfunding, digital musical instruments, embedded hardware, maker community, pluggable communities},
	pages = {6948--6959},
}

@inproceedings{ferdous_celebratory_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Celebratory {Technology} to {Orchestrate} the {Sharing} of {Devices} and {Stories} during {Family} {Mealtimes}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025492},
	doi = {10.1145/3025453.3025492},
	abstract = {While the idea of "celebratory technologies" during family mealtimes to support positive interactions at the dinner table is promising, there are few studies that investigate how these technologies can be meaningfully integrated into family practices. This paper presents the deployment of Chorus - a mealtime technology that orchestrates the sharing of personal devices and stories during family mealtimes, explores related content from all participants' devices, and supports revisiting previously shared content. A three-week field deployment with seven families shows that Chorus augments family interactions through sharing contents of personal and familial significance, supports togetherness and in-depth discussion by combining resources from multiple devices, helps to broach sensitive topics into familial conversation, and encourages participation from all family members including children. We discuss implications of this research and reflect on design choices and opportunities that can further enhance the family mealtime experience.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ferdous, Hasan Shahid and Vetere, Frank and Davis, Hilary and Ploderer, Bernd and O'Hara, Kenton and Comber, Rob and Farr-Wharton, Geremy},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {family, smartphones, collaborative use, collocated interactions, commensality, mealtimes},
	pages = {6960--6972},
}

@inproceedings{sleeper_exploring_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Exploring {Topic}-{Based} {Sharing} {Mechanisms}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025840},
	doi = {10.1145/3025453.3025840},
	abstract = {General-purpose content-sharing platforms make it difficult for users to limit sharing to people interested in particular topics. Additional topic-based controls may allow users to better reach desired audiences. Designing such tools requires understanding current interest-based targeting techniques and the potential impact of additional mechanisms. We present an exploratory, interview-based study (n = 16) that addresses these dynamics for Facebook. We use diary-driven probes to explore general topic-based sharing across applications. We then use Facebook-based mockups to probe use cases and design tensions around adding topic-based sharing mechanisms to Facebook. We find that participants currently draw on various audience-limiting and reaching strategies to target interest-based audiences. Participants felt additional topic-based sharing mechanisms on Facebook might allow them to avoid oversharing or offending others and allow them to target improved audiences or share improved content. Usable topic-based sharing tools would also need to account, however, for participants' varied desired engagement strategies.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sleeper, Manya and Cranor, Lorrie Faith and Pearman, Sarah K.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {facebook, access control, selective sharing, targeting strategies, topic-based sharing, topics},
	pages = {6973--6985},
}

@inproceedings{thomas_hci_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{HCI} and {Environmental} {Public} {Policy}: {Opportunities} for {Engagement}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025579},
	doi = {10.1145/3025453.3025579},
	abstract = {This note discusses opportunities for the HCI community to engage with environmental public policy. It draws on insights and observations made during the primary author's recent work for a policy unit at Global Affairs Canada, which is a federal ministry of the Government of Canada. During that work, the primary author identified several domains of environmental public policy that are of direct relevance to the HCI community. This note contributes a preliminary discussion of how, why, with whom, and in what capacity HCI researchers and practitioners might engage with three types of environmental public policy: climate change, waste electrical and electronic equipment, and green ICT procurement policies. This builds on existing public policy and environmental knowledge within the HCI community and responds directly to calls from some members to engage with environmental public policy.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Thomas, Vanessa and Remy, Christian and Hazas, Mike and Bates, Oliver},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {sustainable hci, climate change, environmental public policy, government, public policy},
	pages = {6986--6992},
}

@inproceedings{roto_utilizing_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Utilizing {Experience} {Goals} in {Design} of {Industrial} {Systems}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025620},
	doi = {10.1145/3025453.3025620},
	abstract = {The core idea of experience-driven design is to define the intended experience before functionality and technology. This is a radical idea for companies that have built their competences around specific technologies. Although many technology companies are willing to shift their focus towards experience-driven design, reports on real-life cases about the utilization of this design approach are rare. As part of an industry-led research program, we introduced experience-driven design to metal industry companies with experience goals as the key technique. Four design cases in three companies showed that the goals are useful in keeping the focus on user experience, but several challenges are still left for future research to tackle. This exploratory research lays ground for future research by providing initial criteria for assessing experience design tools. The results shed light on utilizing experience goals in industrial design projects and help practitioners in planning and managing the product design process with user experience in mind.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Roto, Virpi and Kaasinen, Eija and Heimonen, Tomi and Karvonen, Hannu and Jokinen, Jussi P. P. and Mannonen, Petri and Nousu, Hannu and Hakulinen, Jaakko and Lu, Yichen and Saariluoma, Pertti O. and Kymäläinen, Tiina and Keskinen, Tuuli and Turunen, Markku and Koskinen, Hanna Maria Kaarina},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {user experience, experience design tools, experience goal, experience-driven design, industrial systems},
	pages = {6993--7004},
}

@inproceedings{maiden_evaluating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Evaluating {Digital} {Creativity} {Support} {To} {Improve} {Health}-and-{Safety} in a {Manufacturing} {Plant}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025712},
	doi = {10.1145/3025453.3025712},
	abstract = {This paper reports an evaluation of digital support for human creativity to improve health-and-safety in one manufacturing plant. It reports the use of this support as part of the plant's risk management process over 66 working days. Results revealed that this use led to more complete, more useful and more novel risk resolutions, compared with the original paper process, and informed how digital creativity support can be rolled out across manufacturing plants, as well as to other domains not recognized as creative.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Maiden, Neil and Zachos, Konstantinos and Lockerbie, James and Levis, Sergio and Camargo, Kasia and Hoddy, Shaun and Allemandi, Gianluca},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {creativity, mobile, health-and-safety, risk management},
	pages = {7005--7014},
}

@inproceedings{harmon_design_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {The {Design} {Fictions} of {Philanthropic} {IT}: {Stuck} {Between} an {Imperfect} {Present} and an {Impossible} {Future}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025650},
	doi = {10.1145/3025453.3025650},
	abstract = {In this paper, we examine the stories about philanthropic IT that circulate via product websites, marketing materials, and third-party news articles. Through a series of product-centered case studies, we surface these texts' implicit and explicit visions about the (near) future of philanthropy. We detail their prescriptions about how, why, and in service of what ends nonprofit organizations could, should, and ought to leverage IT. We also examine their underlying assumptions about philanthropy: how social good is accomplished, how philanthropic organizations are - and might be more - effective, to whom organizations and beneficiaries should be accountable, and the terms of that accountability. Analyzing these visions as design fictions, we argue that they help cultivate unrealistic anticipatory relationships to the present and entail concomitantly unrealistic imperatives for the philanthropic sector. We conclude by arguing for the crucial role of HCI scholars in disrupting these impossible futures, and by highlighting areas needing further, re-imagined, research.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Harmon, Ellie and Bopp, Chris and Voida, Amy},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {design fiction, discourse analysis, nonprofits, philanthropy},
	pages = {7015--7028},
}

@inproceedings{gronbaek_proxemic_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Proxemic {Transitions}: {Designing} {Shape}-{Changing} {Furniture} for {Informal} {Meetings}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025487},
	doi = {10.1145/3025453.3025487},
	abstract = {The field of Shape-Changing Interfaces explores the qualities of physically dynamic artifacts. At furniture-scale, such artifacts have the potential of changing the ways we collaborate and engage with interiors and physical spaces. Informed by theories of proxemics, empirical studies of informal meetings and design work with shape-changing furniture, we develop the notion of proxemic transitions. We present three design aspects of proxemic transitions: transition speed, stepwise reconfiguration, and radical shifts. The design aspects focus on how to balance between physical and digital transformations in designing for proxemic transitions. Our contribution is three-fold: 1) the notion of proxemic transitions, 2) three design aspects to consider in designing for proxemic transitions, and 3) initial exploration of how these design aspects might generate designs of dynamic furniture. These contributions outline important aspects to consider when designing shape-changing furniture for informal workplace meetings.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Grønbæk, Jens Emil and Korsgaard, Henrik and Petersen, Marianne Graves and Birk, Morten Henriksen and Krogh, Peter Gall},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {shape-changing interfaces, augmented furniture, f-formations, interaction proxemics, proxemic transitions},
	pages = {7029--7041},
}

@inproceedings{lazar_successful_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Successful {Leisure} in {Independent} {Living} {Communities}: {Understanding} {Older} {Adults}' {Motivations} to {Engage} in {Leisure} {Activities}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025802},
	doi = {10.1145/3025453.3025802},
	abstract = {Leisure activities are a source of meaning and enjoyment for individuals across the lifespan. In this study, we conducted interviews with twenty-four older adults living in four different independent living communities. We present societal and ecological factors and motivations that influenced the way people participated in and decided what constitutes leisure activities. The goal of maintaining physical and cognitive health was often intertwined with motivations to engage in leisure activities. We discuss how this fits into the broader framework of successful aging and implications for technology design. We also provide an example of how findings from this study can be applied to a specific leisure activity: watching television.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lazar, Amanda and Nguyen, David H.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {older adults, aging, ageism, leisure, successful aging},
	pages = {7042--7056},
}

@inproceedings{hornung_navigating_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Navigating {Relationships} and {Boundaries}: {Concerns} around {ICT}-{Uptake} for {Elderly} {People}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025859},
	doi = {10.1145/3025453.3025859},
	abstract = {Despite a proliferation of research in the use of ICTs to support active and healthy ageing, few have considered the privacy and security concerns particular to the elderly. We investigated the appropriation of tablet devices and a neighborhood portal as well as emerging privacy and security issues through ethnographic and action research in a long-term participatory design (PD) project with elderly participants. We discuss two major themes: a) the tensions related to perceived digital threats and the social pressures of online disclosure to the social environment; and b) the relation of these issues to the ICT appropriation process and the referring challenges we encountered. We argue that there is a need to understand the interleaving of physical and virtual habitats, the various ways resulting in discomfort and the senior citizens' actions – which at first glance appear contradictory. We consider the implications of the issues observed for examining privacy and security concerns more broadly as well as discussing implications for the design of the portal and the shaping of social measures for appropriation support.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hornung, Dominik and Müller, Claudia and Shklovski, Irina and Jakobi, Timo and Wulf, Volker},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {participatory design, ethnography, appropriation, privacy, disclosure, action research, communities of practice, design case studies, elderly people},
	pages = {7057--7069},
}

@inproceedings{guo_older_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {Older {Adults} {Learning} {Computer} {Programming}: {Motivations}, {Frustrations}, and {Design} {Opportunities}},
	isbn = {978-1-4503-4655-9},
	url = {https://doi.org/10.1145/3025453.3025945},
	doi = {10.1145/3025453.3025945},
	abstract = {Computer programming is a highly in-demand skill, but most learn-to-code initiatives and research target some of the youngest members of society: children and college students. We present the first known study of older adults learning computer programming. Using an online survey with 504 respondents aged 60 to 85 who are from 52 different countries, we discovered that older adults were motivated to learn to keep their brains challenged as they aged, to make up for missed opportunities during youth, to connect with younger family members, and to improve job prospects. They reported frustrations including a perceived decline in cognitive abilities, lack of opportunities to interact with tutors and peers, and trouble dealing with constantly-changing software technologies. Based on these findings, we propose a learner-centered design of techniques and tools for motivating older adults to learn programming and discuss broader societal implications of a future where more older adults have access to computer programming – not merely computer literacy – as a skill set.},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Philip J.},
	year = {2017},
	note = {event-place: Denver, Colorado, USA},
	keywords = {older adults, computational literacy, learning programming},
	pages = {7070--7083},
}
